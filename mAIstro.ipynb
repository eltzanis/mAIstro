{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import yaml\n",
    "import json\n",
    "import glob\n",
    "import time\n",
    "from datetime import datetime\n",
    "import logging\n",
    "import tempfile\n",
    "import subprocess\n",
    "from typing import Dict, Optional, List, Tuple\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.api.types import is_numeric_dtype, is_datetime64_any_dtype, is_categorical_dtype\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import models, transforms, datasets\n",
    "import SimpleITK as sitk\n",
    "from radiomics import featureextractor\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, f_regression, mutual_info_classif, mutual_info_regression, RFE\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, roc_auc_score, roc_curve, auc,\n",
    "    classification_report, mean_squared_error, mean_absolute_error,\n",
    "    r2_score, explained_variance_score, mean_absolute_percentage_error\n",
    ")\n",
    "\n",
    "\n",
    "from smolagents import Tool, CodeAgent, LiteLLMModel, GradioUI, HfApiModel\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Radiomic Extraction Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PyRadiomicsFeatureExtractionTool(Tool):\n",
    "    name = \"pyradiomics_feature_extraction\"\n",
    "    description = \"\"\"\n",
    "    This tool extracts radiomic features from medical images (CT, MRI, etc.) using PyRadiomics.\n",
    "    It processes images and their corresponding segmentation masks, and can handle multiple labels within masks.\n",
    "    Features are extracted for each label and saved separately to CSV files.\n",
    "    \"\"\"\n",
    "    \n",
    "    inputs = {\n",
    "        \"image_dir\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"Directory containing the medical images (e.g., MRI, CT)\"\n",
    "        },\n",
    "        \"mask_dir\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"Directory containing the segmentation masks corresponding to the images\"\n",
    "        },\n",
    "        \"output_dir\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"Directory where extracted features will be saved\"\n",
    "        },\n",
    "        \"image_types\": {\n",
    "            \"type\": \"array\",\n",
    "            \"description\": \"Types of image preprocessing to apply (e.g., Original, Wavelet, LoG). Default is all available types.\",\n",
    "            \"required\": False,\n",
    "            \"nullable\": True\n",
    "        },\n",
    "        \"feature_classes\": {\n",
    "            \"type\": \"array\",\n",
    "            \"description\": \"Classes of features to extract (e.g., firstorder, shape, glcm). Default is all available classes.\",\n",
    "            \"required\": False,\n",
    "            \"nullable\": True\n",
    "        },\n",
    "        \"specific_features\": {\n",
    "            \"type\": \"array\",\n",
    "            \"description\": \"Specific features to extract. If provided, only these features will be extracted.\",\n",
    "            \"required\": False,\n",
    "            \"nullable\": True\n",
    "        },\n",
    "        \"mask_labels\": {\n",
    "            \"type\": \"array\",\n",
    "            \"description\": \"Specific label values to extract features for. Default is all non-zero labels found in masks.\",\n",
    "            \"required\": False,\n",
    "            \"nullable\": True\n",
    "        },\n",
    "        \"image_pattern\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"Filename pattern to match image files. Default is '*.nii.gz'.\",\n",
    "            \"required\": False,\n",
    "            \"nullable\": True\n",
    "        },\n",
    "        \"mask_pattern\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"Filename pattern to match mask files. Default is '*.nii.gz'.\",\n",
    "            \"required\": False,\n",
    "            \"nullable\": True\n",
    "        },\n",
    "        \"normalize\": {\n",
    "            \"type\": \"boolean\",\n",
    "            \"description\": \"Whether to normalize the image intensity values. Default is True.\",\n",
    "            \"required\": False,\n",
    "            \"nullable\": True\n",
    "        },\n",
    "        \"bin_width\": {\n",
    "            \"type\": \"number\",\n",
    "            \"description\": \"Bin width for discretizing image intensities. Default is 25.\",\n",
    "            \"required\": False,\n",
    "            \"nullable\": True\n",
    "        },\n",
    "        \"resample\": {\n",
    "            \"type\": \"boolean\",\n",
    "            \"description\": \"Whether to resample the image to isotropic voxels. Default is True.\",\n",
    "            \"required\": False,\n",
    "            \"nullable\": True\n",
    "        },\n",
    "        \"pixel_spacing\": {\n",
    "            \"type\": \"array\",\n",
    "            \"description\": \"Target pixel spacing for resampling [x, y, z]. Default is [1, 1, 1].\",\n",
    "            \"required\": False,\n",
    "            \"nullable\": True\n",
    "        },\n",
    "        \"force_2d\": {\n",
    "            \"type\": \"boolean\",\n",
    "            \"description\": \"Whether to extract features slice by slice (2D) instead of 3D. Default is False.\",\n",
    "            \"required\": False,\n",
    "            \"nullable\": True\n",
    "        },\n",
    "        \"n_workers\": {\n",
    "            \"type\": \"integer\",\n",
    "            \"description\": \"Number of parallel workers for processing images. Default is 1 (serial processing).\",\n",
    "            \"required\": False,\n",
    "            \"nullable\": True\n",
    "        },\n",
    "        \"targets_csv\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"Path to a CSV file containing target values for each subject. Optional.\",\n",
    "            \"required\": False,\n",
    "            \"nullable\": True\n",
    "        },\n",
    "        \"id_column\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"Name of the ID column in the targets CSV. Only used if targets_csv is provided.\",\n",
    "            \"required\": False,\n",
    "            \"nullable\": True\n",
    "        },\n",
    "        \"target_column\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"Name of the target column in the targets CSV. Only used if targets_csv is provided.\",\n",
    "            \"required\": False,\n",
    "            \"nullable\": True\n",
    "        },\n",
    "        \"id_pattern\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"Regex pattern to extract subject ID from filenames. Default is to use the filename without extension.\",\n",
    "            \"required\": False,\n",
    "            \"nullable\": True\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    output_type = \"object\"\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        image_dir: str,\n",
    "        mask_dir: str,\n",
    "        output_dir: str,\n",
    "        image_types: Optional[List[str]] = None,\n",
    "        feature_classes: Optional[List[str]] = None,\n",
    "        specific_features: Optional[List[str]] = None,\n",
    "        mask_labels: Optional[List[int]] = None,\n",
    "        image_pattern: Optional[str] = \"*.nii.gz\",\n",
    "        mask_pattern: Optional[str] = \"*.nii.gz\",\n",
    "        normalize: Optional[bool] = True,\n",
    "        bin_width: Optional[float] = 25.0,\n",
    "        resample: Optional[bool] = True,\n",
    "        pixel_spacing: Optional[List[float]] = None,\n",
    "        force_2d: Optional[bool] = False,\n",
    "        n_workers: Optional[int] = 1,\n",
    "        targets_csv: Optional[str] = None,\n",
    "        id_column: Optional[str] = None,\n",
    "        target_column: Optional[str] = None,\n",
    "        id_pattern: Optional[str] = None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Extract radiomic features from medical images using PyRadiomics.\n",
    "        \n",
    "        Args:\n",
    "            image_dir: Directory containing the medical images\n",
    "            mask_dir: Directory containing the segmentation masks\n",
    "            output_dir: Directory where extracted features will be saved\n",
    "            image_types: Image preprocessing types (e.g., [\"Original\", \"Wavelet\", \"LoG\"])\n",
    "            feature_classes: Feature classes to extract (e.g., [\"firstorder\", \"shape\", \"glcm\"])\n",
    "            specific_features: Specific features to extract (e.g., [\"original_firstorder_Mean\"])\n",
    "            mask_labels: Specific label values to extract features for\n",
    "            image_pattern: Filename pattern to match image files\n",
    "            mask_pattern: Filename pattern to match mask files\n",
    "            normalize: Whether to normalize image intensities\n",
    "            bin_width: Bin width for discretizing image intensities\n",
    "            resample: Whether to resample the image\n",
    "            pixel_spacing: Target pixel spacing for resampling [x, y, z]\n",
    "            force_2d: Whether to extract features in 2D mode\n",
    "            n_workers: Number of parallel workers\n",
    "            targets_csv: Path to CSV with target values\n",
    "            id_column: Name of ID column in targets CSV\n",
    "            target_column: Name of target column in targets CSV\n",
    "            id_pattern: Regex pattern to extract subject ID from filenames\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with feature extraction results and file paths\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Set up logging\n",
    "            os.makedirs(output_dir, exist_ok=True)\n",
    "            log_file = os.path.join(output_dir, \"radiomics_extraction.log\")\n",
    "            logging.basicConfig(\n",
    "                level=logging.INFO,\n",
    "                format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "                handlers=[\n",
    "                    logging.FileHandler(log_file),\n",
    "                    logging.StreamHandler()\n",
    "                ]\n",
    "            )\n",
    "            \n",
    "            # Set default values\n",
    "            if pixel_spacing is None:\n",
    "                pixel_spacing = [1.0, 1.0, 1.0]\n",
    "            \n",
    "            # Log configuration\n",
    "            logging.info(f\"Starting radiomic feature extraction with configuration:\")\n",
    "            logging.info(f\"Image directory: {image_dir}\")\n",
    "            logging.info(f\"Mask directory: {mask_dir}\")\n",
    "            logging.info(f\"Output directory: {output_dir}\")\n",
    "            logging.info(f\"Image types: {image_types if image_types else 'All available'}\")\n",
    "            logging.info(f\"Feature classes: {feature_classes if feature_classes else 'All available'}\")\n",
    "            logging.info(f\"Specific features: {specific_features if specific_features else 'None'}\")\n",
    "            logging.info(f\"Mask labels: {mask_labels if mask_labels else 'All non-zero'}\")\n",
    "            logging.info(f\"Normalize: {normalize}\")\n",
    "            logging.info(f\"Bin width: {bin_width}\")\n",
    "            logging.info(f\"Resample: {resample}\")\n",
    "            logging.info(f\"Pixel spacing: {pixel_spacing}\")\n",
    "            logging.info(f\"Force 2D: {force_2d}\")\n",
    "            logging.info(f\"Number of workers: {n_workers}\")\n",
    "            \n",
    "            # Prepare PyRadiomics parameters\n",
    "            params = self._create_pyradiomics_params(\n",
    "                image_types=image_types,\n",
    "                feature_classes=feature_classes,\n",
    "                specific_features=specific_features,\n",
    "                normalize=normalize,\n",
    "                bin_width=bin_width,\n",
    "                resample=resample,\n",
    "                pixel_spacing=pixel_spacing,\n",
    "                force_2d=force_2d\n",
    "            )\n",
    "            \n",
    "            # Save parameters to a YAML file\n",
    "            params_file = os.path.join(output_dir, \"radiomics_params.yaml\")\n",
    "            with open(params_file, 'w') as f:\n",
    "                yaml.dump(params, f)\n",
    "            logging.info(f\"PyRadiomics parameters saved to {params_file}\")\n",
    "            \n",
    "            # Initialize feature extractor\n",
    "            extractor = featureextractor.RadiomicsFeatureExtractor(params_file)\n",
    "            logging.info(f\"Feature extractor initialized with {len(extractor.enabledFeatures)} features\")\n",
    "            \n",
    "            # Load target values if provided\n",
    "            target_dict = None\n",
    "            if targets_csv and os.path.exists(targets_csv):\n",
    "                logging.info(f\"Loading target values from {targets_csv}\")\n",
    "                target_dict = self._load_targets(\n",
    "                    targets_csv, \n",
    "                    id_column=id_column, \n",
    "                    target_column=target_column\n",
    "                )\n",
    "                logging.info(f\"Loaded {len(target_dict)} target values\")\n",
    "            \n",
    "            # Find image and mask files\n",
    "            image_files = sorted(glob.glob(os.path.join(image_dir, image_pattern)))\n",
    "            mask_files = sorted(glob.glob(os.path.join(mask_dir, mask_pattern)))\n",
    "            \n",
    "            logging.info(f\"Found {len(image_files)} image files and {len(mask_files)} mask files\")\n",
    "            \n",
    "            # Match images and masks\n",
    "            image_mask_pairs, unmatched = self._match_images_and_masks(\n",
    "                image_files, \n",
    "                mask_files, \n",
    "                id_pattern=id_pattern\n",
    "            )\n",
    "            \n",
    "            logging.info(f\"Matched {len(image_mask_pairs)} image-mask pairs\")\n",
    "            if unmatched['images'] or unmatched['masks']:\n",
    "                logging.warning(f\"Unmatched images: {len(unmatched['images'])}, unmatched masks: {len(unmatched['masks'])}\")\n",
    "            \n",
    "            # Check if we have any pairs to process\n",
    "            if not image_mask_pairs:\n",
    "                logging.error(\"No matching image-mask pairs found. Check your files and patterns.\")\n",
    "                return {\n",
    "                    \"status\": \"error\",\n",
    "                    \"error_message\": \"No matching image-mask pairs found\",\n",
    "                    \"image_dir\": image_dir,\n",
    "                    \"mask_dir\": mask_dir\n",
    "                }\n",
    "            \n",
    "            # Determine available labels in masks if not specified\n",
    "            if mask_labels is None:\n",
    "                mask_labels = self._find_available_labels(mask_files)\n",
    "                logging.info(f\"Found {len(mask_labels)} unique labels in masks: {mask_labels}\")\n",
    "            \n",
    "            # Create a temporary directory for extracted masks by label\n",
    "            temp_dir = tempfile.mkdtemp(dir=output_dir)\n",
    "            logging.info(f\"Created temporary directory for masks: {temp_dir}\")\n",
    "            \n",
    "            # Process each image-mask pair\n",
    "            start_time = time.time()\n",
    "            \n",
    "            # Initialize feature dictionaries for each label\n",
    "            all_features_by_label = {label: [] for label in mask_labels}\n",
    "            \n",
    "            # Process serially or in parallel based on n_workers\n",
    "            if n_workers <= 1:\n",
    "                # Serial processing\n",
    "                for i, (subject_id, files) in enumerate(image_mask_pairs.items()):\n",
    "                    self._process_subject(\n",
    "                        subject_id, \n",
    "                        files, \n",
    "                        mask_labels, \n",
    "                        extractor, \n",
    "                        temp_dir, \n",
    "                        all_features_by_label, \n",
    "                        target_dict,\n",
    "                        i + 1,\n",
    "                        len(image_mask_pairs)\n",
    "                    )\n",
    "            else:\n",
    "                # Parallel processing\n",
    "                logging.info(f\"Using {n_workers} workers for parallel processing\")\n",
    "                \n",
    "                # Create arguments for parallel processing\n",
    "                process_args = []\n",
    "                for i, (subject_id, files) in enumerate(image_mask_pairs.items()):\n",
    "                    subject_temp_dir = os.path.join(temp_dir, subject_id)\n",
    "                    os.makedirs(subject_temp_dir, exist_ok=True)\n",
    "                    \n",
    "                    target_value = None\n",
    "                    if target_dict and subject_id in target_dict:\n",
    "                        target_value = target_dict[subject_id]\n",
    "                    \n",
    "                    process_args.append((\n",
    "                        subject_id, \n",
    "                        files['image'],\n",
    "                        files['mask'], \n",
    "                        mask_labels, \n",
    "                        params_file, \n",
    "                        subject_temp_dir,\n",
    "                        target_value,\n",
    "                        i + 1,\n",
    "                        len(image_mask_pairs)\n",
    "                    ))\n",
    "                \n",
    "                # Execute in parallel\n",
    "                with ProcessPoolExecutor(max_workers=n_workers) as executor:\n",
    "                    futures = [executor.submit(self._process_subject_parallel, *args) for args in process_args]\n",
    "                    \n",
    "                    for future in as_completed(futures):\n",
    "                        try:\n",
    "                            subject_id, subject_features_by_label = future.result()\n",
    "                            # Merge results\n",
    "                            for label, features in subject_features_by_label.items():\n",
    "                                if features:  # Only add if features were extracted\n",
    "                                    all_features_by_label[label].append(features)\n",
    "                        except Exception as e:\n",
    "                            logging.error(f\"Error in parallel processing: {str(e)}\")\n",
    "            \n",
    "            # Calculate processing time\n",
    "            processing_time = time.time() - start_time\n",
    "            logging.info(f\"Feature extraction completed in {processing_time:.2f} seconds\")\n",
    "            \n",
    "            # Save features to CSV files\n",
    "            csv_paths = {}\n",
    "            for label, features in all_features_by_label.items():\n",
    "                if not features:\n",
    "                    logging.warning(f\"No features extracted for label {label}\")\n",
    "                    continue\n",
    "                \n",
    "                # Convert to DataFrame\n",
    "                df = pd.DataFrame(features)\n",
    "                \n",
    "                # Arrange columns (ID and target columns first, then features)\n",
    "                id_cols = [col for col in df.columns if col.lower() in ['id', 'subject_id', 'patientid']]\n",
    "                target_cols = [col for col in df.columns if col.lower() in ['target', 'label', 'outcome']]\n",
    "                feature_cols = [col for col in df.columns if col not in id_cols + target_cols]\n",
    "                \n",
    "                # Sort feature columns for consistency\n",
    "                feature_cols.sort()\n",
    "                \n",
    "                # Reorder columns\n",
    "                df = df[id_cols + target_cols + feature_cols]\n",
    "                \n",
    "                # Save to CSV\n",
    "                csv_path = os.path.join(output_dir, f\"radiomic_features_label_{label}.csv\")\n",
    "                df.to_csv(csv_path, index=False)\n",
    "                logging.info(f\"Saved {len(df)} subjects with {len(df.columns) - len(id_cols) - len(target_cols)} features for label {label} to {csv_path}\")\n",
    "                \n",
    "                csv_paths[f\"label_{label}\"] = csv_path\n",
    "            \n",
    "            # Clean up temporary directory\n",
    "            import shutil\n",
    "            shutil.rmtree(temp_dir)\n",
    "            logging.info(f\"Removed temporary directory: {temp_dir}\")\n",
    "            \n",
    "            # Return results\n",
    "            return {\n",
    "                \"status\": \"success\",\n",
    "                \"csv_paths\": csv_paths,\n",
    "                \"num_subjects\": len(image_mask_pairs),\n",
    "                \"num_features\": {label: len(df.columns) - 1 - (1 if target_dict else 0) for label, df in \n",
    "                               [(label, pd.DataFrame(features)) for label, features in all_features_by_label.items() if features]},\n",
    "                \"mask_labels\": mask_labels,\n",
    "                \"log_file\": log_file,\n",
    "                \"params_file\": params_file,\n",
    "                \"processing_time_seconds\": processing_time\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error during radiomic feature extraction: {str(e)}\", exc_info=True)\n",
    "            return {\n",
    "                \"status\": \"error\",\n",
    "                \"error_message\": str(e),\n",
    "                \"image_dir\": image_dir,\n",
    "                \"mask_dir\": mask_dir,\n",
    "                \"output_dir\": output_dir\n",
    "            }\n",
    "    \n",
    "    def _create_pyradiomics_params(\n",
    "        self, \n",
    "        image_types=None, \n",
    "        feature_classes=None, \n",
    "        specific_features=None,\n",
    "        normalize=True,\n",
    "        bin_width=25.0,\n",
    "        resample=True,\n",
    "        pixel_spacing=None,\n",
    "        force_2d=False\n",
    "    ):\n",
    "        \"\"\"Create PyRadiomics parameters dictionary\"\"\"\n",
    "        # Default image types\n",
    "        if image_types is None:\n",
    "            image_types = [\"Original\", \"Wavelet\", \"LoG\"]\n",
    "        \n",
    "        # Default feature classes with all their features\n",
    "        if feature_classes is None and specific_features is None:\n",
    "            feature_classes = [\n",
    "                \"firstorder\", \"shape\", \"glcm\", \"glrlm\", \"glszm\", \"gldm\", \"ngtdm\"\n",
    "            ]\n",
    "        \n",
    "        # Initialize parameters dictionary\n",
    "        params = {\n",
    "            \"imageType\": {},\n",
    "            \"featureClass\": {},\n",
    "            \"setting\": {\n",
    "                \"force2D\": force_2d,\n",
    "                \"normalize\": normalize,\n",
    "                \"normalizeScale\": 100,\n",
    "                \"binWidth\": bin_width,\n",
    "                \"interpolator\": \"sitkBSpline\",\n",
    "                \"correctMask\": True,\n",
    "                \"padDistance\": 10,\n",
    "                \"removeOutliers\": 3\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Add resampling if needed\n",
    "        if resample:\n",
    "            if pixel_spacing is None:\n",
    "                pixel_spacing = [1.0, 1.0, 1.0]\n",
    "            params[\"setting\"][\"resampledPixelSpacing\"] = pixel_spacing\n",
    "        \n",
    "        # Configure image types\n",
    "        for img_type in image_types:\n",
    "            if img_type == \"Original\":\n",
    "                params[\"imageType\"][\"Original\"] = {}\n",
    "            elif img_type == \"Wavelet\":\n",
    "                params[\"imageType\"][\"Wavelet\"] = {}\n",
    "            elif img_type == \"LoG\":\n",
    "                # Configure LoG filter with different sigma values\n",
    "                params[\"imageType\"][\"LoG\"] = {\"sigma\": [1.0, 2.0, 3.0, 4.0, 5.0]}\n",
    "            elif img_type == \"Exponential\":\n",
    "                params[\"imageType\"][\"Exponential\"] = {}\n",
    "            elif img_type == \"Gradient\":\n",
    "                params[\"imageType\"][\"Gradient\"] = {}\n",
    "            elif img_type == \"LBP2D\":\n",
    "                params[\"imageType\"][\"LBP2D\"] = {}\n",
    "            elif img_type == \"LBP3D\":\n",
    "                params[\"imageType\"][\"LBP3D\"] = {}\n",
    "        \n",
    "        # Configure feature classes if using all features\n",
    "        if specific_features is None:\n",
    "            for feature_class in feature_classes:\n",
    "                params[\"featureClass\"][feature_class] = []\n",
    "        else:\n",
    "            # If specific features are provided, parse and enable them\n",
    "            for feature in specific_features:\n",
    "                # Parse feature string (e.g., \"original_firstorder_Mean\")\n",
    "                parts = feature.split('_')\n",
    "                if len(parts) >= 2:\n",
    "                    # The last part is the feature name\n",
    "                    feature_name = parts[-1]\n",
    "                    # The second-to-last part is the feature class\n",
    "                    feature_class = parts[-2]\n",
    "                    \n",
    "                    # Ensure the feature class exists in the parameters\n",
    "                    if feature_class not in params[\"featureClass\"]:\n",
    "                        params[\"featureClass\"][feature_class] = []\n",
    "                    \n",
    "                    # Add the feature to the list if it's not already there\n",
    "                    if feature_name not in params[\"featureClass\"][feature_class]:\n",
    "                        params[\"featureClass\"][feature_class].append(feature_name)\n",
    "        \n",
    "        return params\n",
    "    \n",
    "    def _load_targets(self, targets_csv, id_column=None, target_column=None):\n",
    "        \"\"\"Load target values from CSV file\"\"\"\n",
    "        try:\n",
    "            # Read the CSV file\n",
    "            df = pd.read_csv(targets_csv)\n",
    "            \n",
    "            # Determine columns to use\n",
    "            if id_column is None:\n",
    "                id_column = df.columns[0]  # Use first column as ID\n",
    "            \n",
    "            if target_column is None:\n",
    "                target_column = df.columns[1]  # Use second column as target\n",
    "            \n",
    "            # Check if columns exist\n",
    "            if id_column not in df.columns:\n",
    "                raise ValueError(f\"ID column '{id_column}' not found in {targets_csv}\")\n",
    "            \n",
    "            if target_column not in df.columns:\n",
    "                raise ValueError(f\"Target column '{target_column}' not found in {targets_csv}\")\n",
    "            \n",
    "            # Create dictionary mapping IDs to target values\n",
    "            target_dict = {}\n",
    "            for _, row in df.iterrows():\n",
    "                # Convert ID to string for consistent matching\n",
    "                subject_id = str(row[id_column])\n",
    "                target_dict[subject_id] = row[target_column]\n",
    "            \n",
    "            return target_dict\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error loading targets: {str(e)}\")\n",
    "            return {}\n",
    "    \n",
    "    def _match_images_and_masks(self, image_files, mask_files, id_pattern=None):\n",
    "        \"\"\"Match image and mask files by subject ID\"\"\"\n",
    "        image_mask_pairs = {}\n",
    "        unmatched = {\"images\": [], \"masks\": []}\n",
    "        \n",
    "        # Extract subject IDs based on the pattern or use filenames\n",
    "        def extract_id(filepath, pattern=None):\n",
    "            filename = os.path.basename(filepath)\n",
    "            if pattern:\n",
    "                match = re.search(pattern, filename)\n",
    "                if match:\n",
    "                    return match.group(1)\n",
    "            # Default: use filename without extension\n",
    "            return os.path.splitext(filename)[0]\n",
    "        \n",
    "        # Create dictionaries for images and masks with subject IDs as keys\n",
    "        images_dict = {extract_id(f, id_pattern): f for f in image_files}\n",
    "        masks_dict = {extract_id(f, id_pattern): f for f in mask_files}\n",
    "        \n",
    "        # Find matching pairs\n",
    "        all_subjects = set(images_dict.keys()) | set(masks_dict.keys())\n",
    "        for subject_id in all_subjects:\n",
    "            if subject_id in images_dict and subject_id in masks_dict:\n",
    "                image_mask_pairs[subject_id] = {\n",
    "                    \"image\": images_dict[subject_id],\n",
    "                    \"mask\": masks_dict[subject_id]\n",
    "                }\n",
    "            else:\n",
    "                if subject_id in images_dict:\n",
    "                    unmatched[\"images\"].append(images_dict[subject_id])\n",
    "                if subject_id in masks_dict:\n",
    "                    unmatched[\"masks\"].append(masks_dict[subject_id])\n",
    "        \n",
    "        return image_mask_pairs, unmatched\n",
    "    \n",
    "    def _find_available_labels(self, mask_files, max_files_to_check=10):\n",
    "        \"\"\"Find unique label values in mask files\"\"\"\n",
    "        unique_labels = set()\n",
    "        \n",
    "        # Limit the number of files to check to avoid lengthy processing\n",
    "        files_to_check = mask_files[:min(max_files_to_check, len(mask_files))]\n",
    "        \n",
    "        for mask_file in files_to_check:\n",
    "            try:\n",
    "                mask_image = sitk.ReadImage(mask_file)\n",
    "                mask_array = sitk.GetArrayFromImage(mask_image)\n",
    "                \n",
    "                # Find unique non-zero values\n",
    "                labels = np.unique(mask_array)\n",
    "                labels = labels[labels > 0]  # Exclude background (0)\n",
    "                \n",
    "                for label in labels:\n",
    "                    unique_labels.add(int(label))\n",
    "                    \n",
    "            except Exception as e:\n",
    "                logging.warning(f\"Error reading mask {mask_file}: {str(e)}\")\n",
    "        \n",
    "        return list(unique_labels)\n",
    "    \n",
    "    def _process_subject(\n",
    "        self, \n",
    "        subject_id, \n",
    "        files, \n",
    "        mask_labels, \n",
    "        extractor, \n",
    "        temp_dir, \n",
    "        all_features_by_label, \n",
    "        target_dict,\n",
    "        subject_index,\n",
    "        total_subjects\n",
    "    ):\n",
    "        \"\"\"Process a single subject and extract features for each label\"\"\"\n",
    "        try:\n",
    "            logging.info(f\"Processing subject {subject_id} ({subject_index}/{total_subjects})\")\n",
    "            \n",
    "            # Read image and mask\n",
    "            image_path = files['image']\n",
    "            mask_path = files['mask']\n",
    "            \n",
    "            orig_image = sitk.ReadImage(image_path)\n",
    "            orig_mask = sitk.ReadImage(mask_path)\n",
    "            \n",
    "            # Process each label\n",
    "            for label in mask_labels:\n",
    "                # Create a binary mask for this label\n",
    "                mask_array = sitk.GetArrayFromImage(orig_mask)\n",
    "                binary_mask = (mask_array == label).astype(np.uint8)\n",
    "                \n",
    "                # Skip if no voxels with this label\n",
    "                if np.sum(binary_mask) == 0:\n",
    "                    logging.info(f\"Subject {subject_id}: No voxels with label {label}, skipping\")\n",
    "                    continue\n",
    "                \n",
    "                # Create a SimpleITK image from the binary mask\n",
    "                binary_mask_image = sitk.GetImageFromArray(binary_mask)\n",
    "                binary_mask_image.CopyInformation(orig_mask)\n",
    "                \n",
    "                # Save the binary mask temporarily\n",
    "                label_mask_path = os.path.join(temp_dir, f\"{subject_id}_label_{label}.nii.gz\")\n",
    "                sitk.WriteImage(binary_mask_image, label_mask_path)\n",
    "                \n",
    "                # Extract features\n",
    "                try:\n",
    "                    result = extractor.execute(image_path, label_mask_path)\n",
    "                    \n",
    "                    # Convert to regular dictionary and remove diagnostic keys\n",
    "                    features = {str(key): value for key, value in result.items() \n",
    "                               if not key.startswith('diagnostics_')}\n",
    "                    \n",
    "                    # Add subject ID and target value if available\n",
    "                    features['subject_id'] = subject_id\n",
    "                    if target_dict and subject_id in target_dict:\n",
    "                        features['target'] = target_dict[subject_id]\n",
    "                    \n",
    "                    # Add to the corresponding label's feature list\n",
    "                    all_features_by_label[label].append(features)\n",
    "                    \n",
    "                    logging.info(f\"Subject {subject_id}: Extracted {len(features)} features for label {label}\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    logging.error(f\"Error extracting features for subject {subject_id}, label {label}: {str(e)}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error processing subject {subject_id}: {str(e)}\")\n",
    "    \n",
    "    def _process_subject_parallel(\n",
    "        self, \n",
    "        subject_id,\n",
    "        image_path,\n",
    "        mask_path, \n",
    "        mask_labels, \n",
    "        params_file,\n",
    "        temp_dir,\n",
    "        target_value,\n",
    "        subject_index,\n",
    "        total_subjects\n",
    "    ):\n",
    "        \"\"\"Process a single subject in parallel mode\"\"\"\n",
    "        try:\n",
    "            # Initialize local extractor\n",
    "            extractor = featureextractor.RadiomicsFeatureExtractor(params_file)\n",
    "            \n",
    "            # Read image and mask\n",
    "            orig_image = sitk.ReadImage(image_path)\n",
    "            orig_mask = sitk.ReadImage(mask_path)\n",
    "            \n",
    "            # Initialize features by label\n",
    "            features_by_label = {label: None for label in mask_labels}\n",
    "            \n",
    "            # Process each label\n",
    "            for label in mask_labels:\n",
    "                # Create a binary mask for this label\n",
    "                mask_array = sitk.GetArrayFromImage(orig_mask)\n",
    "                binary_mask = (mask_array == label).astype(np.uint8)\n",
    "                \n",
    "                # Skip if no voxels with this label\n",
    "                if np.sum(binary_mask) == 0:\n",
    "                    print(f\"Subject {subject_id}: No voxels with label {label}, skipping\")\n",
    "                    continue\n",
    "                \n",
    "                # Create a SimpleITK image from the binary mask\n",
    "                binary_mask_image = sitk.GetImageFromArray(binary_mask)\n",
    "                binary_mask_image.CopyInformation(orig_mask)\n",
    "                \n",
    "                # Save the binary mask temporarily\n",
    "                label_mask_path = os.path.join(temp_dir, f\"{subject_id}_label_{label}.nii.gz\")\n",
    "                sitk.WriteImage(binary_mask_image, label_mask_path)\n",
    "                \n",
    "                # Extract features\n",
    "                try:\n",
    "                    result = extractor.execute(image_path, label_mask_path)\n",
    "                    \n",
    "                    # Convert to regular dictionary and remove diagnostic keys\n",
    "                    features = {str(key): value for key, value in result.items() \n",
    "                               if not key.startswith('diagnostics_')}\n",
    "                    \n",
    "                    # Add subject ID and target value if available\n",
    "                    features['subject_id'] = subject_id\n",
    "                    if target_value is not None:\n",
    "                        features['target'] = target_value\n",
    "                    \n",
    "                    # Store features for this label\n",
    "                    features_by_label[label] = features\n",
    "                    \n",
    "                    print(f\"Subject {subject_id}: Extracted {len(features)} features for label {label}\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Error extracting features for subject {subject_id}, label {label}: {str(e)}\")\n",
    "            \n",
    "            return subject_id, features_by_label\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing subject {subject_id}: {str(e)}\")\n",
    "            return subject_id, {}\n",
    "\n",
    "pyradiomics_feature_extraction_tool = PyRadiomicsFeatureExtractionTool()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploratory Data Analysis Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EDAToolException(Exception):\n",
    "    \"\"\"Custom exception for EDA Tool errors.\"\"\"\n",
    "    pass\n",
    "\n",
    "class ExploratoryDataAnalysisTool(Tool):\n",
    "    name = \"exploratory_data_analysis\"\n",
    "    description = \"\"\"\n",
    "    This tool performs comprehensive exploratory data analysis on tabulated data (Excel or CSV).\n",
    "    It analyzes the data structure, generates statistics, creates visualizations, and produces reports.\n",
    "    Results are saved to a specified output directory for further inspection.\n",
    "    \"\"\"\n",
    "    \n",
    "    inputs = {\n",
    "        \"input_path\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"Path to input data file (CSV or Excel)\"\n",
    "        },\n",
    "        \"output_dir\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"Output directory where analysis results will be saved\"\n",
    "        },\n",
    "        \"sheet_name\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"Sheet name for Excel files (ignored for CSV files)\",\n",
    "            \"required\": False,\n",
    "            \"nullable\": True\n",
    "        },\n",
    "        \"target_column\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"Target column for analyzing relationships (e.g., for classification/regression)\",\n",
    "            \"required\": False,\n",
    "            \"nullable\": True\n",
    "        },\n",
    "        \"categorical_threshold\": {\n",
    "            \"type\": \"integer\",\n",
    "            \"description\": \"Maximum number of unique values to consider a column categorical\",\n",
    "            \"required\": False,\n",
    "            \"nullable\": True\n",
    "        },\n",
    "        \"correlation_method\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"Method for correlation calculation ('pearson', 'spearman', or 'kendall')\",\n",
    "            \"required\": False,\n",
    "            \"nullable\": True\n",
    "        },\n",
    "        \"visualize_distributions\": {\n",
    "            \"type\": \"boolean\",\n",
    "            \"description\": \"Generate distribution plots for numerical columns\",\n",
    "            \"required\": False,\n",
    "            \"nullable\": True\n",
    "        },\n",
    "        \"visualize_correlations\": {\n",
    "            \"type\": \"boolean\",\n",
    "            \"description\": \"Generate correlation heatmap\",\n",
    "            \"required\": False,\n",
    "            \"nullable\": True\n",
    "        },\n",
    "        \"visualize_pairplot\": {\n",
    "            \"type\": \"boolean\",\n",
    "            \"description\": \"Generate pairplot for numerical columns\",\n",
    "            \"required\": False,\n",
    "            \"nullable\": True\n",
    "        },\n",
    "        \"visualize_target_relationships\": {\n",
    "            \"type\": \"boolean\",\n",
    "            \"description\": \"Generate plots showing relationships with target column\",\n",
    "            \"required\": False,\n",
    "            \"nullable\": True\n",
    "        },\n",
    "        \"max_categories_pie\": {\n",
    "            \"type\": \"integer\",\n",
    "            \"description\": \"Maximum number of categories to display in pie charts\",\n",
    "            \"required\": False,\n",
    "            \"nullable\": True\n",
    "        },\n",
    "        \"sampling_for_large_data\": {\n",
    "            \"type\": \"boolean\", \n",
    "            \"description\": \"Sample data if it's too large for visualization\",\n",
    "            \"required\": False,\n",
    "            \"nullable\": True\n",
    "        },\n",
    "        \"sample_size\": {\n",
    "            \"type\": \"integer\",\n",
    "            \"description\": \"Number of rows to sample for large datasets\",\n",
    "            \"required\": False,\n",
    "            \"nullable\": True\n",
    "        },\n",
    "        \"time_series_analysis\": {\n",
    "            \"type\": \"boolean\",\n",
    "            \"description\": \"Perform time series analysis if datetime columns are detected\",\n",
    "            \"required\": False,\n",
    "            \"nullable\": True\n",
    "        },\n",
    "        \"columns_to_exclude\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"Comma-separated list of columns to exclude from analysis\",\n",
    "            \"required\": False,\n",
    "            \"nullable\": True\n",
    "        },\n",
    "        \"detect_outliers\": {\n",
    "            \"type\": \"boolean\",\n",
    "            \"description\": \"Detect and analyze outliers in numerical columns\",\n",
    "            \"required\": False,\n",
    "            \"nullable\": True\n",
    "        },\n",
    "        \"create_summary_report\": {\n",
    "            \"type\": \"boolean\",\n",
    "            \"description\": \"Create a comprehensive summary report in text format\",\n",
    "            \"required\": False,\n",
    "            \"nullable\": True\n",
    "        },\n",
    "        \"max_columns_for_correlation\": {\n",
    "            \"type\": \"integer\",\n",
    "            \"description\": \"Maximum number of columns to include in correlation matrix\",\n",
    "            \"required\": False,\n",
    "            \"nullable\": True\n",
    "        },\n",
    "        \"max_columns_for_pairplot\": {\n",
    "            \"type\": \"integer\",\n",
    "            \"description\": \"Maximum number of columns to include in pairplot\",\n",
    "            \"required\": False,\n",
    "            \"nullable\": True\n",
    "        },\n",
    "        \"create_figures\": {\n",
    "            \"type\": \"boolean\",\n",
    "            \"description\": \"Master switch to enable/disable all visualizations\",\n",
    "            \"required\": False,\n",
    "            \"nullable\": True\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    output_type = \"object\"\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_path: str,\n",
    "        output_dir: str,\n",
    "        sheet_name: Optional[str] = None,\n",
    "        target_column: Optional[str] = None,\n",
    "        categorical_threshold: Optional[int] = 10,\n",
    "        correlation_method: Optional[str] = \"pearson\",\n",
    "        visualize_distributions: Optional[bool] = True,\n",
    "        visualize_correlations: Optional[bool] = True,\n",
    "        visualize_pairplot: Optional[bool] = True,\n",
    "        visualize_target_relationships: Optional[bool] = True,\n",
    "        max_categories_pie: Optional[int] = 10,\n",
    "        sampling_for_large_data: Optional[bool] = True,\n",
    "        sample_size: Optional[int] = 10000,\n",
    "        time_series_analysis: Optional[bool] = True,\n",
    "        columns_to_exclude: Optional[str] = None,\n",
    "        detect_outliers: Optional[bool] = True,\n",
    "        create_summary_report: Optional[bool] = True,\n",
    "        max_columns_for_correlation: Optional[int] = 100,\n",
    "        max_columns_for_pairplot: Optional[int] = 10,\n",
    "        create_figures: Optional[bool] = True\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Perform exploratory data analysis on tabulated data and save results.\n",
    "        \n",
    "        Args:\n",
    "            input_path: Path to input data file (CSV or Excel)\n",
    "            output_dir: Output directory where analysis results will be saved\n",
    "            sheet_name: Sheet name for Excel files\n",
    "            target_column: Target column for analyzing relationships\n",
    "            categorical_threshold: Max unique values to consider a column categorical\n",
    "            correlation_method: Method for correlation calculation\n",
    "            visualize_distributions: Generate distribution plots\n",
    "            visualize_correlations: Generate correlation heatmap\n",
    "            visualize_pairplot: Generate pairplot for numerical columns\n",
    "            visualize_target_relationships: Generate plots showing relationships with target\n",
    "            max_categories_pie: Maximum categories to display in pie charts\n",
    "            sampling_for_large_data: Sample data if it's too large\n",
    "            sample_size: Number of rows to sample for large datasets\n",
    "            time_series_analysis: Perform time series analysis if possible\n",
    "            columns_to_exclude: Columns to exclude from analysis\n",
    "            detect_outliers: Detect and analyze outliers\n",
    "            create_summary_report: Create a comprehensive summary report\n",
    "            max_columns_for_correlation: Maximum number of columns to include in correlation matrix\n",
    "            max_columns_for_pairplot: Maximum number of columns to include in pairplot\n",
    "            create_figures: Master switch to enable/disable all visualizations\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with analysis results and file paths\n",
    "        \"\"\"\n",
    "        # Override visualization settings if create_figures is False\n",
    "        if not create_figures:\n",
    "            visualize_distributions = False\n",
    "            visualize_correlations = False\n",
    "            visualize_pairplot = False\n",
    "            visualize_target_relationships = False\n",
    "            \n",
    "        try:\n",
    "            # Create output directory if it doesn't exist\n",
    "            os.makedirs(output_dir, exist_ok=True)\n",
    "            \n",
    "            # Set up logging\n",
    "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            log_file = os.path.join(output_dir, f\"eda_log_{timestamp}.txt\")\n",
    "            \n",
    "            with open(log_file, 'w') as f:\n",
    "                f.write(f\"EDA started at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "                f.write(f\"Input file: {input_path}\\n\")\n",
    "                f.write(f\"Output directory: {output_dir}\\n\\n\")\n",
    "            \n",
    "            # Load data\n",
    "            self._log(log_file, \"Loading data...\")\n",
    "            df = self._load_data(input_path, sheet_name)\n",
    "            self._log(log_file, f\"Data loaded successfully. Shape: {df.shape}\")\n",
    "            \n",
    "            # Process columns to exclude\n",
    "            excluded_columns = []\n",
    "            if columns_to_exclude:\n",
    "                excluded_columns = [col.strip() for col in columns_to_exclude.split(',')]\n",
    "                df = df.drop(columns=[col for col in excluded_columns if col in df.columns])\n",
    "                self._log(log_file, f\"Excluded columns: {excluded_columns}\")\n",
    "            \n",
    "            # Sample data if needed\n",
    "            original_row_count = len(df)\n",
    "            if sampling_for_large_data and len(df) > sample_size:\n",
    "                self._log(log_file, f\"Sampling data from {len(df)} to {sample_size} rows for visualization\")\n",
    "                df_sampled = df.sample(sample_size, random_state=42)\n",
    "            else:\n",
    "                df_sampled = df\n",
    "            \n",
    "            # Basic data profiling\n",
    "            self._log(log_file, \"Performing basic data profiling...\")\n",
    "            profile_results = self._profile_data(df)\n",
    "            profile_path = os.path.join(output_dir, \"data_profile.json\")\n",
    "            with open(profile_path, 'w') as f:\n",
    "                json.dump(profile_results, f, indent=2, default=str)\n",
    "            \n",
    "            # Save data summary\n",
    "            summary_stats_path = os.path.join(output_dir, \"summary_statistics.txt\")\n",
    "            with open(summary_stats_path, 'w') as f:\n",
    "                f.write(\"=== DATA SUMMARY ===\\n\\n\")\n",
    "                f.write(f\"Total rows: {len(df)}\\n\")\n",
    "                f.write(f\"Total columns: {len(df.columns)}\\n\\n\")\n",
    "                \n",
    "                f.write(\"=== COLUMN INFORMATION ===\\n\\n\")\n",
    "                for col in df.columns:\n",
    "                    f.write(f\"Column: {col}\\n\")\n",
    "                    f.write(f\"  Type: {df[col].dtype}\\n\")\n",
    "                    f.write(f\"  Missing values: {df[col].isna().sum()} ({(df[col].isna().sum() / len(df)) * 100:.2f}%)\\n\")\n",
    "                    \n",
    "                    if is_numeric_dtype(df[col]):\n",
    "                        f.write(f\"  Min: {df[col].min()}\\n\")\n",
    "                        f.write(f\"  Max: {df[col].max()}\\n\")\n",
    "                        f.write(f\"  Mean: {df[col].mean()}\\n\")\n",
    "                        f.write(f\"  Median: {df[col].median()}\\n\")\n",
    "                        f.write(f\"  Std: {df[col].std()}\\n\")\n",
    "                    \n",
    "                    unique_count = df[col].nunique()\n",
    "                    f.write(f\"  Unique values: {unique_count}\\n\")\n",
    "                    \n",
    "                    if unique_count <= categorical_threshold or is_categorical_dtype(df[col]):\n",
    "                        f.write(\"  Value counts:\\n\")\n",
    "                        for val, count in df[col].value_counts().head(10).items():\n",
    "                            f.write(f\"    {val}: {count} ({(count / len(df)) * 100:.2f}%)\\n\")\n",
    "                        if unique_count > 10:\n",
    "                            f.write(f\"    ... and {unique_count - 10} more values\\n\")\n",
    "                    \n",
    "                    f.write(\"\\n\")\n",
    "            \n",
    "            # Perform visualizations\n",
    "            generated_files = []\n",
    "            \n",
    "            # Create a figures directory\n",
    "            figures_dir = os.path.join(output_dir, \"figures\")\n",
    "            os.makedirs(figures_dir, exist_ok=True)\n",
    "            \n",
    "            # Detect column types\n",
    "            numeric_cols = df_sampled.select_dtypes(include=['number']).columns.tolist()\n",
    "            categorical_cols = [col for col in df_sampled.columns if col not in numeric_cols \n",
    "                                or df_sampled[col].nunique() <= categorical_threshold]\n",
    "            datetime_cols = [col for col in df_sampled.columns if is_datetime64_any_dtype(df_sampled[col])]\n",
    "            \n",
    "            # Visualize distributions\n",
    "            if visualize_distributions:\n",
    "                self._log(log_file, \"Generating distribution visualizations...\")\n",
    "                dist_files = self._visualize_distributions(df_sampled, figures_dir, \n",
    "                                                          categorical_threshold, max_categories_pie)\n",
    "                generated_files.extend(dist_files)\n",
    "                \n",
    "            # Visualize correlations\n",
    "            if visualize_correlations and len(numeric_cols) > 1:\n",
    "                self._log(log_file, \"Generating correlation visualizations...\")\n",
    "                try:\n",
    "                    # Limit columns for correlation if specified\n",
    "                    if max_columns_for_correlation and len(numeric_cols) > max_columns_for_correlation:\n",
    "                        self._log(log_file, f\"Too many numeric columns ({len(numeric_cols)}) for correlation matrix. Limiting to top {max_columns_for_correlation}.\")\n",
    "                        numeric_cols_corr = numeric_cols[:max_columns_for_correlation]\n",
    "                    else:\n",
    "                        numeric_cols_corr = numeric_cols\n",
    "                        \n",
    "                    corr_file = self._visualize_correlations(df_sampled[numeric_cols_corr], figures_dir, correlation_method)\n",
    "                    if corr_file:\n",
    "                        generated_files.append(corr_file)\n",
    "                except Exception as e:\n",
    "                    self._log(log_file, f\"Error generating correlation visualization: {str(e)}\")\n",
    "                \n",
    "            # Visualize pairplot\n",
    "            if visualize_pairplot and len(numeric_cols) > 1:\n",
    "                self._log(log_file, \"Generating pairplot...\")\n",
    "                try:\n",
    "                    # Limit columns for pairplot\n",
    "                    num_cols_for_pairplot = min(len(numeric_cols), max_columns_for_pairplot if max_columns_for_pairplot else 10)\n",
    "                    if len(numeric_cols) > num_cols_for_pairplot:\n",
    "                        self._log(log_file, f\"Too many numeric columns ({len(numeric_cols)}) for pairplot. Limiting to top {num_cols_for_pairplot}.\")\n",
    "                        numeric_cols_subset = numeric_cols[:num_cols_for_pairplot]\n",
    "                    else:\n",
    "                        numeric_cols_subset = numeric_cols\n",
    "                        \n",
    "                    pairplot_file = self._visualize_pairplot(df_sampled, numeric_cols_subset, figures_dir, target_column)\n",
    "                    if pairplot_file:\n",
    "                        generated_files.append(pairplot_file)\n",
    "                except Exception as e:\n",
    "                    self._log(log_file, f\"Error generating pairplot: {str(e)}\")\n",
    "                \n",
    "            # Visualize relationships with target if requested\n",
    "            if visualize_target_relationships and visualize_distributions and target_column and target_column in df.columns:\n",
    "                self._log(log_file, f\"Generating visualizations for relationships with target: {target_column}\")\n",
    "                target_files = self._visualize_target_relationships(df_sampled, target_column, figures_dir, \n",
    "                                                                   categorical_threshold)\n",
    "                generated_files.extend(target_files)\n",
    "                \n",
    "            # Perform time series analysis if requested and datetime columns exist\n",
    "            if time_series_analysis and datetime_cols and visualize_distributions:\n",
    "                self._log(log_file, f\"Performing time series analysis on columns: {datetime_cols}\")\n",
    "                time_series_files = self._time_series_analysis(df, datetime_cols, figures_dir)\n",
    "                generated_files.extend(time_series_files)\n",
    "                \n",
    "            # Detect outliers\n",
    "            outlier_info = None\n",
    "            if detect_outliers and numeric_cols:\n",
    "                self._log(log_file, \"Detecting outliers in numerical columns...\")\n",
    "                outlier_info = self._detect_outliers(df, numeric_cols)\n",
    "                outlier_path = os.path.join(output_dir, \"outliers.json\")\n",
    "                with open(outlier_path, 'w') as f:\n",
    "                    json.dump(outlier_info, f, indent=2, default=str)\n",
    "                \n",
    "                # Visualize outliers only if visualizations are enabled\n",
    "                if visualize_distributions:\n",
    "                    outlier_files = self._visualize_outliers(df_sampled, numeric_cols, figures_dir)\n",
    "                    generated_files.extend(outlier_files)\n",
    "                \n",
    "            # Create summary report\n",
    "            report_path = None\n",
    "            if create_summary_report:\n",
    "                self._log(log_file, \"Creating summary report...\")\n",
    "                report_path = self._create_summary_report(df, profile_results, outlier_info, \n",
    "                                                        generated_files, output_dir)\n",
    "            \n",
    "            self._log(log_file, \"EDA completed successfully.\")\n",
    "            \n",
    "            return {\n",
    "                \"status\": \"success\",\n",
    "                \"input_path\": input_path,\n",
    "                \"output_dir\": output_dir,\n",
    "                \"profile_path\": profile_path,\n",
    "                \"summary_stats_path\": summary_stats_path,\n",
    "                \"generated_files\": generated_files,\n",
    "                \"report_path\": report_path if create_summary_report else None,\n",
    "                \"row_count\": original_row_count,\n",
    "                \"column_count\": len(df.columns) + len(excluded_columns),\n",
    "                \"numeric_columns\": numeric_cols,\n",
    "                \"categorical_columns\": categorical_cols,\n",
    "                \"datetime_columns\": datetime_cols,\n",
    "                \"has_missing_data\": df.isna().any().any()\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            self._log(log_file, f\"Error: {str(e)}\")\n",
    "            return {\n",
    "                \"status\": \"error\",\n",
    "                \"error_message\": str(e),\n",
    "                \"input_path\": input_path,\n",
    "                \"output_dir\": output_dir\n",
    "            }\n",
    "    \n",
    "    def _load_data(self, input_path: str, sheet_name: Optional[str] = None) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Load data from CSV or Excel file.\n",
    "        \n",
    "        Args:\n",
    "            input_path: Path to the input file\n",
    "            sheet_name: Sheet name for Excel files\n",
    "            \n",
    "        Returns:\n",
    "            Pandas DataFrame containing the data\n",
    "        \"\"\"\n",
    "        if not os.path.exists(input_path):\n",
    "            raise EDAToolException(f\"Input file not found: {input_path}\")\n",
    "        \n",
    "        file_extension = os.path.splitext(input_path)[1].lower()\n",
    "        \n",
    "        try:\n",
    "            if file_extension in ['.csv', '.txt']:\n",
    "                # Try different encodings and delimiters for CSV\n",
    "                try:\n",
    "                    df = pd.read_csv(input_path)\n",
    "                except:\n",
    "                    try:\n",
    "                        df = pd.read_csv(input_path, sep=';')\n",
    "                    except:\n",
    "                        try:\n",
    "                            df = pd.read_csv(input_path, encoding='latin1')\n",
    "                        except:\n",
    "                            df = pd.read_csv(input_path, encoding='latin1', sep=';')\n",
    "            \n",
    "            elif file_extension in ['.xlsx', '.xls']:\n",
    "                if sheet_name:\n",
    "                    df = pd.read_excel(input_path, sheet_name=sheet_name)\n",
    "                else:\n",
    "                    df = pd.read_excel(input_path)\n",
    "            else:\n",
    "                raise EDAToolException(f\"Unsupported file format: {file_extension}\")\n",
    "            \n",
    "            # Convert date columns to datetime\n",
    "            for col in df.columns:\n",
    "                if df[col].dtype == 'object':\n",
    "                    try:\n",
    "                        pd.to_datetime(df[col], errors='raise')\n",
    "                        df[col] = pd.to_datetime(df[col])\n",
    "                    except:\n",
    "                        pass\n",
    "            \n",
    "            return df\n",
    "        \n",
    "        except Exception as e:\n",
    "            raise EDAToolException(f\"Error loading data: {str(e)}\")\n",
    "    \n",
    "    def _profile_data(self, df: pd.DataFrame) -> Dict:\n",
    "        \"\"\"\n",
    "        Generate a profile of the data.\n",
    "        \n",
    "        Args:\n",
    "            df: Pandas DataFrame to profile\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary containing profile information\n",
    "        \"\"\"\n",
    "        profile = {\n",
    "            \"row_count\": len(df),\n",
    "            \"column_count\": len(df.columns),\n",
    "            \"memory_usage\": df.memory_usage(deep=True).sum(),\n",
    "            \"duplicated_rows\": int(df.duplicated().sum()),\n",
    "            \"columns\": {},\n",
    "            \"missing_data\": {\n",
    "                \"total_missing_cells\": int(df.isna().sum().sum()),\n",
    "                \"total_cells\": df.size,\n",
    "                \"missing_percentage\": float((df.isna().sum().sum() / df.size) * 100)\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        for col in df.columns:\n",
    "            col_profile = {\n",
    "                \"dtype\": str(df[col].dtype),\n",
    "                \"is_numeric\": is_numeric_dtype(df[col]),\n",
    "                \"missing_count\": int(df[col].isna().sum()),\n",
    "                \"missing_percentage\": float((df[col].isna().sum() / len(df)) * 100),\n",
    "                \"unique_count\": int(df[col].nunique())\n",
    "            }\n",
    "            \n",
    "            if is_numeric_dtype(df[col]):\n",
    "                col_profile.update({\n",
    "                    \"min\": float(df[col].min()) if not pd.isna(df[col].min()) else None,\n",
    "                    \"max\": float(df[col].max()) if not pd.isna(df[col].max()) else None,\n",
    "                    \"mean\": float(df[col].mean()) if not pd.isna(df[col].mean()) else None,\n",
    "                    \"median\": float(df[col].median()) if not pd.isna(df[col].median()) else None,\n",
    "                    \"std\": float(df[col].std()) if not pd.isna(df[col].std()) else None,\n",
    "                    \"skewness\": float(df[col].skew()) if not pd.isna(df[col].skew()) else None,\n",
    "                    \"kurtosis\": float(df[col].kurtosis()) if not pd.isna(df[col].kurtosis()) else None\n",
    "                })\n",
    "            \n",
    "            # For categorical or low-cardinality columns, include value counts\n",
    "            if col_profile[\"unique_count\"] <= 20 or not col_profile[\"is_numeric\"]:\n",
    "                value_counts = df[col].value_counts().head(20).to_dict()\n",
    "                col_profile[\"value_counts\"] = {str(k): int(v) for k, v in value_counts.items()}\n",
    "            \n",
    "            profile[\"columns\"][col] = col_profile\n",
    "        \n",
    "        return profile\n",
    "    \n",
    "    def _visualize_distributions(self, df: pd.DataFrame, output_dir: str, \n",
    "                               categorical_threshold: int, max_categories_pie: int) -> List[str]:\n",
    "        \"\"\"\n",
    "        Create visualizations for the distributions of each column.\n",
    "        \n",
    "        Args:\n",
    "            df: DataFrame to visualize\n",
    "            output_dir: Directory to save visualizations\n",
    "            categorical_threshold: Maximum number of unique values to consider categorical\n",
    "            max_categories_pie: Maximum number of categories to display in pie charts\n",
    "            \n",
    "        Returns:\n",
    "            List of paths to generated visualization files\n",
    "        \"\"\"\n",
    "        generated_files = []\n",
    "        \n",
    "        # Identify column types\n",
    "        numeric_cols = df.select_dtypes(include=['number']).columns.tolist()\n",
    "        \n",
    "        # Histograms for numeric columns\n",
    "        if numeric_cols:\n",
    "            for col in numeric_cols:\n",
    "                plt.figure(figsize=(12, 6))\n",
    "                \n",
    "                # Histogram with KDE\n",
    "                sns.histplot(df[col].dropna(), kde=True)\n",
    "                plt.title(f'Distribution of {col}')\n",
    "                plt.xlabel(col)\n",
    "                plt.ylabel('Frequency')\n",
    "                plt.grid(True, alpha=0.3)\n",
    "                \n",
    "                # Add basic statistics as text\n",
    "                if len(df[col].dropna()) > 0:\n",
    "                    stats_text = (\n",
    "                        f\"Mean: {df[col].mean():.2f}\\n\"\n",
    "                        f\"Median: {df[col].median():.2f}\\n\"\n",
    "                        f\"Std Dev: {df[col].std():.2f}\\n\"\n",
    "                        f\"Min: {df[col].min():.2f}\\n\"\n",
    "                        f\"Max: {df[col].max():.2f}\"\n",
    "                    )\n",
    "                    plt.annotate(stats_text, xy=(0.95, 0.95), xycoords='axes fraction',\n",
    "                                bbox=dict(boxstyle=\"round,pad=0.5\", fc=\"white\", alpha=0.8),\n",
    "                                va='top', ha='right')\n",
    "                \n",
    "                file_path = os.path.join(output_dir, f\"dist_{col.replace(' ', '_')}.png\")\n",
    "                plt.tight_layout()\n",
    "                plt.savefig(file_path)\n",
    "                plt.close()\n",
    "                generated_files.append(file_path)\n",
    "                \n",
    "                # Box plot\n",
    "                plt.figure(figsize=(12, 6))\n",
    "                sns.boxplot(x=df[col].dropna())\n",
    "                plt.title(f'Box Plot of {col}')\n",
    "                plt.xlabel(col)\n",
    "                plt.grid(True, alpha=0.3)\n",
    "                \n",
    "                file_path = os.path.join(output_dir, f\"boxplot_{col.replace(' ', '_')}.png\")\n",
    "                plt.tight_layout()\n",
    "                plt.savefig(file_path)\n",
    "                plt.close()\n",
    "                generated_files.append(file_path)\n",
    "        \n",
    "        # Bar charts or pie charts for categorical columns\n",
    "        for col in df.columns:\n",
    "            if col in numeric_cols and df[col].nunique() > categorical_threshold:\n",
    "                continue\n",
    "                \n",
    "            value_counts = df[col].value_counts()\n",
    "            if len(value_counts) <= max_categories_pie:\n",
    "                # Pie chart for fewer categories\n",
    "                plt.figure(figsize=(12, 8))\n",
    "                plt.pie(value_counts, labels=value_counts.index, autopct='%1.1f%%', \n",
    "                      startangle=90, shadow=True)\n",
    "                plt.axis('equal')\n",
    "                plt.title(f'Distribution of {col}')\n",
    "                \n",
    "                file_path = os.path.join(output_dir, f\"pie_{col.replace(' ', '_')}.png\")\n",
    "                plt.tight_layout()\n",
    "                plt.savefig(file_path)\n",
    "                plt.close()\n",
    "                generated_files.append(file_path)\n",
    "            else:\n",
    "                # Bar chart for more categories (limit to top categories)\n",
    "                plt.figure(figsize=(14, 8))\n",
    "                top_categories = value_counts.head(max_categories_pie)\n",
    "                sns.barplot(x=top_categories.index, y=top_categories.values)\n",
    "                plt.title(f'Top {max_categories_pie} Categories of {col}')\n",
    "                plt.xlabel(col)\n",
    "                plt.ylabel('Count')\n",
    "                plt.xticks(rotation=45, ha='right')\n",
    "                \n",
    "                file_path = os.path.join(output_dir, f\"bar_{col.replace(' ', '_')}.png\")\n",
    "                plt.tight_layout()\n",
    "                plt.savefig(file_path)\n",
    "                plt.close()\n",
    "                generated_files.append(file_path)\n",
    "                \n",
    "        # Missing values visualization\n",
    "        missing_data = df.isna().sum()\n",
    "        if missing_data.sum() > 0:\n",
    "            plt.figure(figsize=(14, 8))\n",
    "            sns.barplot(x=missing_data.index, y=missing_data.values)\n",
    "            plt.title('Missing Values by Column')\n",
    "            plt.xlabel('Column')\n",
    "            plt.ylabel('Missing Values Count')\n",
    "            plt.xticks(rotation=45, ha='right')\n",
    "            \n",
    "            file_path = os.path.join(output_dir, \"missing_values.png\")\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(file_path)\n",
    "            plt.close()\n",
    "            generated_files.append(file_path)\n",
    "        \n",
    "        return generated_files\n",
    "    \n",
    "    def _visualize_correlations(self, df: pd.DataFrame, output_dir: str, \n",
    "                              correlation_method: str) -> str:\n",
    "        \"\"\"\n",
    "        Create correlation heatmap.\n",
    "        \n",
    "        Args:\n",
    "            df: DataFrame to visualize\n",
    "            output_dir: Directory to save visualizations\n",
    "            correlation_method: Method for correlation calculation\n",
    "            \n",
    "        Returns:\n",
    "            Path to the generated correlation heatmap file\n",
    "        \"\"\"\n",
    "        # Get only numeric columns for correlation\n",
    "        numeric_df = df.select_dtypes(include=['number'])\n",
    "        \n",
    "        if numeric_df.shape[1] < 2:\n",
    "            return None\n",
    "            \n",
    "        # If we have too many columns, limit to the most relevant ones\n",
    "        if numeric_df.shape[1] > 100:\n",
    "            # Log warning about limiting columns\n",
    "            print(f\"Warning: Too many numeric columns ({numeric_df.shape[1]}) for correlation visualization. Limiting to top 100.\")\n",
    "            \n",
    "            # One approach: Use columns with most non-null values\n",
    "            non_null_counts = numeric_df.count()\n",
    "            top_cols = non_null_counts.nlargest(100).index.tolist()\n",
    "            numeric_df = numeric_df[top_cols]\n",
    "        \n",
    "        # Calculate correlation matrix\n",
    "        corr_matrix = numeric_df.corr(method=correlation_method)\n",
    "        \n",
    "        # For large correlation matrices, break into chunks\n",
    "        if numeric_df.shape[1] > 50:\n",
    "            # Split into chunks of 50 columns max\n",
    "            chunk_size = 50\n",
    "            chunks = []\n",
    "            \n",
    "            for i in range(0, len(corr_matrix.columns), chunk_size):\n",
    "                chunk_cols = corr_matrix.columns[i:i+chunk_size]\n",
    "                chunks.append(corr_matrix.loc[chunk_cols, chunk_cols])\n",
    "            \n",
    "            # Create correlation heatmaps for each chunk\n",
    "            file_paths = []\n",
    "            for i, chunk in enumerate(chunks):\n",
    "                plt.figure(figsize=(20, 16))\n",
    "                mask = np.triu(np.ones_like(chunk, dtype=bool))\n",
    "                cmap = sns.diverging_palette(230, 20, as_cmap=True)\n",
    "                \n",
    "                sns.heatmap(chunk, mask=mask, cmap=cmap, vmax=1, vmin=-1, center=0,\n",
    "                          annot=True, fmt=\".2f\", square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\n",
    "                \n",
    "                plt.title(f'Correlation Matrix ({correlation_method.capitalize()}) - Part {i+1}')\n",
    "                file_path = os.path.join(output_dir, f\"correlation_{correlation_method}_part{i+1}.png\")\n",
    "                plt.tight_layout()\n",
    "                plt.savefig(file_path)\n",
    "                plt.close()\n",
    "                file_paths.append(file_path)\n",
    "            \n",
    "            return file_paths[0]  # Return first file path\n",
    "        else:\n",
    "            # Original code for smaller correlation matrices\n",
    "            plt.figure(figsize=(max(12, len(numeric_df.columns)), max(10, len(numeric_df.columns))))\n",
    "            mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "            cmap = sns.diverging_palette(230, 20, as_cmap=True)\n",
    "            \n",
    "            sns.heatmap(corr_matrix, mask=mask, cmap=cmap, vmax=1, vmin=-1, center=0,\n",
    "                      annot=True, fmt=\".2f\", square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\n",
    "            \n",
    "            plt.title(f'Correlation Matrix ({correlation_method.capitalize()})')\n",
    "            file_path = os.path.join(output_dir, f\"correlation_{correlation_method}.png\")\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(file_path)\n",
    "            plt.close\n",
    "            \n",
    "            return file_path\n",
    "    \n",
    "    def _visualize_pairplot(self, df: pd.DataFrame, numeric_cols: List[str], \n",
    "                          output_dir: str, target_column: Optional[str] = None) -> str:\n",
    "        \"\"\"\n",
    "        Create pairplot for numerical columns.\n",
    "        \n",
    "        Args:\n",
    "            df: DataFrame to visualize\n",
    "            numeric_cols: List of numeric columns to include\n",
    "            output_dir: Directory to save visualization\n",
    "            target_column: Target column for coloring (if applicable)\n",
    "            \n",
    "        Returns:\n",
    "            Path to the generated pairplot file\n",
    "        \"\"\"\n",
    "        # Limit the number of columns for pairplot to avoid performance issues\n",
    "        if len(numeric_cols) > 10:\n",
    "            print(f\"Warning: Too many numeric columns ({len(numeric_cols)}) for pairplot. Limiting to 10 columns.\")\n",
    "            numeric_cols = numeric_cols[:10]\n",
    "            \n",
    "        plot_df = df[numeric_cols].copy()\n",
    "        \n",
    "        # If target column exists and is categorical (for coloring)\n",
    "        hue = None\n",
    "        if target_column and target_column in df.columns:\n",
    "            if df[target_column].nunique() <= 10:  # Limit to reasonable number of categories\n",
    "                plot_df[target_column] = df[target_column]\n",
    "                hue = target_column\n",
    "                \n",
    "        try:\n",
    "            # Create pairplot\n",
    "            plt.figure(figsize=(12, 10))\n",
    "            g = sns.pairplot(plot_df, hue=hue, diag_kind='kde', \n",
    "                            plot_kws={'alpha': 0.6}, diag_kws={'alpha': 0.6})\n",
    "            \n",
    "            plt.suptitle('Pairplot of Numerical Features', y=1.02, fontsize=16)\n",
    "            file_path = os.path.join(output_dir, \"pairplot.png\")\n",
    "            plt.savefig(file_path)\n",
    "            plt.close()\n",
    "            \n",
    "            return file_path\n",
    "        except Exception as e:\n",
    "            print(f\"Error creating pairplot: {str(e)}\")\n",
    "            # Create alternative correlation plot if pairplot fails\n",
    "            return self._visualize_correlations(plot_df, output_dir, \"pearson\")\n",
    "    \n",
    "    def _visualize_target_relationships(self, df: pd.DataFrame, target_column: str,\n",
    "                                      output_dir: str, categorical_threshold: int) -> List[str]:\n",
    "        \"\"\"\n",
    "        Create visualizations showing relationships between features and target.\n",
    "        \n",
    "        Args:\n",
    "            df: DataFrame to visualize\n",
    "            target_column: Target column for relationship analysis\n",
    "            output_dir: Directory to save visualizations\n",
    "            categorical_threshold: Maximum number of unique values to consider categorical\n",
    "            \n",
    "        Returns:\n",
    "            List of paths to generated visualization files\n",
    "        \"\"\"\n",
    "        if target_column not in df.columns:\n",
    "            return []\n",
    "            \n",
    "        generated_files = []\n",
    "        numeric_cols = df.select_dtypes(include=['number']).columns.tolist()\n",
    "        \n",
    "        # Determine if target is categorical or numerical\n",
    "        target_is_categorical = df[target_column].nunique() <= categorical_threshold or not is_numeric_dtype(df[target_column])\n",
    "        \n",
    "        for col in df.columns:\n",
    "            if col == target_column:\n",
    "                continue\n",
    "                \n",
    "            # Check if feature is categorical\n",
    "            col_is_categorical = col not in numeric_cols or df[col].nunique() <= categorical_threshold\n",
    "            \n",
    "            # Case 1: Categorical target vs. Numerical feature\n",
    "            if target_is_categorical and not col_is_categorical:\n",
    "                plt.figure(figsize=(14, 8))\n",
    "                sns.boxplot(x=target_column, y=col, data=df)\n",
    "                plt.title(f'Distribution of {col} by {target_column}')\n",
    "                plt.xticks(rotation=45, ha='right')\n",
    "                \n",
    "                file_path = os.path.join(output_dir, f\"target_box_{col.replace(' ', '_')}.png\")\n",
    "                plt.tight_layout()\n",
    "                plt.savefig(file_path)\n",
    "                plt.close()\n",
    "                generated_files.append(file_path)\n",
    "                \n",
    "                # Add violin plot for more detail\n",
    "                plt.figure(figsize=(14, 8))\n",
    "                sns.violinplot(x=target_column, y=col, data=df)\n",
    "                plt.title(f'Violin Plot of {col} by {target_column}')\n",
    "                plt.xticks(rotation=45, ha='right')\n",
    "                \n",
    "                file_path = os.path.join(output_dir, f\"target_violin_{col.replace(' ', '_')}.png\")\n",
    "                plt.tight_layout()\n",
    "                plt.savefig(file_path)\n",
    "                plt.close()\n",
    "                generated_files.append(file_path)\n",
    "                \n",
    "            # Case 2: Numerical target vs. Numerical feature\n",
    "            elif not target_is_categorical and not col_is_categorical:\n",
    "                plt.figure(figsize=(12, 8))\n",
    "                sns.scatterplot(x=col, y=target_column, data=df, alpha=0.6)\n",
    "                \n",
    "                # Add regression line\n",
    "                try:\n",
    "                    sns.regplot(x=col, y=target_column, data=df, scatter=False, line_kws={\"color\": \"red\"})\n",
    "                except:\n",
    "                    pass\n",
    "                    \n",
    "                plt.title(f'Relationship between {col} and {target_column}')\n",
    "                \n",
    "                file_path = os.path.join(output_dir, f\"target_scatter_{col.replace(' ', '_')}.png\")\n",
    "                plt.tight_layout()\n",
    "                plt.savefig(file_path)\n",
    "                plt.close()\n",
    "                generated_files.append(file_path)\n",
    "                \n",
    "            # Case 3: Categorical target vs. Categorical feature\n",
    "            elif target_is_categorical and col_is_categorical:\n",
    "                # Create grouped bar chart or heatmap\n",
    "                plt.figure(figsize=(14, 10))\n",
    "                \n",
    "                # Create a crosstab\n",
    "                cross_tab = pd.crosstab(df[col], df[target_column], normalize='index')\n",
    "                cross_tab.plot(kind='bar', stacked=True)\n",
    "                \n",
    "                plt.title(f'Relationship between {col} and {target_column}')\n",
    "                plt.xlabel(col)\n",
    "                plt.ylabel('Proportion')\n",
    "                plt.xticks(rotation=45, ha='right')\n",
    "                plt.legend(title=target_column)\n",
    "                \n",
    "                file_path = os.path.join(output_dir, f\"target_bar_{col.replace(' ', '_')}.png\")\n",
    "                plt.tight_layout()\n",
    "                plt.savefig(file_path)\n",
    "                plt.close()\n",
    "                generated_files.append(file_path)\n",
    "                \n",
    "                # Heatmap of association\n",
    "                plt.figure(figsize=(12, 8))\n",
    "                cross_tab_counts = pd.crosstab(df[col], df[target_column])\n",
    "                sns.heatmap(cross_tab_counts, annot=True, fmt='d', cmap='Blues')\n",
    "                plt.title(f'Heatmap of {col} vs {target_column}')\n",
    "                plt.tight_layout()\n",
    "                \n",
    "                file_path = os.path.join(output_dir, f\"target_heatmap_{col.replace(' ', '_')}.png\")\n",
    "                plt.savefig(file_path)\n",
    "                plt.close()\n",
    "                generated_files.append(file_path)\n",
    "                \n",
    "            # Case 4: Numerical target vs. Categorical feature\n",
    "            elif not target_is_categorical and col_is_categorical:\n",
    "                plt.figure(figsize=(14, 8))\n",
    "                sns.boxplot(x=col, y=target_column, data=df)\n",
    "                plt.title(f'Distribution of {target_column} by {col}')\n",
    "                plt.xticks(rotation=45, ha='right')\n",
    "                \n",
    "                file_path = os.path.join(output_dir, f\"target_revbox_{col.replace(' ', '_')}.png\")\n",
    "                plt.tight_layout()\n",
    "                plt.savefig(file_path)\n",
    "                plt.close()\n",
    "                generated_files.append(file_path)\n",
    "        \n",
    "        return generated_files\n",
    "    \n",
    "    def _time_series_analysis(self, df: pd.DataFrame, datetime_cols: List[str], \n",
    "                            output_dir: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        Perform time series analysis for datetime columns.\n",
    "        \n",
    "        Args:\n",
    "            df: DataFrame to analyze\n",
    "            datetime_cols: List of datetime columns to analyze\n",
    "            output_dir: Directory to save visualizations\n",
    "            \n",
    "        Returns:\n",
    "            List of paths to generated visualization files\n",
    "        \"\"\"\n",
    "        generated_files = []\n",
    "        \n",
    "        for date_col in datetime_cols:\n",
    "            # Ensure column is datetime type\n",
    "            if not is_datetime64_any_dtype(df[date_col]):\n",
    "                try:\n",
    "                    df[date_col] = pd.to_datetime(df[date_col])\n",
    "                except:\n",
    "                    continue\n",
    "            \n",
    "            # Sort by date\n",
    "            df_sorted = df.sort_values(by=date_col)\n",
    "            \n",
    "            # Check if there are numeric columns to plot\n",
    "            numeric_cols = df.select_dtypes(include=['number']).columns.tolist()\n",
    "            \n",
    "            if not numeric_cols:\n",
    "                continue\n",
    "                \n",
    "            # For each numeric column, create time series plot\n",
    "            for num_col in numeric_cols[:5]:  # Limit to first 5 numeric columns\n",
    "                plt.figure(figsize=(16, 6))\n",
    "                \n",
    "                # Plot time series\n",
    "                plt.plot(df_sorted[date_col], df_sorted[num_col])\n",
    "                plt.title(f'Time Series of {num_col} by {date_col}')\n",
    "                plt.xlabel(date_col)\n",
    "                plt.ylabel(num_col)\n",
    "                plt.grid(True, alpha=0.3)\n",
    "                \n",
    "                # Format x-axis dates\n",
    "                plt.gcf().autofmt_xdate()\n",
    "                \n",
    "                file_path = os.path.join(output_dir, f\"timeseries_{date_col.replace(' ', '_')}_{num_col.replace(' ', '_')}.png\")\n",
    "                plt.tight_layout()\n",
    "                plt.savefig(file_path)\n",
    "                plt.close()\n",
    "                generated_files.append(file_path)\n",
    "                \n",
    "            # Create count plot by time periods\n",
    "            plt.figure(figsize=(16, 6))\n",
    "            \n",
    "            # Extract date components and count by period\n",
    "            if len(df) > 1000:\n",
    "                # For large datasets, aggregate by month\n",
    "                date_counts = df[date_col].dt.to_period('M').value_counts().sort_index()\n",
    "                date_counts.index = date_counts.index.astype(str)\n",
    "            else:\n",
    "                # For smaller datasets, aggregate by day\n",
    "                date_counts = df[date_col].dt.date.value_counts().sort_index()\n",
    "            \n",
    "            plt.bar(date_counts.index, date_counts.values)\n",
    "            plt.title(f'Counts by {date_col}')\n",
    "            plt.xlabel(date_col)\n",
    "            plt.ylabel('Count')\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            \n",
    "            # Format x-axis\n",
    "            plt.xticks(rotation=45, ha='right')\n",
    "            \n",
    "            file_path = os.path.join(output_dir, f\"datecount_{date_col.replace(' ', '_')}.png\")\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(file_path)\n",
    "            plt.close()\n",
    "            generated_files.append(file_path)\n",
    "        \n",
    "        return generated_files\n",
    "    \n",
    "    def _detect_outliers(self, df: pd.DataFrame, numeric_cols: List[str]) -> Dict:\n",
    "        \"\"\"\n",
    "        Detect outliers in numerical columns.\n",
    "        \n",
    "        Args:\n",
    "            df: DataFrame to analyze\n",
    "            numeric_cols: List of numeric columns to check for outliers\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary containing outlier information\n",
    "        \"\"\"\n",
    "        outlier_info = {}\n",
    "        \n",
    "        for col in numeric_cols:\n",
    "            # Skip columns with all NaN values\n",
    "            if df[col].isna().all():\n",
    "                continue\n",
    "                \n",
    "            # Calculate IQR\n",
    "            Q1 = df[col].quantile(0.25)\n",
    "            Q3 = df[col].quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "            \n",
    "            # Define outlier bounds\n",
    "            lower_bound = Q1 - 1.5 * IQR\n",
    "            upper_bound = Q3 + 1.5 * IQR\n",
    "            \n",
    "            # Find outliers\n",
    "            outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)][col]\n",
    "            \n",
    "            outlier_info[col] = {\n",
    "                \"Q1\": float(Q1),\n",
    "                \"Q3\": float(Q3),\n",
    "                \"IQR\": float(IQR),\n",
    "                \"lower_bound\": float(lower_bound),\n",
    "                \"upper_bound\": float(upper_bound),\n",
    "                \"outlier_count\": int(len(outliers)),\n",
    "                \"outlier_percentage\": float((len(outliers) / df[col].count()) * 100),\n",
    "                \"min_outlier\": float(outliers.min()) if not outliers.empty else None,\n",
    "                \"max_outlier\": float(outliers.max()) if not outliers.empty else None\n",
    "            }\n",
    "            \n",
    "        return outlier_info\n",
    "    \n",
    "    def _visualize_outliers(self, df: pd.DataFrame, numeric_cols: List[str], \n",
    "                          output_dir: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        Create visualizations for outlier detection.\n",
    "        \n",
    "        Args:\n",
    "            df: DataFrame to visualize\n",
    "            numeric_cols: List of numeric columns to analyze\n",
    "            output_dir: Directory to save visualizations\n",
    "            \n",
    "        Returns:\n",
    "            List of paths to generated visualization files\n",
    "        \"\"\"\n",
    "        generated_files = []\n",
    "        \n",
    "        for col in numeric_cols:\n",
    "            # Skip columns with all NaN values\n",
    "            if df[col].isna().all():\n",
    "                continue\n",
    "                \n",
    "            # Create a subplot with 1 row and 2 columns\n",
    "            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 6))\n",
    "            \n",
    "            # Box plot for outlier visualization\n",
    "            sns.boxplot(x=df[col], ax=ax1)\n",
    "            ax1.set_title(f'Box Plot with Outliers: {col}')\n",
    "            ax1.grid(True, alpha=0.3)\n",
    "            \n",
    "            # Histogram with KDE for distribution with outliers\n",
    "            sns.histplot(df[col].dropna(), kde=True, ax=ax2)\n",
    "            ax2.set_title(f'Distribution with Outliers: {col}')\n",
    "            ax2.grid(True, alpha=0.3)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            file_path = os.path.join(output_dir, f\"outliers_{col.replace(' ', '_')}.png\")\n",
    "            plt.savefig(file_path)\n",
    "            plt.close(fig)  # Close the figure explicitly\n",
    "            generated_files.append(file_path)\n",
    "            \n",
    "        return generated_files\n",
    "    \n",
    "    def _create_summary_report(self, df: pd.DataFrame, profile_results: Dict, \n",
    "                             outlier_info: Dict, generated_files: List[str],\n",
    "                             output_dir: str) -> str:\n",
    "        \"\"\"\n",
    "        Create a comprehensive summary report.\n",
    "        \n",
    "        Args:\n",
    "            df: DataFrame analyzed\n",
    "            profile_results: Data profile information\n",
    "            outlier_info: Outlier detection results\n",
    "            generated_files: List of generated visualization files\n",
    "            output_dir: Directory to save the report\n",
    "            \n",
    "        Returns:\n",
    "            Path to the generated report file\n",
    "        \"\"\"\n",
    "        report_path = os.path.join(output_dir, \"eda_summary_report.txt\")\n",
    "        \n",
    "        with open(report_path, 'w') as f:\n",
    "            f.write(\"============================================\\n\")\n",
    "            f.write(\"    EXPLORATORY DATA ANALYSIS REPORT        \\n\")\n",
    "            f.write(\"============================================\\n\\n\")\n",
    "            \n",
    "            f.write(f\"Report generated on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\\n\")\n",
    "            \n",
    "            f.write(\"1. DATASET OVERVIEW\\n\")\n",
    "            f.write(\"===================\\n\\n\")\n",
    "            f.write(f\"Total rows: {len(df)}\\n\")\n",
    "            f.write(f\"Total columns: {len(df.columns)}\\n\")\n",
    "            f.write(f\"Memory usage: {profile_results['memory_usage'] / (1024*1024):.2f} MB\\n\")\n",
    "            f.write(f\"Duplicate rows: {profile_results['duplicated_rows']}\\n\")\n",
    "            f.write(f\"Missing cells: {profile_results['missing_data']['total_missing_cells']} ({profile_results['missing_data']['missing_percentage']:.2f}%)\\n\\n\")\n",
    "            \n",
    "            f.write(\"2. COLUMN SUMMARY\\n\")\n",
    "            f.write(\"=================\\n\\n\")\n",
    "            \n",
    "            # Numeric columns\n",
    "            numeric_cols = df.select_dtypes(include=['number']).columns.tolist()\n",
    "            f.write(f\"Numeric columns ({len(numeric_cols)}):\\n\")\n",
    "            for col in numeric_cols:\n",
    "                f.write(f\"  - {col}\\n\")\n",
    "            f.write(\"\\n\")\n",
    "            \n",
    "            # Categorical columns (including low-cardinality numeric)\n",
    "            cat_cols = [col for col in df.columns if col not in numeric_cols or df[col].nunique() <= 10]\n",
    "            f.write(f\"Categorical columns ({len(cat_cols)}):\\n\")\n",
    "            for col in cat_cols:\n",
    "                f.write(f\"  - {col}\\n\")\n",
    "            f.write(\"\\n\")\n",
    "            \n",
    "            # Datetime columns\n",
    "            datetime_cols = [col for col in df.columns if is_datetime64_any_dtype(df[col])]\n",
    "            if datetime_cols:\n",
    "                f.write(f\"Datetime columns ({len(datetime_cols)}):\\n\")\n",
    "                for col in datetime_cols:\n",
    "                    f.write(f\"  - {col}\\n\")\n",
    "                f.write(\"\\n\")\n",
    "            \n",
    "            # Columns with missing values\n",
    "            missing_cols = [col for col in df.columns if df[col].isna().any()]\n",
    "            if missing_cols:\n",
    "                f.write(f\"Columns with missing values ({len(missing_cols)}):\\n\")\n",
    "                for col in missing_cols:\n",
    "                    missing_count = df[col].isna().sum()\n",
    "                    missing_percent = (missing_count / len(df)) * 100\n",
    "                    f.write(f\"  - {col}: {missing_count} ({missing_percent:.2f}%)\\n\")\n",
    "                f.write(\"\\n\")\n",
    "            \n",
    "            f.write(\"3. STATISTICAL SUMMARY\\n\")\n",
    "            f.write(\"=====================\\n\\n\")\n",
    "            \n",
    "            # Add numeric column statistics\n",
    "            for col in numeric_cols:\n",
    "                profile = profile_results['columns'].get(col, {})\n",
    "                f.write(f\"Column: {col}\\n\")\n",
    "                f.write(f\"  - Type: {profile.get('dtype', 'Unknown')}\\n\")\n",
    "                f.write(f\"  - Missing: {profile.get('missing_count', 'N/A')} ({profile.get('missing_percentage', 'N/A'):.2f}%)\\n\")\n",
    "                f.write(f\"  - Unique values: {profile.get('unique_count', 'N/A')}\\n\")\n",
    "                f.write(f\"  - Min: {profile.get('min', 'N/A')}\\n\")\n",
    "                f.write(f\"  - Max: {profile.get('max', 'N/A')}\\n\")\n",
    "                f.write(f\"  - Mean: {profile.get('mean', 'N/A')}\\n\")\n",
    "                f.write(f\"  - Median: {profile.get('median', 'N/A')}\\n\")\n",
    "                f.write(f\"  - Std Dev: {profile.get('std', 'N/A')}\\n\")\n",
    "                f.write(f\"  - Skewness: {profile.get('skewness', 'N/A')}\\n\")\n",
    "                f.write(f\"  - Kurtosis: {profile.get('kurtosis', 'N/A')}\\n\")\n",
    "                f.write(\"\\n\")\n",
    "            \n",
    "            # Add categorical column statistics\n",
    "            for col in cat_cols:\n",
    "                if col in numeric_cols:\n",
    "                    continue  # Skip numeric columns already covered\n",
    "                    \n",
    "                profile = profile_results['columns'].get(col, {})\n",
    "                f.write(f\"Column: {col}\\n\")\n",
    "                f.write(f\"  - Type: {profile.get('dtype', 'Unknown')}\\n\")\n",
    "                f.write(f\"  - Missing: {profile.get('missing_count', 'N/A')} ({profile.get('missing_percentage', 'N/A'):.2f}%)\\n\")\n",
    "                f.write(f\"  - Unique values: {profile.get('unique_count', 'N/A')}\\n\")\n",
    "                \n",
    "                value_counts = profile.get('value_counts', {})\n",
    "                if value_counts:\n",
    "                    f.write(\"  - Top categories:\\n\")\n",
    "                    for val, count in list(value_counts.items())[:10]:\n",
    "                        percent = (count / len(df)) * 100\n",
    "                        f.write(f\"    * {val}: {count} ({percent:.2f}%)\\n\")\n",
    "                f.write(\"\\n\")\n",
    "            \n",
    "            # Add outlier information\n",
    "            if outlier_info:\n",
    "                f.write(\"4. OUTLIER DETECTION\\n\")\n",
    "                f.write(\"===================\\n\\n\")\n",
    "                \n",
    "                for col, info in outlier_info.items():\n",
    "                    f.write(f\"Column: {col}\\n\")\n",
    "                    f.write(f\"  - IQR: {info['IQR']:.2f}\\n\")\n",
    "                    f.write(f\"  - Lower bound: {info['lower_bound']:.2f}\\n\")\n",
    "                    f.write(f\"  - Upper bound: {info['upper_bound']:.2f}\\n\")\n",
    "                    f.write(f\"  - Outlier count: {info['outlier_count']} ({info['outlier_percentage']:.2f}%)\\n\")\n",
    "                    if info['outlier_count'] > 0:\n",
    "                        f.write(f\"  - Min outlier: {info['min_outlier']}\\n\")\n",
    "                        f.write(f\"  - Max outlier: {info['max_outlier']}\\n\")\n",
    "                    f.write(\"\\n\")\n",
    "            \n",
    "            # Generated visualizations summary\n",
    "            f.write(\"5. GENERATED VISUALIZATIONS\\n\")\n",
    "            f.write(\"==========================\\n\\n\")\n",
    "            \n",
    "            # Group visualizations by type\n",
    "            viz_types = {\n",
    "                \"Distributions\": [f for f in generated_files if \"dist_\" in os.path.basename(f)],\n",
    "                \"Box plots\": [f for f in generated_files if \"boxplot_\" in os.path.basename(f)],\n",
    "                \"Pie charts\": [f for f in generated_files if \"pie_\" in os.path.basename(f)],\n",
    "                \"Bar charts\": [f for f in generated_files if \"bar_\" in os.path.basename(f) and not \"target_\" in os.path.basename(f)],\n",
    "                \"Correlation\": [f for f in generated_files if \"correlation_\" in os.path.basename(f)],\n",
    "                \"Pairplot\": [f for f in generated_files if \"pairplot\" in os.path.basename(f)],\n",
    "                \"Target relationships\": [f for f in generated_files if \"target_\" in os.path.basename(f)],\n",
    "                \"Time series\": [f for f in generated_files if \"timeseries_\" in os.path.basename(f) or \"datecount_\" in os.path.basename(f)],\n",
    "                \"Outliers\": [f for f in generated_files if \"outliers_\" in os.path.basename(f)],\n",
    "                \"Missing values\": [f for f in generated_files if \"missing_values\" in os.path.basename(f)]\n",
    "            }\n",
    "            \n",
    "            for viz_type, files in viz_types.items():\n",
    "                if files:\n",
    "                    f.write(f\"{viz_type} ({len(files)}):\\n\")\n",
    "                    for file_path in files:\n",
    "                        f.write(f\"  - {os.path.basename(file_path)}\\n\")\n",
    "                    f.write(\"\\n\")\n",
    "            \n",
    "            # Final insights\n",
    "            f.write(\"6. KEY INSIGHTS\\n\")\n",
    "            f.write(\"==============\\n\\n\")\n",
    "            \n",
    "            # Missing data insights\n",
    "            if profile_results['missing_data']['total_missing_cells'] > 0:\n",
    "                missing_pct = profile_results['missing_data']['missing_percentage']\n",
    "                if missing_pct > 20:\n",
    "                    f.write(\"- High level of missing data detected (>20%). Consider imputation strategies.\\n\")\n",
    "                elif missing_pct > 5:\n",
    "                    f.write(\"- Moderate level of missing data detected (5-20%). Review columns with missing values.\\n\")\n",
    "                else:\n",
    "                    f.write(\"- Low level of missing data detected (<5%).\\n\")\n",
    "            else:\n",
    "                f.write(\"- No missing data detected.\\n\")\n",
    "            \n",
    "            # Outlier insights\n",
    "            if outlier_info:\n",
    "                outlier_cols = [col for col, info in outlier_info.items() if info['outlier_percentage'] > 5]\n",
    "                if outlier_cols:\n",
    "                    f.write(f\"- Significant outliers detected in {len(outlier_cols)} columns: {', '.join(outlier_cols)}.\\n\")\n",
    "                    f.write(\"  These may impact statistical models and should be investigated.\\n\")\n",
    "            \n",
    "            # Distribution insights\n",
    "            skewed_cols = [col for col in numeric_cols \n",
    "                         if 'skewness' in profile_results['columns'].get(col, {}) \n",
    "                         and abs(profile_results['columns'][col]['skewness']) > 1]\n",
    "            if skewed_cols:\n",
    "                f.write(f\"- Highly skewed distributions detected in {len(skewed_cols)} columns.\\n\")\n",
    "                f.write(\"  Consider transformations (log, sqrt, etc.) for modeling.\\n\")\n",
    "            \n",
    "            # Duplicate data insight\n",
    "            if profile_results['duplicated_rows'] > 0:\n",
    "                dup_pct = (profile_results['duplicated_rows'] / len(df)) * 100\n",
    "                f.write(f\"- {profile_results['duplicated_rows']} duplicate rows detected ({dup_pct:.2f}%).\\n\")\n",
    "            \n",
    "            f.write(\"\\n============================================\\n\")\n",
    "            f.write(\"End of report\\n\")\n",
    "        \n",
    "        return report_path\n",
    "    \n",
    "    def _log(self, log_file: str, message: str):\n",
    "        \"\"\"\n",
    "        Write a message to the log file.\n",
    "        \n",
    "        Args:\n",
    "            log_file: Path to the log file\n",
    "            message: Message to log\n",
    "        \"\"\"\n",
    "        timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        log_message = f\"[{timestamp}] {message}\\n\"\n",
    "        \n",
    "        with open(log_file, 'a') as f:\n",
    "            f.write(log_message)\n",
    "        \n",
    "        print(log_message.strip())\n",
    "\n",
    "eda_tool = ExploratoryDataAnalysisTool()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Importance and Feature Selection Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "class FeatureImportanceAnalysisTool(Tool):\n",
    "    name = \"feature_importance_analysis\"\n",
    "    description = \"\"\"\n",
    "    This tool performs feature importance analysis on tabular data for classification or regression tasks.\n",
    "    It identifies the most relevant features using various methods and outputs CSV files with selected features.\n",
    "    Optionally generates visualization plots for feature importance and data distribution.\n",
    "    \"\"\"\n",
    "    \n",
    "    inputs = {\n",
    "        \"input_path\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"Path to input data file (CSV format)\"\n",
    "        },\n",
    "        \"output_dir\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"Output directory where results will be saved\"\n",
    "        },\n",
    "        \"target_column\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"Name of the target column for prediction\"\n",
    "        },\n",
    "        \"task_type\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"Type of machine learning task ('classification' or 'regression')\",\n",
    "            \"required\": False,\n",
    "            \"nullable\": True\n",
    "        },\n",
    "        \"method\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"Feature selection method ('rf', 'f_test', 'mutual_info', 'rfe')\",\n",
    "            \"required\": False,\n",
    "            \"nullable\": True\n",
    "        },\n",
    "        \"top_features\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"Comma-separated list of top features counts to select (e.g., '10,50,100')\",\n",
    "            \"required\": False,\n",
    "            \"nullable\": True\n",
    "        },\n",
    "        \"encode_categorical\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"Method to encode categorical features ('auto', 'onehot', 'label', 'target', 'none')\",\n",
    "            \"required\": False,\n",
    "            \"nullable\": True\n",
    "        },\n",
    "        \"max_onehot_cardinality\": {\n",
    "            \"type\": \"integer\",\n",
    "            \"description\": \"Maximum unique values for one-hot encoding if encode_categorical='auto'\",\n",
    "            \"required\": False,\n",
    "            \"nullable\": True\n",
    "        },\n",
    "        \"create_plots\": {\n",
    "            \"type\": \"boolean\",\n",
    "            \"description\": \"Whether to generate visualization plots\",\n",
    "            \"required\": False,\n",
    "            \"nullable\": True\n",
    "        },\n",
    "        \"top_n_plot\": {\n",
    "            \"type\": \"integer\",\n",
    "            \"description\": \"Number of top features to show in importance plots\",\n",
    "            \"required\": False,\n",
    "            \"nullable\": True\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    output_type = \"object\"\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_path: str,\n",
    "        output_dir: str,\n",
    "        target_column: str,\n",
    "        task_type: Optional[str] = None,\n",
    "        method: Optional[str] = \"rf\",\n",
    "        top_features: Optional[str] = \"10,50,100\",\n",
    "        encode_categorical: Optional[str] = \"auto\",\n",
    "        max_onehot_cardinality: Optional[int] = 10,\n",
    "        create_plots: Optional[bool] = True,\n",
    "        top_n_plot: Optional[int] = 30\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Analyze feature importance for tabular data and output results.\n",
    "        \n",
    "        Args:\n",
    "            input_path: Path to input CSV file\n",
    "            output_dir: Directory to save results\n",
    "            target_column: Target column name for prediction\n",
    "            task_type: 'classification' or 'regression' (auto-detected if None)\n",
    "            method: Feature selection method ('rf', 'f_test', 'mutual_info', 'rfe')\n",
    "            top_features: Comma-separated list of top features to select\n",
    "            encode_categorical: Method for encoding categorical features\n",
    "            max_onehot_cardinality: Max unique values for one-hot encoding if auto\n",
    "            create_plots: Whether to generate visualization plots\n",
    "            top_n_plot: Number of top features to show in importance plots\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with analysis results and file paths\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Create output directory\n",
    "            os.makedirs(output_dir, exist_ok=True)\n",
    "            \n",
    "            # Set up logging file\n",
    "            log_file = os.path.join(output_dir, \"feature_importance_analysis.log\")\n",
    "            file_handler = logging.FileHandler(log_file)\n",
    "            file_handler.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - %(message)s'))\n",
    "            logging.getLogger().addHandler(file_handler)\n",
    "            \n",
    "            logging.info(f\"Starting feature importance analysis for {input_path}\")\n",
    "            logging.info(f\"Target column: {target_column}\")\n",
    "            \n",
    "            # Load data and prepare it for analysis\n",
    "            X, y, original_df, X_original, detected_task_type = self._load_and_prepare_data(\n",
    "                input_path, \n",
    "                target_column, \n",
    "                task_type,\n",
    "                encode_categorical, \n",
    "                max_onehot_cardinality\n",
    "            )\n",
    "            \n",
    "            if task_type is None:\n",
    "                task_type = detected_task_type\n",
    "                logging.info(f\"Auto-detected task type: {task_type}\")\n",
    "            \n",
    "            # Parse the list of top features counts\n",
    "            top_features_list = [int(n) for n in top_features.split(',')]\n",
    "            top_features_list.sort()  # Ensure ascending order\n",
    "            \n",
    "            # Apply feature selection based on the method\n",
    "            result_files = self._perform_feature_selection(\n",
    "                X, y, original_df, X_original,\n",
    "                output_dir, target_column, task_type,\n",
    "                method, top_features_list, create_plots, top_n_plot\n",
    "            )\n",
    "            \n",
    "            logging.info(\"Feature importance analysis completed successfully\")\n",
    "            \n",
    "            return {\n",
    "                \"status\": \"success\",\n",
    "                \"input_path\": input_path,\n",
    "                \"output_dir\": output_dir,\n",
    "                \"target_column\": target_column,\n",
    "                \"task_type\": task_type,\n",
    "                \"method\": method,\n",
    "                \"result_files\": result_files,\n",
    "                \"feature_count\": X.shape[1],\n",
    "                \"log_file\": log_file\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error in feature importance analysis: {str(e)}\", exc_info=True)\n",
    "            return {\n",
    "                \"status\": \"error\",\n",
    "                \"error_message\": str(e),\n",
    "                \"input_path\": input_path,\n",
    "                \"output_dir\": output_dir\n",
    "            }\n",
    "    \n",
    "    def _load_and_prepare_data(\n",
    "        self, \n",
    "        input_path: str, \n",
    "        target_column: str, \n",
    "        task_type: Optional[str],\n",
    "        encode_categorical: str, \n",
    "        max_onehot_cardinality: int\n",
    "    ) -> Tuple[pd.DataFrame, pd.Series, pd.DataFrame, pd.DataFrame, str]:\n",
    "        \"\"\"\n",
    "        Load data, separate features from target, and handle categorical features.\n",
    "        \n",
    "        Args:\n",
    "            input_path: Path to input CSV file\n",
    "            target_column: Target column name\n",
    "            task_type: Task type ('classification' or 'regression')\n",
    "            encode_categorical: Method to encode categorical features\n",
    "            max_onehot_cardinality: Max unique values for one-hot encoding\n",
    "            \n",
    "        Returns:\n",
    "            Tuple of (X processed, y, original dataframe, X original, detected task type)\n",
    "        \"\"\"\n",
    "        logging.info(f\"Loading data from {input_path}\")\n",
    "        \n",
    "        try:\n",
    "            # Load data\n",
    "            df = pd.read_csv(input_path)\n",
    "            \n",
    "            if target_column not in df.columns:\n",
    "                raise ValueError(f\"Target column '{target_column}' not found in the dataset\")\n",
    "            \n",
    "            # Separate features and target\n",
    "            X = df.drop(columns=[target_column])\n",
    "            y = df[target_column]\n",
    "            \n",
    "            # Detect task type if not specified\n",
    "            detected_task_type = self._detect_task_type(y) if task_type is None else task_type\n",
    "            \n",
    "            # Save original X for feature mapping\n",
    "            X_original = X.copy()\n",
    "            \n",
    "            # Handle categorical features\n",
    "            if encode_categorical != 'none':\n",
    "                X = self._encode_categorical_features(X, y, target_column, encode_categorical, max_onehot_cardinality)\n",
    "            else:\n",
    "                # Remove categorical columns if encoding is disabled\n",
    "                cat_cols = X.select_dtypes(include=['object', 'category']).columns\n",
    "                if len(cat_cols) > 0:\n",
    "                    logging.warning(f\"Removing {len(cat_cols)} categorical columns as encoding is disabled\")\n",
    "                    X = X.select_dtypes(include=[np.number])\n",
    "            \n",
    "            # Handle missing values\n",
    "            if X.isna().any().any():\n",
    "                logging.warning(f\"Dataset contains missing values. Filling with appropriate values.\")\n",
    "                # For numeric columns, fill with median\n",
    "                num_cols = X.select_dtypes(include=[np.number]).columns\n",
    "                X[num_cols] = X[num_cols].fillna(X[num_cols].median())\n",
    "                \n",
    "                # For any remaining columns, fill with mode\n",
    "                for col in X.columns:\n",
    "                    if col not in num_cols:\n",
    "                        X[col] = X[col].fillna(X[col].mode()[0] if not X[col].mode().empty else 0)\n",
    "            \n",
    "            logging.info(f\"Data loaded successfully: {X.shape[0]} rows, {X.shape[1]} features\")\n",
    "            \n",
    "            return X, y, df, X_original, detected_task_type\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error loading data: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    def _detect_task_type(self, y: pd.Series) -> str:\n",
    "        \"\"\"\n",
    "        Automatically detect whether the task is classification or regression.\n",
    "        \n",
    "        Args:\n",
    "            y: Target variable\n",
    "            \n",
    "        Returns:\n",
    "            'classification' or 'regression'\n",
    "        \"\"\"\n",
    "        # If target is object type, it's definitely classification\n",
    "        if y.dtype == 'object' or y.dtype.name == 'category':\n",
    "            return 'classification'\n",
    "        \n",
    "        # Check number of unique values relative to the length of the series\n",
    "        unique_ratio = len(y.unique()) / len(y)\n",
    "        \n",
    "        # Heuristic: if less than 5% unique values or fewer than 10 distinct values, likely classification\n",
    "        if unique_ratio < 0.05 or len(y.unique()) < 10:\n",
    "            return 'classification'\n",
    "        else:\n",
    "            return 'regression'\n",
    "    \n",
    "    def _encode_categorical_features(\n",
    "        self, \n",
    "        X: pd.DataFrame, \n",
    "        y: pd.Series, \n",
    "        target_column: str,\n",
    "        encode_method: str, \n",
    "        max_onehot_cardinality: int\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Encode categorical features using the specified method.\n",
    "        \n",
    "        Args:\n",
    "            X: Feature dataframe\n",
    "            y: Target variable\n",
    "            target_column: Name of target column (for target encoding)\n",
    "            encode_method: Encoding method\n",
    "            max_onehot_cardinality: Maximum cardinality for one-hot encoding\n",
    "            \n",
    "        Returns:\n",
    "            Processed dataframe with encoded features\n",
    "        \"\"\"\n",
    "        # Identify categorical columns\n",
    "        cat_cols = X.select_dtypes(include=['object', 'category']).columns\n",
    "        \n",
    "        if len(cat_cols) == 0:\n",
    "            logging.info(\"No categorical features found\")\n",
    "            return X\n",
    "        \n",
    "        logging.info(f\"Found {len(cat_cols)} categorical features. Encoding method: {encode_method}\")\n",
    "        \n",
    "        # Create a copy to work with\n",
    "        X_encoded = X.copy()\n",
    "        \n",
    "        # Store encoding mappings for interpretability\n",
    "        encoding_mappings = {}\n",
    "        \n",
    "        for col in cat_cols:\n",
    "            unique_count = X[col].nunique()\n",
    "            logging.info(f\"Column '{col}' has {unique_count} unique values\")\n",
    "            \n",
    "            # Determine encoding method if auto\n",
    "            method = encode_method\n",
    "            if encode_method == 'auto':\n",
    "                if unique_count <= max_onehot_cardinality:\n",
    "                    method = 'onehot'\n",
    "                else:\n",
    "                    method = 'label'\n",
    "            \n",
    "            # Apply the selected encoding method\n",
    "            if method == 'onehot':\n",
    "                logging.info(f\"Using one-hot encoding for '{col}'\")\n",
    "                # Get dummies and drop original column\n",
    "                dummies = pd.get_dummies(X[col], prefix=col, drop_first=True)\n",
    "                X_encoded = pd.concat([X_encoded, dummies], axis=1)\n",
    "                # Store mapping info\n",
    "                encoding_mappings[col] = {'method': 'onehot', 'categories': X[col].unique().tolist()}\n",
    "                \n",
    "            elif method == 'label':\n",
    "                logging.info(f\"Using label encoding for '{col}'\")\n",
    "                le = LabelEncoder()\n",
    "                X_encoded[f\"{col}_label\"] = le.fit_transform(X[col].astype(str))\n",
    "                # Store mapping\n",
    "                encoding_mappings[col] = {\n",
    "                    'method': 'label',\n",
    "                    'mapping': dict(zip(le.classes_, range(len(le.classes_))))\n",
    "                }\n",
    "                \n",
    "            elif method == 'target':\n",
    "                logging.info(f\"Using target encoding for '{col}'\")\n",
    "                # Create temporary df with target for encoding\n",
    "                temp_df = pd.concat([X[[col]], y], axis=1)\n",
    "                \n",
    "                # Target encoding (mean target value per category)\n",
    "                means = temp_df.groupby(col)[target_column].mean()\n",
    "                X_encoded[f\"{col}_target\"] = X[col].map(means)\n",
    "                \n",
    "                # Add frequency encoding as well\n",
    "                freq = X[col].value_counts(normalize=True)\n",
    "                X_encoded[f\"{col}_freq\"] = X[col].map(freq)\n",
    "                \n",
    "                # Store mapping\n",
    "                encoding_mappings[col] = {\n",
    "                    'method': 'target',\n",
    "                    'target_means': means.to_dict(),\n",
    "                    'frequency': freq.to_dict()\n",
    "                }\n",
    "        \n",
    "        # Drop original categorical columns after encoding\n",
    "        X_encoded = X_encoded.drop(columns=cat_cols)\n",
    "        \n",
    "        # Handle any NaN values from the encoding process\n",
    "        if X_encoded.isna().any().any():\n",
    "            missing_count = X_encoded.isna().sum().sum()\n",
    "            logging.warning(f\"Encoding created {missing_count} missing values. Filling with zeros.\")\n",
    "            X_encoded = X_encoded.fillna(0)\n",
    "        \n",
    "        # Save encoding mappings for interpretability\n",
    "        encoding_file = os.path.join(os.path.dirname(os.path.abspath(encoding_mappings.get('file', 'encodings.json'))), \n",
    "                                   'categorical_encodings.json')\n",
    "        with open(encoding_file, 'w') as f:\n",
    "            json.dump(encoding_mappings, f, indent=2, default=str)\n",
    "        logging.info(f\"Saved categorical encoding mappings to {encoding_file}\")\n",
    "        \n",
    "        return X_encoded\n",
    "    \n",
    "    def _perform_feature_selection(\n",
    "        self,\n",
    "        X: pd.DataFrame,\n",
    "        y: pd.Series,\n",
    "        original_df: pd.DataFrame,\n",
    "        X_original: pd.DataFrame,\n",
    "        output_dir: str,\n",
    "        target_column: str,\n",
    "        task_type: str,\n",
    "        method: str,\n",
    "        top_features_list: List[int],\n",
    "        create_plots: bool,\n",
    "        top_n_plot: int\n",
    "    ) -> Dict[str, str]:\n",
    "        \"\"\"\n",
    "        Perform feature selection using the specified method.\n",
    "        \n",
    "        Args:\n",
    "            X: Processed feature dataframe\n",
    "            y: Target variable\n",
    "            original_df: Original dataframe with all columns\n",
    "            X_original: Original features before encoding\n",
    "            output_dir: Directory to save results\n",
    "            target_column: Target column name\n",
    "            task_type: 'classification' or 'regression'\n",
    "            method: Feature selection method\n",
    "            top_features_list: List of top feature counts to select\n",
    "            create_plots: Whether to create visualization plots\n",
    "            top_n_plot: Number of top features to show in plots\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary of output file paths\n",
    "        \"\"\"\n",
    "        result_files = {}\n",
    "        \n",
    "        # Define available methods with their functions\n",
    "        if task_type == 'classification':\n",
    "            methods = {\n",
    "                'rf': self._select_features_rf_classifier,\n",
    "                'f_test': self._select_features_f_test_classifier,\n",
    "                'mutual_info': self._select_features_mutual_info_classifier,\n",
    "                'rfe': self._select_features_rfe_classifier\n",
    "            }\n",
    "        else:  # regression\n",
    "            methods = {\n",
    "                'rf': self._select_features_rf_regressor,\n",
    "                'f_test': self._select_features_f_test_regressor,\n",
    "                'mutual_info': self._select_features_mutual_info_regressor,\n",
    "                'rfe': self._select_features_rfe_regressor\n",
    "            }\n",
    "        \n",
    "        # Check if method is valid\n",
    "        if method not in methods:\n",
    "            raise ValueError(f\"Unknown feature selection method: {method}\")\n",
    "        \n",
    "        selection_method = methods[method]\n",
    "        logging.info(f\"Using {method} method for feature selection\")\n",
    "        \n",
    "        # Create feature importance file\n",
    "        importance_file = os.path.join(output_dir, 'feature_importance.csv')\n",
    "        result_files['importance'] = importance_file\n",
    "        \n",
    "        # Output files for selected features\n",
    "        selected_files = {}\n",
    "        \n",
    "        # For RFE, we need to run the algorithm separately for each n_features\n",
    "        if method == 'rfe':\n",
    "            logging.info(\"Using Recursive Feature Elimination (RFE)\")\n",
    "            selected_features_all = {}\n",
    "            \n",
    "            for n_features in top_features_list:\n",
    "                if n_features > X.shape[1]:\n",
    "                    logging.warning(f\"Requested {n_features} features, but only {X.shape[1]} are available\")\n",
    "                    n_features = X.shape[1]\n",
    "                \n",
    "                # Get selected features\n",
    "                selected_features, _ = selection_method(X, y, n_features)\n",
    "                selected_features_all[n_features] = selected_features\n",
    "                \n",
    "                # Save selected features\n",
    "                output_file = self._save_selected_features(\n",
    "                    original_df, target_column, selected_features, \n",
    "                    output_dir, n_features, X_original, X\n",
    "                )\n",
    "                selected_files[n_features] = output_file\n",
    "                \n",
    "                # Generate visualizations if requested\n",
    "                if create_plots and n_features > 1 and n_features <= 500:\n",
    "                    self._create_visualizations(X, y, selected_features, output_dir, n_features, task_type)\n",
    "            \n",
    "            # For RFE, create an importance file using RF for visualization purposes\n",
    "            if create_plots:\n",
    "                all_features, scores = (self._select_features_rf_classifier(X, y, X.shape[1]) if task_type == 'classification' \n",
    "                                     else self._select_features_rf_regressor(X, y, X.shape[1]))\n",
    "                \n",
    "                importance_df = pd.DataFrame({\n",
    "                    'feature': X.columns,\n",
    "                    'importance': scores\n",
    "                })\n",
    "                importance_df = importance_df.sort_values('importance', ascending=False)\n",
    "                importance_df.to_csv(importance_file, index=False)\n",
    "                \n",
    "                # Create visualization plots\n",
    "                self._plot_feature_importance(importance_df, output_dir, top_n_plot)\n",
    "                self._plot_cumulative_importance(importance_df, output_dir)\n",
    "                \n",
    "                # Plot correlation of top features\n",
    "                top_features = importance_df['feature'].iloc[:min(20, len(importance_df))].tolist()\n",
    "                self._plot_feature_correlation(X, top_features, output_dir)\n",
    "        \n",
    "        else:  # For other methods\n",
    "            # Get all features with importance values\n",
    "            all_features, scores = selection_method(X, y, X.shape[1])\n",
    "            \n",
    "            # Create importance dataframe\n",
    "            importance_df = pd.DataFrame({\n",
    "                'feature': X.columns,\n",
    "                'importance': scores\n",
    "            })\n",
    "            importance_df = importance_df.sort_values('importance', ascending=False)\n",
    "            importance_df.to_csv(importance_file, index=False)\n",
    "            logging.info(f\"Saved feature importance to {importance_file}\")\n",
    "            \n",
    "            # Generate visualization plots if requested\n",
    "            if create_plots:\n",
    "                logging.info(\"Generating visualization plots...\")\n",
    "                \n",
    "                # Plot feature importance\n",
    "                self._plot_feature_importance(importance_df, output_dir, top_n_plot)\n",
    "                \n",
    "                # Plot cumulative importance\n",
    "                thresholds = self._plot_cumulative_importance(importance_df, output_dir)\n",
    "                logging.info(f\"Feature threshold analysis: {thresholds}\")\n",
    "                \n",
    "                # Plot correlation of top features\n",
    "                top_features = importance_df['feature'].iloc[:min(20, len(importance_df))].tolist()\n",
    "                self._plot_feature_correlation(X, top_features, output_dir)\n",
    "            \n",
    "            # For each number of features, save a CSV and create visualizations\n",
    "            for n_features in top_features_list:\n",
    "                if n_features > X.shape[1]:\n",
    "                    logging.warning(f\"Requested {n_features} features, but only {X.shape[1]} are available\")\n",
    "                    n_features = X.shape[1]\n",
    "                \n",
    "                # Get top n_features from importance dataframe\n",
    "                selected_features = importance_df['feature'].iloc[:n_features].tolist()\n",
    "                \n",
    "                # Save selected features to CSV\n",
    "                output_file = self._save_selected_features(\n",
    "                    original_df, target_column, selected_features, \n",
    "                    output_dir, n_features, X_original, X\n",
    "                )\n",
    "                selected_files[n_features] = output_file\n",
    "                \n",
    "                # Generate PCA and t-SNE visualizations if requested\n",
    "                if create_plots and n_features > 1 and n_features <= 500:\n",
    "                    self._create_visualizations(X, y, selected_features, output_dir, n_features, task_type)\n",
    "                \n",
    "        # Add selected feature files to result\n",
    "        result_files['selected_features'] = selected_files\n",
    "        \n",
    "        return result_files\n",
    "    \n",
    "    def _select_features_rf_classifier(self, X: pd.DataFrame, y: pd.Series, n_features: int) -> Tuple[List[str], np.ndarray]:\n",
    "        \"\"\"Select features using Random Forest classifier importance\"\"\"\n",
    "        logging.info(f\"Selecting top {n_features} features using Random Forest classifier importance\")\n",
    "        \n",
    "        # Create and fit Random Forest\n",
    "        rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "        rf.fit(X, y)\n",
    "        \n",
    "        # Get feature importances\n",
    "        importances = rf.feature_importances_\n",
    "        \n",
    "        # Create indices of top features\n",
    "        indices = np.argsort(importances)[::-1][:n_features]\n",
    "        \n",
    "        return X.columns[indices].tolist(), importances\n",
    "    \n",
    "    def _select_features_rf_regressor(self, X: pd.DataFrame, y: pd.Series, n_features: int) -> Tuple[List[str], np.ndarray]:\n",
    "        \"\"\"Select features using Random Forest regressor importance\"\"\"\n",
    "        logging.info(f\"Selecting top {n_features} features using Random Forest regressor importance\")\n",
    "        \n",
    "        # Create and fit Random Forest\n",
    "        rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "        rf.fit(X, y)\n",
    "        \n",
    "        # Get feature importances\n",
    "        importances = rf.feature_importances_\n",
    "        \n",
    "        # Create indices of top features\n",
    "        indices = np.argsort(importances)[::-1][:n_features]\n",
    "        \n",
    "        return X.columns[indices].tolist(), importances\n",
    "    \n",
    "    def _select_features_f_test_classifier(self, X: pd.DataFrame, y: pd.Series, n_features: int) -> Tuple[List[str], np.ndarray]:\n",
    "        \"\"\"Select features using ANOVA F-value for classification\"\"\"\n",
    "        logging.info(f\"Selecting top {n_features} features using ANOVA F-test for classification\")\n",
    "        \n",
    "        # Apply SelectKBest with f_classif\n",
    "        selector = SelectKBest(f_classif, k=min(n_features, X.shape[1]))\n",
    "        selector.fit(X, y)\n",
    "        \n",
    "        # Get selected feature indices\n",
    "        mask = selector.get_support()\n",
    "        \n",
    "        return X.columns[mask].tolist(), selector.scores_\n",
    "    \n",
    "    def _select_features_f_test_regressor(self, X: pd.DataFrame, y: pd.Series, n_features: int) -> Tuple[List[str], np.ndarray]:\n",
    "        \"\"\"Select features using F-test for regression\"\"\"\n",
    "        logging.info(f\"Selecting top {n_features} features using F-test for regression\")\n",
    "        \n",
    "        # Apply SelectKBest with f_regression\n",
    "        selector = SelectKBest(f_regression, k=min(n_features, X.shape[1]))\n",
    "        selector.fit(X, y)\n",
    "        \n",
    "        # Get selected feature indices\n",
    "        mask = selector.get_support()\n",
    "        \n",
    "        return X.columns[mask].tolist(), selector.scores_\n",
    "    \n",
    "    def _select_features_mutual_info_classifier(self, X: pd.DataFrame, y: pd.Series, n_features: int) -> Tuple[List[str], np.ndarray]:\n",
    "        \"\"\"Select features using mutual information for classification\"\"\"\n",
    "        logging.info(f\"Selecting top {n_features} features using mutual information for classification\")\n",
    "        \n",
    "        # Apply SelectKBest with mutual_info_classif\n",
    "        selector = SelectKBest(mutual_info_classif, k=min(n_features, X.shape[1]))\n",
    "        selector.fit(X, y)\n",
    "        \n",
    "        # Get selected feature indices\n",
    "        mask = selector.get_support()\n",
    "        \n",
    "        return X.columns[mask].tolist(), selector.scores_\n",
    "    \n",
    "    def _select_features_mutual_info_regressor(self, X: pd.DataFrame, y: pd.Series, n_features: int) -> Tuple[List[str], np.ndarray]:\n",
    "        \"\"\"Select features using mutual information for regression\"\"\"\n",
    "        logging.info(f\"Selecting top {n_features} features using mutual information for regression\")\n",
    "        \n",
    "        # Apply SelectKBest with mutual_info_regression\n",
    "        selector = SelectKBest(mutual_info_regression, k=min(n_features, X.shape[1]))\n",
    "        selector.fit(X, y)\n",
    "        \n",
    "        # Get selected feature indices\n",
    "        mask = selector.get_support()\n",
    "        \n",
    "        return X.columns[mask].tolist(), selector.scores_\n",
    "    \n",
    "    def _select_features_rfe_classifier(self, X: pd.DataFrame, y: pd.Series, n_features: int) -> Tuple[List[str], np.ndarray]:\n",
    "        \"\"\"Select features using RFE with Random Forest classifier\"\"\"\n",
    "        logging.info(f\"Selecting top {n_features} features using RFE with classifier\")\n",
    "        \n",
    "        # Create RFE with RandomForestClassifier\n",
    "        estimator = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "        selector = RFE(estimator, n_features_to_select=min(n_features, X.shape[1]), step=0.1)\n",
    "        \n",
    "        # Fit RFE\n",
    "        selector.fit(X, y)\n",
    "        \n",
    "        # Get selected feature indices\n",
    "        mask = selector.get_support()\n",
    "        \n",
    "        return X.columns[mask].tolist(), selector.ranking_\n",
    "    \n",
    "    def _select_features_rfe_regressor(self, X: pd.DataFrame, y: pd.Series, n_features: int) -> Tuple[List[str], np.ndarray]:\n",
    "        \"\"\"Select features using RFE with Random Forest regressor\"\"\"\n",
    "        logging.info(f\"Selecting top {n_features} features using RFE with regressor\")\n",
    "        \n",
    "        # Create RFE with RandomForestRegressor\n",
    "        estimator = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "        selector = RFE(estimator, n_features_to_select=min(n_features, X.shape[1]), step=0.1)\n",
    "        \n",
    "        # Fit RFE\n",
    "        selector.fit(X, y)\n",
    "        \n",
    "        # Get selected feature indices\n",
    "        mask = selector.get_support()\n",
    "        \n",
    "        return X.columns[mask].tolist(), selector.ranking_\n",
    "    \n",
    "    def _save_selected_features(\n",
    "        self, \n",
    "        original_df: pd.DataFrame, \n",
    "        target_column: str, \n",
    "        selected_features: List[str],\n",
    "        output_dir: str, \n",
    "        n_features: int,\n",
    "        X_original: Optional[pd.DataFrame] = None,\n",
    "        X_encoded: Optional[pd.DataFrame] = None\n",
    "    ) -> str:\n",
    "        \"\"\"\n",
    "        Save selected features to CSV file with interpretation of encoded features.\n",
    "        \n",
    "        Args:\n",
    "            original_df: Original dataframe with all columns\n",
    "            target_column: Target column name\n",
    "            selected_features: List of selected feature names\n",
    "            output_dir: Directory to save output\n",
    "            n_features: Number of features selected\n",
    "            X_original: Original features before encoding\n",
    "            X_encoded: Encoded features\n",
    "            \n",
    "        Returns:\n",
    "            Path to the output CSV file\n",
    "        \"\"\"\n",
    "        output_file = os.path.join(output_dir, f'top_{n_features}_features.csv')\n",
    "        meta_file = os.path.join(output_dir, f'top_{n_features}_features_metadata.json')\n",
    "        \n",
    "        # Create feature metadata for interpretability\n",
    "        if X_encoded is not None and X_original is not None:\n",
    "            feature_metadata = {}\n",
    "            \n",
    "            for feature in selected_features:\n",
    "                # Check if this is an encoded feature\n",
    "                if '_' in feature and any(col in feature for col in X_original.columns):\n",
    "                    # This is likely an encoded feature\n",
    "                    # Extract original column name and encoding method\n",
    "                    \n",
    "                    # Try to find the original column name\n",
    "                    possible_orig_cols = []\n",
    "                    for col in X_original.columns:\n",
    "                        if feature.startswith(col + '_'):\n",
    "                            possible_orig_cols.append(col)\n",
    "                    \n",
    "                    if possible_orig_cols:\n",
    "                        # Sort by length to get the longest match (most specific)\n",
    "                        orig_col = sorted(possible_orig_cols, key=len, reverse=True)[0]\n",
    "                        encoding_type = feature[len(orig_col)+1:]\n",
    "                        \n",
    "                        # Store metadata about the encoding\n",
    "                        feature_metadata[feature] = {\n",
    "                            'original_column': orig_col,\n",
    "                            'encoding_type': encoding_type\n",
    "                        }\n",
    "            \n",
    "            # Save feature metadata for interpretability\n",
    "            if feature_metadata:\n",
    "                with open(meta_file, 'w') as f:\n",
    "                    json.dump(feature_metadata, f, indent=2)\n",
    "                logging.info(f\"Saved feature metadata to {meta_file}\")\n",
    "        \n",
    "        # Prepare data to save\n",
    "        try:\n",
    "            # Include target column in selected features\n",
    "            columns_to_save = selected_features.copy()\n",
    "            if target_column not in columns_to_save:\n",
    "                columns_to_save.append(target_column)\n",
    "            \n",
    "            # Check if all columns exist in the original dataframe\n",
    "            missing_cols = [col for col in columns_to_save if col not in original_df.columns]\n",
    "            \n",
    "            if missing_cols:\n",
    "                # Some selected features are encoded and not in original_df\n",
    "                logging.info(f\"Creating dataset with encoded features: {', '.join(missing_cols)}\")\n",
    "                \n",
    "                # Start with features that exist in original_df\n",
    "                existing_cols = [col for col in columns_to_save if col in original_df.columns]\n",
    "                selected_df = original_df[existing_cols].copy()\n",
    "                \n",
    "                # Add encoded features from X_encoded\n",
    "                for col in missing_cols:\n",
    "                    if col in X_encoded.columns:\n",
    "                        selected_df[col] = X_encoded[col]\n",
    "                    else:\n",
    "                        logging.warning(f\"Feature {col} not found in encoded or original data\")\n",
    "            else:\n",
    "                # All selected features are in the original dataframe\n",
    "                selected_df = original_df[columns_to_save].copy()\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error creating selected features dataset: {str(e)}\")\n",
    "            # Fallback: just save the selected features from X_encoded with the target\n",
    "            selected_df = pd.DataFrame()\n",
    "            for col in selected_features:\n",
    "                if col in X_encoded.columns:\n",
    "                    selected_df[col] = X_encoded[col]\n",
    "            selected_df[target_column] = original_df[target_column].reset_index(drop=True)\n",
    "        \n",
    "        # Save to CSV\n",
    "        selected_df.to_csv(output_file, index=False)\n",
    "        logging.info(f\"Saved top {n_features} features to {output_file}\")\n",
    "        \n",
    "        return output_file\n",
    "    \n",
    "    def _create_visualizations(\n",
    "        self, \n",
    "        X: pd.DataFrame, \n",
    "        y: pd.Series,\n",
    "        selected_features: List[str],\n",
    "        output_dir: str,\n",
    "        n_features: int,\n",
    "        task_type: str\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Create visualization plots for the selected features.\n",
    "        \n",
    "        Args:\n",
    "            X: Feature dataframe\n",
    "            y: Target variable\n",
    "            selected_features: List of selected feature names\n",
    "            output_dir: Directory to save output\n",
    "            n_features: Number of features selected\n",
    "            task_type: 'classification' or 'regression'\n",
    "        \"\"\"\n",
    "        # Create plots directory\n",
    "        plots_dir = os.path.join(output_dir, \"plots\")\n",
    "        os.makedirs(plots_dir, exist_ok=True)\n",
    "        \n",
    "        # Generate PCA visualization\n",
    "        self._plot_pca_visualization(X, y, selected_features, plots_dir, n_features, task_type)\n",
    "        \n",
    "        # Generate t-SNE visualization for smaller datasets\n",
    "        if len(X) <= 5000 or n_features <= 100:\n",
    "            self._plot_tsne_visualization(X, y, selected_features, plots_dir, n_features, task_type)\n",
    "        else:\n",
    "            logging.info(f\"Skipping t-SNE for {n_features} features due to dataset size\")\n",
    "    \n",
    "    def _plot_feature_importance(self, importance_df: pd.DataFrame, output_dir: str, top_n: int = 30):\n",
    "        \"\"\"\n",
    "        Create a bar plot of feature importance for the top N features.\n",
    "        \n",
    "        Args:\n",
    "            importance_df: DataFrame with feature names and importance scores\n",
    "            output_dir: Directory to save the plot\n",
    "            top_n: Number of top features to show\n",
    "        \"\"\"\n",
    "        plots_dir = os.path.join(output_dir, \"plots\")\n",
    "        os.makedirs(plots_dir, exist_ok=True)\n",
    "        \n",
    "        plt.figure(figsize=(12, 8))\n",
    "        \n",
    "        # Use only top N features for readability\n",
    "        plot_df = importance_df.head(min(top_n, len(importance_df)))\n",
    "        \n",
    "        # Create horizontal bar plot\n",
    "        sns.barplot(x='importance', y='feature', data=plot_df)\n",
    "        \n",
    "        # Customize plot\n",
    "        plt.title(f'Top {len(plot_df)} Feature Importance', fontsize=16)\n",
    "        plt.xlabel('Importance Score', fontsize=14)\n",
    "        plt.ylabel('Features', fontsize=14)\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save plot\n",
    "        importance_plot_file = os.path.join(plots_dir, 'feature_importance_plot.png')\n",
    "        plt.savefig(importance_plot_file, dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        logging.info(f\"Saved feature importance plot to {importance_plot_file}\")\n",
    "    \n",
    "    def _plot_cumulative_importance(self, importance_df: pd.DataFrame, output_dir: str) -> Dict[float, int]:\n",
    "        \"\"\"\n",
    "        Create a plot of cumulative feature importance.\n",
    "        \n",
    "        Args:\n",
    "            importance_df: DataFrame with feature names and importance scores\n",
    "            output_dir: Directory to save the plot\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary mapping importance thresholds to number of features\n",
    "        \"\"\"\n",
    "        plots_dir = os.path.join(output_dir, \"plots\")\n",
    "        os.makedirs(plots_dir, exist_ok=True)\n",
    "        \n",
    "        plt.figure(figsize=(10, 6))\n",
    "        \n",
    "        # Calculate cumulative importance\n",
    "        importance_df = importance_df.sort_values('importance', ascending=False).reset_index(drop=True)\n",
    "        cumulative_importance = importance_df['importance'].cumsum() / importance_df['importance'].sum()\n",
    "        \n",
    "        # Plot\n",
    "        plt.plot(range(1, len(cumulative_importance) + 1), cumulative_importance, 'b-')\n",
    "        plt.xlabel('Number of Features', fontsize=14)\n",
    "        plt.ylabel('Cumulative Importance', fontsize=14)\n",
    "        plt.title('Cumulative Feature Importance', fontsize=16)\n",
    "        \n",
    "        # Add horizontal lines at common thresholds\n",
    "        thresholds = [0.8, 0.9, 0.95, 0.99]\n",
    "        colors = ['r', 'g', 'orange', 'purple']\n",
    "        for threshold, color in zip(thresholds, colors):\n",
    "            n_features = (cumulative_importance >= threshold).argmax() + 1\n",
    "            plt.axhline(y=threshold, color=color, linestyle='--', \n",
    "                      label=f'{threshold*100:.0f}% importance: {n_features} features')\n",
    "            plt.axvline(x=n_features, color=color, linestyle='--')\n",
    "        \n",
    "        plt.legend(loc='lower right')\n",
    "        plt.grid(True, linestyle='--', alpha=0.7)\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save plot\n",
    "        cumulative_plot_file = os.path.join(plots_dir, 'cumulative_importance_plot.png')\n",
    "        plt.savefig(cumulative_plot_file, dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        logging.info(f\"Saved cumulative importance plot to {cumulative_plot_file}\")\n",
    "        \n",
    "        return dict([(t, (cumulative_importance >= t).argmax() + 1) for t in thresholds])\n",
    "    \n",
    "    def _plot_feature_correlation(self, X: pd.DataFrame, selected_features: List[str], output_dir: str, max_features: int = 20):\n",
    "        \"\"\"\n",
    "        Create a correlation heatmap of the selected features.\n",
    "        \n",
    "        Args:\n",
    "            X: Feature dataframe\n",
    "            selected_features: List of feature names to include\n",
    "            output_dir: Directory to save the plot\n",
    "            max_features: Maximum number of features to include in the plot\n",
    "        \"\"\"\n",
    "        plots_dir = os.path.join(output_dir, \"plots\")\n",
    "        os.makedirs(plots_dir, exist_ok=True)\n",
    "        \n",
    "        # Use up to max_features to keep the plot readable\n",
    "        if len(selected_features) > max_features:\n",
    "            plot_features = selected_features[:max_features]\n",
    "            logging.info(f\"Showing correlation heatmap with top {max_features} features\")\n",
    "        else:\n",
    "            plot_features = selected_features\n",
    "            \n",
    "        # Calculate correlation matrix\n",
    "        corr = X[plot_features].corr()\n",
    "        \n",
    "        # Create heatmap\n",
    "        plt.figure(figsize=(12, 10))\n",
    "        mask = np.triu(np.ones_like(corr, dtype=bool))  # Create mask for upper triangle\n",
    "        cmap = sns.diverging_palette(230, 20, as_cmap=True)\n",
    "        \n",
    "        sns.heatmap(corr, mask=mask, cmap=cmap, vmax=1, vmin=-1, center=0,\n",
    "                   square=True, linewidths=.5, annot=True, fmt=\".2f\", cbar_kws={\"shrink\": .7})\n",
    "        \n",
    "        plt.title(f'Correlation Matrix of Top {len(plot_features)} Features', fontsize=16)\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save plot\n",
    "        corr_plot_file = os.path.join(plots_dir, 'feature_correlation_heatmap.png')\n",
    "        plt.savefig(corr_plot_file, dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        logging.info(f\"Saved correlation heatmap to {corr_plot_file}\")\n",
    "    \n",
    "    def _plot_pca_visualization(\n",
    "        self,\n",
    "        X: pd.DataFrame,\n",
    "        y: pd.Series,\n",
    "        selected_features: List[str],\n",
    "        output_dir: str,\n",
    "        n_features: int,\n",
    "        task_type: str\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Create a PCA visualization of the selected features.\n",
    "        \n",
    "        Args:\n",
    "            X: Feature dataframe\n",
    "            y: Target variable\n",
    "            selected_features: List of feature names to include\n",
    "            output_dir: Directory to save the plot\n",
    "            n_features: Number of features selected\n",
    "            task_type: 'classification' or 'regression'\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Only use selected features\n",
    "            X_selected = X[selected_features]\n",
    "            \n",
    "            # Apply PCA for dimensionality reduction to 2D\n",
    "            pca = PCA(n_components=2, random_state=42)\n",
    "            X_pca = pca.fit_transform(X_selected)\n",
    "            \n",
    "            # Create dataframe for plotting\n",
    "            pca_df = pd.DataFrame({\n",
    "                'PCA1': X_pca[:, 0],\n",
    "                'PCA2': X_pca[:, 1],\n",
    "                'target': y\n",
    "            })\n",
    "            \n",
    "            # Plot\n",
    "            plt.figure(figsize=(10, 8))\n",
    "            \n",
    "            # Handle both categorical and numerical targets\n",
    "            if task_type == 'classification' or (pd.api.types.is_numeric_dtype(y) and len(y.unique()) <= 10):\n",
    "                # Categorical target - use hue\n",
    "                ax = sns.scatterplot(x='PCA1', y='PCA2', hue='target', \n",
    "                                 data=pca_df, palette='viridis', alpha=0.7, s=60)\n",
    "                plt.legend(title='Class', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "            else:\n",
    "                # Continuous target - use a scatter plot with colormap\n",
    "                scatter = plt.scatter(pca_df['PCA1'], pca_df['PCA2'], c=pca_df['target'], \n",
    "                                    cmap='viridis', alpha=0.6, s=50)\n",
    "                plt.colorbar(scatter, label='Target Value')\n",
    "            \n",
    "            # Calculate explained variance\n",
    "            explained_variance = pca.explained_variance_ratio_\n",
    "            \n",
    "            plt.title(f'PCA of Top {n_features} Features\\nExplained Variance: {sum(explained_variance):.2%}', \n",
    "                    fontsize=14)\n",
    "            plt.xlabel(f'PCA1 ({explained_variance[0]:.2%})', fontsize=12)\n",
    "            plt.ylabel(f'PCA2 ({explained_variance[1]:.2%})', fontsize=12)\n",
    "            plt.grid(True, linestyle='--', alpha=0.7)\n",
    "            plt.tight_layout()\n",
    "            \n",
    "            # Save plot\n",
    "            pca_plot_file = os.path.join(output_dir, f'pca_visualization_top_{n_features}.png')\n",
    "            plt.savefig(pca_plot_file, dpi=300, bbox_inches='tight')\n",
    "            plt.close()\n",
    "            logging.info(f\"Saved PCA visualization to {pca_plot_file}\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error creating PCA visualization: {str(e)}\")\n",
    "    \n",
    "    def _plot_tsne_visualization(\n",
    "        self,\n",
    "        X: pd.DataFrame,\n",
    "        y: pd.Series,\n",
    "        selected_features: List[str],\n",
    "        output_dir: str,\n",
    "        n_features: int,\n",
    "        task_type: str,\n",
    "        perplexity: int = 30\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Create a t-SNE visualization of the selected features.\n",
    "        \n",
    "        Args:\n",
    "            X: Feature dataframe\n",
    "            y: Target variable\n",
    "            selected_features: List of feature names to include\n",
    "            output_dir: Directory to save the plot\n",
    "            n_features: Number of features selected\n",
    "            task_type: 'classification' or 'regression'\n",
    "            perplexity: Perplexity parameter for t-SNE\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Only use selected features\n",
    "            X_selected = X[selected_features]\n",
    "            \n",
    "            # Apply t-SNE for dimensionality reduction to 2D\n",
    "            perplexity = min(perplexity, len(X_selected) - 1)  # Perplexity must be less than n_samples - 1\n",
    "            tsne = TSNE(n_components=2, perplexity=perplexity, random_state=42, n_jobs=-1)\n",
    "            X_tsne = tsne.fit_transform(X_selected)\n",
    "            \n",
    "            # Create dataframe for plotting\n",
    "            tsne_df = pd.DataFrame({\n",
    "                'TSNE1': X_tsne[:, 0],\n",
    "                'TSNE2': X_tsne[:, 1],\n",
    "                'target': y\n",
    "            })\n",
    "            \n",
    "            # Plot\n",
    "            plt.figure(figsize=(10, 8))\n",
    "            \n",
    "            # Handle both categorical and numerical targets\n",
    "            if task_type == 'classification' or (pd.api.types.is_numeric_dtype(y) and len(y.unique()) <= 10):\n",
    "                # Categorical target - use hue\n",
    "                ax = sns.scatterplot(x='TSNE1', y='TSNE2', hue='target', \n",
    "                                 data=tsne_df, palette='viridis', alpha=0.7, s=60)\n",
    "                plt.legend(title='Class', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "            else:\n",
    "                # Continuous target - use a scatter plot with colormap\n",
    "                scatter = plt.scatter(tsne_df['TSNE1'], tsne_df['TSNE2'], c=tsne_df['target'], \n",
    "                                    cmap='viridis', alpha=0.6, s=50)\n",
    "                plt.colorbar(scatter, label='Target Value')\n",
    "            \n",
    "            plt.title(f't-SNE Visualization of Top {n_features} Features', fontsize=14)\n",
    "            plt.xlabel('t-SNE Dimension 1', fontsize=12)\n",
    "            plt.ylabel('t-SNE Dimension 2', fontsize=12)\n",
    "            plt.grid(True, linestyle='--', alpha=0.7)\n",
    "            plt.tight_layout()\n",
    "            \n",
    "            # Save plot\n",
    "            tsne_plot_file = os.path.join(output_dir, f'tsne_visualization_top_{n_features}.png')\n",
    "            plt.savefig(tsne_plot_file, dpi=300, bbox_inches='tight')\n",
    "            plt.close()\n",
    "            logging.info(f\"Saved t-SNE visualization to {tsne_plot_file}\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error creating t-SNE visualization: {str(e)}\")\n",
    "\n",
    "feature_selection_tool = FeatureImportanceAnalysisTool()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nnUNet Training and Inference Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NNUNetTrainingTool(Tool):\n",
    "    name = \"nnunet_training\"\n",
    "    description = \"\"\"\n",
    "    This tool trains a segmentation model using the nnUNet framework.\n",
    "    It first preprocesses the dataset and then trains the model.\n",
    "    The tool returns the path to the trained model and performance metrics.\n",
    "    \"\"\"\n",
    "    inputs = {\n",
    "        \"dataset_id\": {\n",
    "            \"type\": \"integer\",\n",
    "            \"description\": \"Dataset ID to train with (e.g., 50 for Dataset050, will be zero-padded to 3 digits)\"\n",
    "        },\n",
    "        \"configuration\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"nnUNet configuration to use ('2d', '3d_fullres', '3d_lowres', '3d_cascade_fullres')\",\n",
    "            \"required\": False,\n",
    "            \"nullable\": True\n",
    "        },\n",
    "        \"fold\": {\n",
    "            \"type\": \"string\",  # Use string type which can handle both integers and \"all\"\n",
    "            \"description\": \"Fold of the 5-fold cross-validation. Should be an int between 0 and 4, or 'all' to train all folds.\",\n",
    "            \"required\": False,\n",
    "            \"nullable\": True\n",
    "        },\n",
    "        \"trainer\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"Use a custom trainer. Default: nnUNetTrainer\",\n",
    "            \"required\": False,\n",
    "            \"nullable\": True\n",
    "        },\n",
    "        \"plans_identifier\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"Custom plans identifier. Default: nnUNetPlans\",\n",
    "            \"required\": False,\n",
    "            \"nullable\": True\n",
    "        },\n",
    "        \"pretrained_weights\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"Path to nnU-Net checkpoint file to be used as pretrained model\",\n",
    "            \"required\": False,\n",
    "            \"nullable\": True\n",
    "        },\n",
    "        \"num_gpus\": {\n",
    "            \"type\": \"integer\",\n",
    "            \"description\": \"Number of GPUs to use for training\",\n",
    "            \"required\": False,\n",
    "            \"nullable\": True\n",
    "        },\n",
    "        \"device\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"Device to run training on ('cuda', 'cpu', 'mps')\",\n",
    "            \"required\": False,\n",
    "            \"nullable\": True\n",
    "        },\n",
    "        \"use_compressed\": {\n",
    "            \"type\": \"boolean\",\n",
    "            \"description\": \"If set, the training cases will not be decompressed\",\n",
    "            \"required\": False,\n",
    "            \"nullable\": True\n",
    "        },\n",
    "        \"npz\": {\n",
    "            \"type\": \"boolean\",\n",
    "            \"description\": \"Save softmax predictions from final validation as npz files\",\n",
    "            \"required\": False,\n",
    "            \"nullable\": True\n",
    "        },\n",
    "        \"continue_training\": {\n",
    "            \"type\": \"boolean\",\n",
    "            \"description\": \"Continue training from latest checkpoint\",\n",
    "            \"required\": False,\n",
    "            \"nullable\": True\n",
    "        },\n",
    "        \"validation_only\": {\n",
    "            \"type\": \"boolean\",\n",
    "            \"description\": \"Only run validation (training must have finished)\",\n",
    "            \"required\": False,\n",
    "            \"nullable\": True\n",
    "        },\n",
    "        \"val_best\": {\n",
    "            \"type\": \"boolean\",\n",
    "            \"description\": \"Use checkpoint_best instead of checkpoint_final for validation\",\n",
    "            \"required\": False,\n",
    "            \"nullable\": True\n",
    "        },\n",
    "        \"disable_checkpointing\": {\n",
    "            \"type\": \"boolean\",\n",
    "            \"description\": \"Disable checkpointing during training\",\n",
    "            \"required\": False,\n",
    "            \"nullable\": True\n",
    "        },\n",
    "        \"verify_dataset_integrity\": {\n",
    "            \"type\": \"boolean\",\n",
    "            \"description\": \"Verify dataset integrity during preprocessing\",\n",
    "            \"required\": False,\n",
    "            \"nullable\": True\n",
    "        },\n",
    "        \"no_preprocessing\": {\n",
    "            \"type\": \"boolean\",\n",
    "            \"description\": \"Skip preprocessing step (use only if data is already preprocessed)\",\n",
    "            \"required\": False,\n",
    "            \"nullable\": True\n",
    "        }\n",
    "    }\n",
    "    output_type = \"object\"\n",
    "\n",
    "    def forward(\n",
    "        self, \n",
    "        dataset_id: int,  # Will be formatted as 3 digits, e.g. 50  050\n",
    "        configuration: Optional[str] = \"3d_fullres\",\n",
    "        fold: Optional[str] = None,  # None will be converted to \"all\", can also be \"0\", \"1\", \"2\", \"3\", \"4\", or \"all\"\n",
    "        trainer: Optional[str] = None,\n",
    "        plans_identifier: Optional[str] = None,\n",
    "        pretrained_weights: Optional[str] = None,\n",
    "        num_gpus: Optional[int] = None,\n",
    "        device: Optional[str] = None,\n",
    "        use_compressed: Optional[bool] = False,\n",
    "        npz: Optional[bool] = False,\n",
    "        continue_training: Optional[bool] = False,\n",
    "        validation_only: Optional[bool] = False,\n",
    "        val_best: Optional[bool] = False,\n",
    "        disable_checkpointing: Optional[bool] = False,\n",
    "        verify_dataset_integrity: Optional[bool] = True,\n",
    "        no_preprocessing: Optional[bool] = False\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Train a segmentation model using nnUNet framework with preprocessing.\n",
    "        \n",
    "        Args:\n",
    "            dataset_id: Dataset ID to train with\n",
    "            configuration: nnUNet configuration\n",
    "            fold: Cross-validation fold (0-4)\n",
    "            trainer: Custom trainer\n",
    "            plans_identifier: Custom plans identifier\n",
    "            pretrained_weights: Path to pretrained model\n",
    "            num_gpus: Number of GPUs to use\n",
    "            device: Device to run on ('cuda', 'cpu', 'mps')\n",
    "            use_compressed: Use compressed data\n",
    "            npz: Save softmax predictions\n",
    "            continue_training: Continue from latest checkpoint\n",
    "            validation_only: Only run validation\n",
    "            val_best: Use checkpoint_best for validation\n",
    "            disable_checkpointing: Disable checkpointing\n",
    "            verify_dataset_integrity: Verify dataset integrity during preprocessing\n",
    "            no_preprocessing: Skip preprocessing step\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with training results including model path and performance metrics\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # First, preprocess the data unless no_preprocessing is True\n",
    "            if not no_preprocessing:\n",
    "                preprocess_result = self._preprocess_data(\n",
    "                    dataset_id=dataset_id,\n",
    "                    verify_dataset_integrity=verify_dataset_integrity,\n",
    "                    configuration=configuration\n",
    "                )\n",
    "                \n",
    "                if preprocess_result.get(\"status\") == \"error\":\n",
    "                    return preprocess_result\n",
    "            \n",
    "            # Then train the model\n",
    "            model_path, metrics = self._train_model(\n",
    "                dataset_id=dataset_id,\n",
    "                configuration=configuration,\n",
    "                fold=fold,\n",
    "                trainer=trainer,\n",
    "                plans_identifier=plans_identifier,\n",
    "                pretrained_weights=pretrained_weights,\n",
    "                num_gpus=num_gpus,\n",
    "                device=device,\n",
    "                use_compressed=use_compressed,\n",
    "                npz=npz,\n",
    "                continue_training=continue_training,\n",
    "                validation_only=validation_only,\n",
    "                val_best=val_best,\n",
    "                disable_checkpointing=disable_checkpointing\n",
    "            )\n",
    "            \n",
    "            return {\n",
    "                \"status\": \"success\",\n",
    "                \"model_path\": model_path,\n",
    "                \"dataset_id\": dataset_id,\n",
    "                \"configuration\": configuration,\n",
    "                \"fold\": fold,\n",
    "                \"metrics\": metrics\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {\n",
    "                \"status\": \"error\",\n",
    "                \"error_message\": str(e),\n",
    "                \"dataset_id\": dataset_id\n",
    "            }\n",
    "\n",
    "    def _preprocess_data(self, dataset_id: int, verify_dataset_integrity: bool = True, configuration: Optional[str] = None):\n",
    "        \"\"\"\n",
    "        Preprocess the data using nnUNetv2_plan_and_preprocess.\n",
    "        \n",
    "        Args:\n",
    "            dataset_id: Dataset ID to preprocess\n",
    "            verify_dataset_integrity: Whether to verify dataset integrity\n",
    "            configuration: nnUNet configuration to preprocess for\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with preprocessing results\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Build command for preprocessing\n",
    "            cmd = [\"nnUNetv2_plan_and_preprocess\", \"-d\", f\"{dataset_id:03d}\"]\n",
    "            \n",
    "            # Add configuration-specific option if provided\n",
    "            if configuration:\n",
    "                # Use the -c flag to specify which configuration to preprocess\n",
    "                cmd.extend([\"-c\", configuration])\n",
    "            \n",
    "            if verify_dataset_integrity:\n",
    "                cmd.append(\"--verify_dataset_integrity\")\n",
    "            \n",
    "            print(f\"Running preprocessing command: {' '.join(cmd)}\")\n",
    "            process = subprocess.run(cmd, capture_output=True, text=True)\n",
    "            \n",
    "            if process.returncode != 0:\n",
    "                return {\n",
    "                    \"status\": \"error\",\n",
    "                    \"error_message\": f\"Preprocessing failed: {process.stderr}\",\n",
    "                    \"dataset_id\": dataset_id\n",
    "                }\n",
    "            \n",
    "            return {\n",
    "                \"status\": \"success\",\n",
    "                \"message\": \"Preprocessing completed successfully\",\n",
    "                \"dataset_id\": dataset_id\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {\n",
    "                \"status\": \"error\",\n",
    "                \"error_message\": f\"Error during preprocessing: {str(e)}\",\n",
    "                \"dataset_id\": dataset_id\n",
    "            }\n",
    "    \n",
    "    def _train_model(\n",
    "        self, \n",
    "        dataset_id: int, \n",
    "        configuration: str = \"3d_fullres\",\n",
    "        fold: Optional[str] = None,\n",
    "        trainer: Optional[str] = None,\n",
    "        plans_identifier: Optional[str] = None,\n",
    "        pretrained_weights: Optional[str] = None,\n",
    "        num_gpus: Optional[int] = None,\n",
    "        device: Optional[str] = None,\n",
    "        use_compressed: bool = False,\n",
    "        npz: bool = False,\n",
    "        continue_training: bool = False,\n",
    "        validation_only: bool = False,\n",
    "        val_best: bool = False,\n",
    "        disable_checkpointing: bool = False\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Train the nnUNet model.\n",
    "        \n",
    "        Args:\n",
    "            dataset_id: Dataset ID to train with\n",
    "            configuration: nnUNet configuration\n",
    "            fold: Cross-validation fold (0-4)\n",
    "            trainer: Custom trainer\n",
    "            plans_identifier: Custom plans identifier\n",
    "            pretrained_weights: Path to pretrained model\n",
    "            num_gpus: Number of GPUs to use\n",
    "            device: Device to run on ('cuda', 'cpu', 'mps')\n",
    "            use_compressed: Use compressed data\n",
    "            npz: Save softmax predictions\n",
    "            continue_training: Continue from latest checkpoint\n",
    "            validation_only: Only run validation\n",
    "            val_best: Use checkpoint_best for validation\n",
    "            disable_checkpointing: Disable checkpointing\n",
    "            \n",
    "        Returns:\n",
    "            Tuple of (model_path, metrics)\n",
    "        \"\"\"\n",
    "        # Build command for training - start with required positional arguments\n",
    "        cmd = [\n",
    "            \"nnUNetv2_train\",\n",
    "            f\"{dataset_id:03d}\",\n",
    "            configuration,\n",
    "        ]\n",
    "        \n",
    "        # Add fold parameter\n",
    "        if fold is None:\n",
    "            # Default to 'all' when fold is not specified\n",
    "            cmd.append(\"all\")\n",
    "        else:\n",
    "            # Use the provided fold value (could be an integer or 'all')\n",
    "            cmd.append(str(fold))\n",
    "        \n",
    "        # Add optional arguments\n",
    "        if trainer is not None:\n",
    "            cmd.extend([\"-tr\", trainer])\n",
    "            \n",
    "        if plans_identifier is not None:\n",
    "            cmd.extend([\"-p\", plans_identifier])\n",
    "            \n",
    "        if pretrained_weights is not None:\n",
    "            cmd.extend([\"-pretrained_weights\", pretrained_weights])\n",
    "            \n",
    "        if num_gpus is not None:\n",
    "            cmd.extend([\"-num_gpus\", str(num_gpus)])\n",
    "            \n",
    "        if device is not None:\n",
    "            cmd.extend([\"-device\", device])\n",
    "            \n",
    "        # Add boolean flags\n",
    "        if use_compressed:\n",
    "            cmd.append(\"--use_compressed\")\n",
    "            \n",
    "        if npz:\n",
    "            cmd.append(\"--npz\")\n",
    "            \n",
    "        if continue_training:\n",
    "            cmd.append(\"--c\")\n",
    "            \n",
    "        if validation_only:\n",
    "            cmd.append(\"--val\")\n",
    "            \n",
    "        if val_best:\n",
    "            cmd.append(\"--val_best\")\n",
    "            \n",
    "        if disable_checkpointing:\n",
    "            cmd.append(\"--disable_checkpointing\")\n",
    "        \n",
    "        # Run the training\n",
    "        print(f\"Running training command: {' '.join(cmd)}\")\n",
    "        process = subprocess.run(cmd, capture_output=True, text=True)\n",
    "        \n",
    "        if process.returncode != 0:\n",
    "            raise RuntimeError(f\"Training failed: {process.stderr}\")\n",
    "        \n",
    "        # Get model path\n",
    "        results_folder = os.environ[\"RESULTS_FOLDER\"]\n",
    "        \n",
    "        # Determine the fold directory name\n",
    "        if fold is None or fold == \"all\":\n",
    "            fold_dir = \"fold_all\"\n",
    "        else:\n",
    "            fold_dir = f\"fold_{fold}\"\n",
    "            \n",
    "        model_path = os.path.join(\n",
    "            results_folder, \n",
    "            f\"nnUNetv2_{configuration}\", \n",
    "            f\"Dataset{dataset_id:03d}\",\n",
    "            fold_dir\n",
    "        )\n",
    "        \n",
    "        # Parse metrics from validation output\n",
    "        metrics = self._parse_metrics(process.stdout)\n",
    "        \n",
    "        return model_path, metrics\n",
    "    \n",
    "    def _parse_metrics(self, output_text: str):\n",
    "        \"\"\"\n",
    "        Parse metrics from nnUNet training output.\n",
    "        \n",
    "        Args:\n",
    "            output_text: Output text from the training process\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary of metrics\n",
    "        \"\"\"\n",
    "        metrics = {\n",
    "            \"dice\": None,\n",
    "            \"iou\": None,\n",
    "            \"accuracy\": None,\n",
    "            \"precision\": None,\n",
    "            \"recall\": None,\n",
    "            \"validation_loss\": None\n",
    "        }\n",
    "        \n",
    "        # Extract metrics from output_text\n",
    "        import re\n",
    "        \n",
    "        # Extract Dice score\n",
    "        dice_match = re.search(r\"mean\\s+dice\\s*:\\s*([0-9.]+)\", output_text, re.IGNORECASE)\n",
    "        if dice_match:\n",
    "            metrics[\"dice\"] = float(dice_match.group(1))\n",
    "            \n",
    "        # Extract IoU/Jaccard if available\n",
    "        iou_match = re.search(r\"mean\\s+iou\\s*:\\s*([0-9.]+)\", output_text, re.IGNORECASE)\n",
    "        if iou_match:\n",
    "            metrics[\"iou\"] = float(iou_match.group(1))\n",
    "            \n",
    "        # Extract validation loss if available\n",
    "        loss_match = re.search(r\"validation\\s+loss\\s*:\\s*([0-9.]+)\", output_text, re.IGNORECASE)\n",
    "        if loss_match:\n",
    "            metrics[\"validation_loss\"] = float(loss_match.group(1))\n",
    "        \n",
    "        return metrics\n",
    "\n",
    "\n",
    "class NNUNetInferenceTool(Tool):\n",
    "    name = \"nnunet_inference\"\n",
    "    description = \"\"\"\n",
    "    This tool runs inference with a trained nnUNet model.\n",
    "    It applies the model to input images to generate segmentation masks.\n",
    "    The tool returns the path to the output segmentations.\n",
    "    \"\"\"\n",
    "    inputs = {\n",
    "        \"input_folder\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"Input folder containing images to segment. Files should use correct channel numberings (_0000 etc)\"\n",
    "        },\n",
    "        \"output_folder\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"Output folder where segmentations will be saved\"\n",
    "        },\n",
    "        \"dataset_id\": {\n",
    "            \"type\": \"integer\",\n",
    "            \"description\": \"Dataset ID used for training the model (e.g., 50 for Dataset050, will be zero-padded to 3 digits)\"\n",
    "        },\n",
    "        \"configuration\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"nnUNet configuration to use ('2d', '3d_fullres', '3d_lowres', '3d_cascade_fullres')\"\n",
    "        },\n",
    "        \"model_folder\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"Path to the directory containing the trained model. For example, the path to 'nnUNet_results/Dataset135_BraTS2021/nnUNetTrainer__nnUNetPlans__3d_fullres' or a subdirectory containing the fold\",\n",
    "            \"required\": False,\n",
    "            \"nullable\": True\n",
    "        },\n",
    "        \"results_dir\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"Base directory containing nnUNet results (overrides nnUNet_results environment variable)\",\n",
    "            \"required\": False,\n",
    "            \"nullable\": True\n",
    "        },\n",
    "        \"folds\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"Comma-separated list of folds to use for prediction (e.g., '0,1,2,3,4' or 'all')\",\n",
    "            \"required\": False,\n",
    "            \"nullable\": True\n",
    "        },\n",
    "        \"plans_identifier\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"Plans identifier. Default: nnUNetPlans\",\n",
    "            \"required\": False,\n",
    "            \"nullable\": True\n",
    "        },\n",
    "        \"trainer\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"Trainer class used for training. Default: nnUNetTrainer\",\n",
    "            \"required\": False,\n",
    "            \"nullable\": True\n",
    "        },\n",
    "        \"step_size\": {\n",
    "            \"type\": \"number\",\n",
    "            \"description\": \"Step size for sliding window prediction (0-1). Default: 0.5\",\n",
    "            \"required\": False,\n",
    "            \"nullable\": True\n",
    "        },\n",
    "        \"disable_tta\": {\n",
    "            \"type\": \"boolean\",\n",
    "            \"description\": \"Disable test time augmentation (mirroring). Faster but less accurate\",\n",
    "            \"required\": False,\n",
    "            \"nullable\": True\n",
    "        },\n",
    "        \"save_probabilities\": {\n",
    "            \"type\": \"boolean\",\n",
    "            \"description\": \"Export predicted class probabilities (needed for ensembling)\",\n",
    "            \"required\": False,\n",
    "            \"nullable\": True\n",
    "        },\n",
    "        \"continue_prediction\": {\n",
    "            \"type\": \"boolean\",\n",
    "            \"description\": \"Continue an aborted previous prediction\",\n",
    "            \"required\": False,\n",
    "            \"nullable\": True\n",
    "        },\n",
    "        \"checkpoint\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"Checkpoint name to use. Default: checkpoint_final.pth\",\n",
    "            \"required\": False,\n",
    "            \"nullable\": True\n",
    "        },\n",
    "        \"num_processes_preprocessing\": {\n",
    "            \"type\": \"integer\",\n",
    "            \"description\": \"Number of processes for preprocessing\",\n",
    "            \"required\": False,\n",
    "            \"nullable\": True\n",
    "        },\n",
    "        \"num_processes_segmentation\": {\n",
    "            \"type\": \"integer\",\n",
    "            \"description\": \"Number of processes for segmentation export\",\n",
    "            \"required\": False,\n",
    "            \"nullable\": True\n",
    "        },\n",
    "        \"prev_stage_predictions\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"Folder with predictions from previous stage (for cascade models)\",\n",
    "            \"required\": False,\n",
    "            \"nullable\": True\n",
    "        },\n",
    "        \"num_parts\": {\n",
    "            \"type\": \"integer\",\n",
    "            \"description\": \"Number of separate inference calls for parallelization\",\n",
    "            \"required\": False,\n",
    "            \"nullable\": True\n",
    "        },\n",
    "        \"part_id\": {\n",
    "            \"type\": \"integer\",\n",
    "            \"description\": \"Which part of the parallel inference is this (0 to num_parts-1)\",\n",
    "            \"required\": False,\n",
    "            \"nullable\": True\n",
    "        },\n",
    "        \"device\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"Device for inference: 'cuda' (GPU), 'cpu', or 'mps' (Apple)\",\n",
    "            \"required\": False,\n",
    "            \"nullable\": True\n",
    "        },\n",
    "        \"verbose\": {\n",
    "            \"type\": \"boolean\",\n",
    "            \"description\": \"Enable verbose output\",\n",
    "            \"required\": False,\n",
    "            \"nullable\": True\n",
    "        }\n",
    "    }\n",
    "    output_type = \"object\"\n",
    "\n",
    "    def forward(\n",
    "        self, \n",
    "        input_folder: str,\n",
    "        output_folder: str,\n",
    "        dataset_id: int,  # Will be formatted as 3 digits, e.g. 50  050\n",
    "        configuration: str,\n",
    "        model_folder: Optional[str] = None,\n",
    "        results_dir: Optional[str] = None,\n",
    "        folds: Optional[str] = None,  # \"0,1,2,3,4\" or \"all\"\n",
    "        plans_identifier: Optional[str] = None,\n",
    "        trainer: Optional[str] = None,\n",
    "        step_size: Optional[float] = None,\n",
    "        disable_tta: Optional[bool] = False,\n",
    "        save_probabilities: Optional[bool] = False,\n",
    "        continue_prediction: Optional[bool] = False,\n",
    "        checkpoint: Optional[str] = None,\n",
    "        num_processes_preprocessing: Optional[int] = None,\n",
    "        num_processes_segmentation: Optional[int] = None,\n",
    "        prev_stage_predictions: Optional[str] = None,\n",
    "        num_parts: Optional[int] = None,\n",
    "        part_id: Optional[int] = None,\n",
    "        device: Optional[str] = None,\n",
    "        verbose: Optional[bool] = False\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Run inference with a trained nnUNet model.\n",
    "        \n",
    "        Args:\n",
    "            input_folder: Folder containing images to segment\n",
    "            output_folder: Folder to save segmentation results\n",
    "            dataset_id: Dataset ID used for training\n",
    "            configuration: nnUNet configuration\n",
    "            folds: Comma-separated list of folds to use\n",
    "            plans_identifier: Plans identifier\n",
    "            trainer: Trainer class used\n",
    "            step_size: Step size for sliding window\n",
    "            disable_tta: Disable test time augmentation\n",
    "            save_probabilities: Save softmax outputs\n",
    "            continue_prediction: Continue previous prediction\n",
    "            checkpoint: Checkpoint name to use\n",
    "            num_processes_preprocessing: Processes for preprocessing\n",
    "            num_processes_segmentation: Processes for segmentation export\n",
    "            prev_stage_predictions: Previous stage predictions folder\n",
    "            num_parts: Total number of parallel parts\n",
    "            part_id: ID of this parallel part\n",
    "            device: Device for inference\n",
    "            verbose: Enable verbose output\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with inference results and paths\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Run the inference\n",
    "            output_files = self._run_inference(\n",
    "                input_folder=input_folder,\n",
    "                output_folder=output_folder,\n",
    "                dataset_id=dataset_id,\n",
    "                configuration=configuration,\n",
    "                model_folder=model_folder,\n",
    "                results_dir=results_dir,\n",
    "                folds=folds,\n",
    "                plans_identifier=plans_identifier,\n",
    "                trainer=trainer,\n",
    "                step_size=step_size,\n",
    "                disable_tta=disable_tta,\n",
    "                save_probabilities=save_probabilities,\n",
    "                continue_prediction=continue_prediction,\n",
    "                checkpoint=checkpoint,\n",
    "                num_processes_preprocessing=num_processes_preprocessing,\n",
    "                num_processes_segmentation=num_processes_segmentation,\n",
    "                prev_stage_predictions=prev_stage_predictions,\n",
    "                num_parts=num_parts,\n",
    "                part_id=part_id,\n",
    "                device=device,\n",
    "                verbose=verbose\n",
    "            )\n",
    "            \n",
    "            return {\n",
    "                \"status\": \"success\",\n",
    "                \"output_folder\": output_folder,\n",
    "                \"dataset_id\": dataset_id,\n",
    "                \"configuration\": configuration,\n",
    "                \"num_segmentations\": len(output_files),\n",
    "                \"segmentation_files\": output_files\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {\n",
    "                \"status\": \"error\",\n",
    "                \"error_message\": str(e),\n",
    "                \"dataset_id\": dataset_id,\n",
    "                \"input_folder\": input_folder\n",
    "            }\n",
    "    \n",
    "    def _run_inference(\n",
    "        self,\n",
    "        input_folder: str,\n",
    "        output_folder: str,\n",
    "        dataset_id: int,\n",
    "        configuration: str,\n",
    "        model_folder: Optional[str] = None,\n",
    "        results_dir: Optional[str] = None,\n",
    "        folds: Optional[str] = None,\n",
    "        plans_identifier: Optional[str] = None,\n",
    "        trainer: Optional[str] = None,\n",
    "        step_size: Optional[float] = None,\n",
    "        disable_tta: Optional[bool] = False,\n",
    "        save_probabilities: Optional[bool] = False,\n",
    "        continue_prediction: Optional[bool] = False,\n",
    "        checkpoint: Optional[str] = None,\n",
    "        num_processes_preprocessing: Optional[int] = None,\n",
    "        num_processes_segmentation: Optional[int] = None,\n",
    "        prev_stage_predictions: Optional[str] = None,\n",
    "        num_parts: Optional[int] = None,\n",
    "        part_id: Optional[int] = None,\n",
    "        device: Optional[str] = None,\n",
    "        verbose: Optional[bool] = False\n",
    "    ) -> List[str]:\n",
    "        \"\"\"\n",
    "        Run nnUNet inference.\n",
    "        \n",
    "        Args:\n",
    "            Same as forward method\n",
    "            \n",
    "        Returns:\n",
    "            List of output segmentation file paths\n",
    "        \"\"\"\n",
    "        # Build command for inference - required arguments\n",
    "        cmd = [\n",
    "            \"nnUNetv2_predict\",\n",
    "            \"-i\", input_folder,\n",
    "            \"-o\", output_folder,\n",
    "            \"-d\", f\"{dataset_id:03d}\",\n",
    "            \"-c\", configuration\n",
    "        ]\n",
    "        \n",
    "        # Set RESULTS_FOLDER environment variable if provided\n",
    "        if results_dir:\n",
    "            print(f\"Setting RESULTS_FOLDER to: {results_dir}\")\n",
    "            os.environ[\"RESULTS_FOLDER\"] = results_dir\n",
    "            \n",
    "        # Handle model folder path\n",
    "        if model_folder is not None:\n",
    "            # nnUNetv2_predict looks for models in a specific structure:\n",
    "            # RESULTS_FOLDER/Dataset{dataset_id}_{dataset_name}/{trainer}__{plans}__{configuration}/fold_{fold}/checkpoint_final.pth\n",
    "            \n",
    "            # We need to extract the correct RESULTS_FOLDER from the model_folder path\n",
    "            model_path = os.path.abspath(model_folder)\n",
    "            \n",
    "            # Navigate up the folder structure to find the proper RESULTS_FOLDER\n",
    "            # Typical path pattern: .../nnUNet_results/Dataset{dataset_id}_{dataset_name}/{trainer}__{plans}__{configuration}/fold_{fold}\n",
    "            # We need to go up to the directory containing \"Dataset{dataset_id}...\"\n",
    "            \n",
    "            folder_parts = model_path.split(os.sep)\n",
    "            results_folder = None\n",
    "            \n",
    "            # Look for a directory pattern like \"Dataset{digits}_*\"\n",
    "            for i in range(len(folder_parts)-1, 0, -1):\n",
    "                if folder_parts[i].startswith(\"Dataset\") and \"_\" in folder_parts[i]:\n",
    "                    # Found the dataset folder, set results_folder to its parent\n",
    "                    results_folder = os.sep.join(folder_parts[:i])\n",
    "                    break\n",
    "            \n",
    "            if results_folder:\n",
    "                print(f\"Setting nnUNet_results to: {results_folder}\")\n",
    "                os.environ[\"RESULTS_FOLDER\"] = results_folder\n",
    "            else:\n",
    "                # If we couldn't find the pattern, just set RESULTS_FOLDER to parent of model_folder\n",
    "                parent_dir = os.path.dirname(os.path.dirname(os.path.dirname(model_path)))\n",
    "                print(f\"Could not detect standard nnUNet folder structure. Setting RESULTS_FOLDER to: {parent_dir}\")\n",
    "                os.environ[\"RESULTS_FOLDER\"] = parent_dir\n",
    "        \n",
    "        # Add optional arguments\n",
    "        if folds is not None:\n",
    "            # Handle comma-separated list of folds\n",
    "            if folds.lower() != 'all':\n",
    "                fold_list = folds.split(',')\n",
    "                cmd.extend([\"-f\"] + fold_list)\n",
    "            else:\n",
    "                # If 'all' is specified, use the 'all' keyword\n",
    "                cmd.extend([\"-f\", \"all\"])\n",
    "        \n",
    "        if plans_identifier is not None:\n",
    "            cmd.extend([\"-p\", plans_identifier])\n",
    "            \n",
    "        if trainer is not None:\n",
    "            cmd.extend([\"-tr\", trainer])\n",
    "        \n",
    "        if step_size is not None:\n",
    "            cmd.extend([\"-step_size\", str(step_size)])\n",
    "            \n",
    "        if disable_tta:\n",
    "            cmd.append(\"--disable_tta\")\n",
    "            \n",
    "        if verbose:\n",
    "            cmd.append(\"--verbose\")\n",
    "            \n",
    "        if save_probabilities:\n",
    "            cmd.append(\"--save_probabilities\")\n",
    "            \n",
    "        if continue_prediction:\n",
    "            cmd.append(\"--continue_prediction\")\n",
    "            \n",
    "        if checkpoint is not None:\n",
    "            cmd.extend([\"-chk\", checkpoint])\n",
    "            \n",
    "        if num_processes_preprocessing is not None:\n",
    "            cmd.extend([\"-npp\", str(num_processes_preprocessing)])\n",
    "            \n",
    "        if num_processes_segmentation is not None:\n",
    "            cmd.extend([\"-nps\", str(num_processes_segmentation)])\n",
    "            \n",
    "        if prev_stage_predictions is not None:\n",
    "            cmd.extend([\"-prev_stage_predictions\", prev_stage_predictions])\n",
    "            \n",
    "        if num_parts is not None:\n",
    "            cmd.extend([\"-num_parts\", str(num_parts)])\n",
    "            \n",
    "        if part_id is not None:\n",
    "            cmd.extend([\"-part_id\", str(part_id)])\n",
    "            \n",
    "        if device is not None:\n",
    "            cmd.extend([\"-device\", device])\n",
    "        \n",
    "        # Create output directory if it doesn't exist\n",
    "        os.makedirs(output_folder, exist_ok=True)\n",
    "        \n",
    "        # Run the inference command\n",
    "        print(f\"Running inference command: {' '.join(cmd)}\")\n",
    "        process = subprocess.run(cmd, capture_output=True, text=True)\n",
    "        \n",
    "        if process.returncode != 0:\n",
    "            raise RuntimeError(f\"Inference failed: {process.stderr}\")\n",
    "        \n",
    "        # Get output segmentation files\n",
    "        output_files = self._get_output_files(output_folder)\n",
    "        \n",
    "        return output_files\n",
    "    \n",
    "    def _get_output_files(self, output_folder: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        Get list of segmentation files in the output folder.\n",
    "        \n",
    "        Args:\n",
    "            output_folder: Path to the output folder\n",
    "            \n",
    "        Returns:\n",
    "            List of segmentation file paths\n",
    "        \"\"\"\n",
    "        files = []\n",
    "        \n",
    "        # Get all files in the output folder\n",
    "        for root, _, filenames in os.walk(output_folder):\n",
    "            for filename in filenames:\n",
    "                if filename.endswith(('.nii.gz', '.nii')):\n",
    "                    files.append(os.path.join(root, filename))\n",
    "        \n",
    "        return files\n",
    "\n",
    "\n",
    "nnunet_training_tool = NNUNetTrainingTool()\n",
    "nnunet_inference_tool = NNUNetInferenceTool()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TotalSegmentator Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TotalSegmentatorTool(Tool):\n",
    "    name = \"totalsegmentator\"\n",
    "    description = \"\"\"\n",
    "    This tool uses TotalSegmentator to segment anatomical structures in CT and MR images.\n",
    "    It can process NIFTI files or DICOM slices and supports various tasks and options.\n",
    "    Tasks ending with '_mr' are designed for MR images, while other tasks are for CT images.\n",
    "    \"\"\"\n",
    "    \n",
    "    # All available task options\n",
    "    AVAILABLE_TASKS = [\n",
    "        # Main tasks\n",
    "        \"total\", \"total_mr\",\n",
    "        # Subtasks\n",
    "        \"lung_vessels\", \"body\", \"body_mr\", \"vertebrae_mr\", \"cerebral_bleed\", \n",
    "        \"hip_implant\", \"pleural_pericard_effusion\", \"head_glands_cavities\",\n",
    "        \"head_muscles\", \"headneck_bones_vessels\", \"headneck_muscles\", \n",
    "        \"liver_vessels\", \"oculomotor_muscles\", \"lung_nodules\", \"kidney_cysts\",\n",
    "        \"breasts\", \"liver_segments\", \"liver_segments_mr\", \"heartchambers_highres\",\n",
    "        \"appendicular_bones\", \"appendicular_bones_mr\", \"tissue_types\", \n",
    "        \"tissue_types_mr\", \"tissue_4_types\", \"brain_structures\", \"vertebrae_body\",\n",
    "        \"face\", \"face_mr\", \"thigh_shoulder_muscles\", \"thigh_shoulder_muscles_mr\",\n",
    "        \"coronary_arteries\"\n",
    "    ]\n",
    "    \n",
    "    inputs = {\n",
    "        \"input_path\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"Path to input CT nifti image or folder of dicom slices\"\n",
    "        },\n",
    "        \"output_dir\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"Output directory for segmentation masks\"\n",
    "        },\n",
    "        \"output_type\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"Select if segmentations shall be saved as Nifti or as Dicom RT Struct image\",\n",
    "            \"required\": False,\n",
    "            \"nullable\": True\n",
    "        },\n",
    "        \"multilabel\": {\n",
    "            \"type\": \"boolean\",\n",
    "            \"description\": \"Save one multilabel image for all classes instead of separate binary masks\",\n",
    "            \"required\": False,\n",
    "            \"nullable\": True\n",
    "        },\n",
    "        \"nr_threads_resampling\": {\n",
    "            \"type\": \"integer\",\n",
    "            \"description\": \"Number of threads for resampling\",\n",
    "            \"required\": False,\n",
    "            \"nullable\": True\n",
    "        },\n",
    "        \"nr_threads_saving\": {\n",
    "            \"type\": \"integer\",\n",
    "            \"description\": \"Number of threads for saving segmentations\",\n",
    "            \"required\": False,\n",
    "            \"nullable\": True\n",
    "        },\n",
    "        \"fast\": {\n",
    "            \"type\": \"boolean\",\n",
    "            \"description\": \"Run faster lower resolution model (3mm)\",\n",
    "            \"required\": False,\n",
    "            \"nullable\": True\n",
    "        },\n",
    "        \"fastest\": {\n",
    "            \"type\": \"boolean\",\n",
    "            \"description\": \"Run even faster lower resolution model (6mm)\",\n",
    "            \"required\": False,\n",
    "            \"nullable\": True\n",
    "        },\n",
    "        \"nora_tag\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"Tag in nora as mask. Pass nora project id as argument\",\n",
    "            \"required\": False,\n",
    "            \"nullable\": True\n",
    "        },\n",
    "        \"preview\": {\n",
    "            \"type\": \"boolean\",\n",
    "            \"description\": \"Generate a png preview of segmentation\",\n",
    "            \"required\": False,\n",
    "            \"nullable\": True\n",
    "        },\n",
    "        \"task\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"Select which model to use. Tasks ending with '_mr' are for MR images. Default is 'total' for CT images.\",\n",
    "            \"required\": False,\n",
    "            \"nullable\": True\n",
    "        },\n",
    "        \"roi_subset\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"Define a subset of classes to save (comma separated list of class names). If running 1.5mm model, will only run the appropriate models for these rois\",\n",
    "            \"required\": False,\n",
    "            \"nullable\": True\n",
    "        },\n",
    "        \"roi_subset_robust\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"Like roi_subset but uses a slower but more robust model to find the rois\",\n",
    "            \"required\": False,\n",
    "            \"nullable\": True\n",
    "        },\n",
    "        \"statistics\": {\n",
    "            \"type\": \"boolean\",\n",
    "            \"description\": \"Calculate volume (in mm3) and mean intensity. Results will be in statistics.json\",\n",
    "            \"required\": False,\n",
    "            \"nullable\": True\n",
    "        },\n",
    "        \"radiomics\": {\n",
    "            \"type\": \"boolean\",\n",
    "            \"description\": \"Calculate radiomics features. Requires pyradiomics. Results will be in statistics_radiomics.json\",\n",
    "            \"required\": False,\n",
    "            \"nullable\": True\n",
    "        },\n",
    "        \"stats_include_incomplete\": {\n",
    "            \"type\": \"boolean\",\n",
    "            \"description\": \"Normally statistics are only calculated for ROIs which are not cut off by the beginning or end of image. Use this option to calc anyways\",\n",
    "            \"required\": False,\n",
    "            \"nullable\": True\n",
    "        },\n",
    "        \"crop_path\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"Custom path to masks used for cropping. If not set will use output directory\",\n",
    "            \"required\": False,\n",
    "            \"nullable\": True\n",
    "        },\n",
    "        \"body_seg\": {\n",
    "            \"type\": \"boolean\",\n",
    "            \"description\": \"Do initial rough body segmentation and crop image to body region\",\n",
    "            \"required\": False,\n",
    "            \"nullable\": True\n",
    "        },\n",
    "        \"force_split\": {\n",
    "            \"type\": \"boolean\",\n",
    "            \"description\": \"Process image in 3 chunks for less memory consumption\",\n",
    "            \"required\": False,\n",
    "            \"nullable\": True\n",
    "        },\n",
    "        \"skip_saving\": {\n",
    "            \"type\": \"boolean\",\n",
    "            \"description\": \"Skip saving of segmentations for faster runtime if you are only interested in statistics\",\n",
    "            \"required\": False,\n",
    "            \"nullable\": True\n",
    "        },\n",
    "        \"no_derived_masks\": {\n",
    "            \"type\": \"boolean\",\n",
    "            \"description\": \"Do not create derived masks (e.g. skin from body mask)\",\n",
    "            \"required\": False,\n",
    "            \"nullable\": True\n",
    "        },\n",
    "        \"device\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"Device to use for inference ('gpu', 'cpu', or 'mps')\",\n",
    "            \"required\": False,\n",
    "            \"nullable\": True\n",
    "        },\n",
    "        \"quiet\": {\n",
    "            \"type\": \"boolean\",\n",
    "            \"description\": \"Suppress console output\",\n",
    "            \"required\": False,\n",
    "            \"nullable\": True\n",
    "        },\n",
    "        \"verbose\": {\n",
    "            \"type\": \"boolean\",\n",
    "            \"description\": \"Verbose output\",\n",
    "            \"required\": False,\n",
    "            \"nullable\": True\n",
    "        },\n",
    "        \"license_number\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"License number for TotalSegmentator\",\n",
    "            \"required\": False,\n",
    "            \"nullable\": True\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    output_type = \"object\"\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_path: str,\n",
    "        output_dir: str,\n",
    "        output_type: Optional[str] = None,\n",
    "        multilabel: Optional[bool] = False,\n",
    "        nr_threads_resampling: Optional[int] = None,\n",
    "        nr_threads_saving: Optional[int] = None,\n",
    "        fast: Optional[bool] = False,\n",
    "        fastest: Optional[bool] = False,\n",
    "        nora_tag: Optional[str] = None,\n",
    "        preview: Optional[bool] = False,\n",
    "        task: Optional[str] = None,\n",
    "        roi_subset: Optional[str] = None,\n",
    "        roi_subset_robust: Optional[str] = None,\n",
    "        statistics: Optional[bool] = False,\n",
    "        radiomics: Optional[bool] = False,\n",
    "        stats_include_incomplete: Optional[bool] = False,\n",
    "        crop_path: Optional[str] = None,\n",
    "        body_seg: Optional[bool] = False,\n",
    "        force_split: Optional[bool] = False,\n",
    "        skip_saving: Optional[bool] = False,\n",
    "        no_derived_masks: Optional[bool] = False,\n",
    "        device: Optional[str] = None,\n",
    "        quiet: Optional[bool] = False,\n",
    "        verbose: Optional[bool] = False,\n",
    "        license_number: Optional[str] = None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Run TotalSegmentator to segment anatomical structures in CT or MR images.\n",
    "        \n",
    "        Args:\n",
    "            input_path: Path to input CT/MR nifti image or folder of dicom slices\n",
    "            output_dir: Output directory for segmentation masks\n",
    "            output_type: Save segmentations as 'nifti' or 'dicom'\n",
    "            multilabel: Save one multilabel image for all classes\n",
    "            nr_threads_resampling: Number of threads for resampling\n",
    "            nr_threads_saving: Number of threads for saving segmentations\n",
    "            fast: Run faster lower resolution model (3mm)\n",
    "            fastest: Run even faster lower resolution model (6mm)\n",
    "            nora_tag: Tag in nora as mask\n",
    "            preview: Generate a png preview of segmentation\n",
    "            task: Select which model to use. Tasks ending with '_mr' are for MR images\n",
    "            roi_subset: Define a subset of classes to save (comma separated)\n",
    "            roi_subset_robust: Like roi_subset but uses a more robust model\n",
    "            statistics: Calculate volume and mean intensity\n",
    "            radiomics: Calculate radiomics features\n",
    "            stats_include_incomplete: Calculate statistics for incomplete ROIs\n",
    "            crop_path: Custom path to masks used for cropping\n",
    "            body_seg: Do initial rough body segmentation\n",
    "            force_split: Process image in 3 chunks for less memory consumption\n",
    "            skip_saving: Skip saving of segmentations\n",
    "            no_derived_masks: Do not create derived masks\n",
    "            device: Device to use for inference ('gpu', 'cpu', or 'mps')\n",
    "            quiet: Suppress console output\n",
    "            verbose: Verbose output\n",
    "            license_number: License number for TotalSegmentator\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with segmentation results and stats\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Run TotalSegmentator\n",
    "            result = self._run_totalsegmentator(\n",
    "                input_path=input_path,\n",
    "                output_dir=output_dir,\n",
    "                output_type=output_type,\n",
    "                multilabel=multilabel,\n",
    "                nr_threads_resampling=nr_threads_resampling,\n",
    "                nr_threads_saving=nr_threads_saving,\n",
    "                fast=fast,\n",
    "                fastest=fastest,\n",
    "                nora_tag=nora_tag,\n",
    "                preview=preview,\n",
    "                task=task,\n",
    "                roi_subset=roi_subset,\n",
    "                roi_subset_robust=roi_subset_robust,\n",
    "                statistics=statistics,\n",
    "                radiomics=radiomics,\n",
    "                stats_include_incomplete=stats_include_incomplete,\n",
    "                crop_path=crop_path,\n",
    "                body_seg=body_seg,\n",
    "                force_split=force_split,\n",
    "                skip_saving=skip_saving,\n",
    "                no_derived_masks=no_derived_masks,\n",
    "                device=device,\n",
    "                quiet=quiet,\n",
    "                verbose=verbose,\n",
    "                license_number=license_number\n",
    "            )\n",
    "            \n",
    "            # Collect segmentation files\n",
    "            segmentation_files = []\n",
    "            if not skip_saving:\n",
    "                segmentation_files = self._get_segmentation_files(output_dir)\n",
    "            \n",
    "            # Collect statistics if requested\n",
    "            stats = None\n",
    "            if statistics:\n",
    "                stats_file = os.path.join(output_dir, \"statistics.json\")\n",
    "                if os.path.exists(stats_file):\n",
    "                    with open(stats_file, 'r') as f:\n",
    "                        stats = json.load(f)\n",
    "            \n",
    "            # Collect radiomics features if requested\n",
    "            radiomics_stats = None\n",
    "            if radiomics:\n",
    "                radiomics_file = os.path.join(output_dir, \"statistics_radiomics.json\")\n",
    "                if os.path.exists(radiomics_file):\n",
    "                    with open(radiomics_file, 'r') as f:\n",
    "                        radiomics_stats = json.load(f)\n",
    "            \n",
    "            # Add task information to results\n",
    "            task_info = {\n",
    "                \"task\": task if task else \"total\",\n",
    "                \"is_mr_task\": bool(task and task.endswith(\"_mr\")) if task else False,\n",
    "                \"image_type\": \"MR\" if (task and task.endswith(\"_mr\")) else \"CT\"\n",
    "            }\n",
    "            \n",
    "            return {\n",
    "                \"status\": \"success\",\n",
    "                \"output_dir\": output_dir,\n",
    "                \"command_output\": result,\n",
    "                \"segmentation_files\": segmentation_files,\n",
    "                \"statistics\": stats,\n",
    "                \"radiomics\": radiomics_stats,\n",
    "                \"preview_file\": os.path.join(output_dir, \"preview.png\") if preview and os.path.exists(os.path.join(output_dir, \"preview.png\")) else None,\n",
    "                \"task_info\": task_info\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {\n",
    "                \"status\": \"error\",\n",
    "                \"error_message\": str(e),\n",
    "                \"input_path\": input_path,\n",
    "                \"output_dir\": output_dir\n",
    "            }\n",
    "    \n",
    "    def _run_totalsegmentator(\n",
    "        self,\n",
    "        input_path: str,\n",
    "        output_dir: str,\n",
    "        output_type: Optional[str] = None,\n",
    "        multilabel: Optional[bool] = False,\n",
    "        nr_threads_resampling: Optional[int] = None,\n",
    "        nr_threads_saving: Optional[int] = None,\n",
    "        fast: Optional[bool] = False,\n",
    "        fastest: Optional[bool] = False,\n",
    "        nora_tag: Optional[str] = None,\n",
    "        preview: Optional[bool] = False,\n",
    "        task: Optional[str] = None,\n",
    "        roi_subset: Optional[str] = None,\n",
    "        roi_subset_robust: Optional[str] = None,\n",
    "        statistics: Optional[bool] = False,\n",
    "        radiomics: Optional[bool] = False,\n",
    "        stats_include_incomplete: Optional[bool] = False,\n",
    "        crop_path: Optional[str] = None,\n",
    "        body_seg: Optional[bool] = False,\n",
    "        force_split: Optional[bool] = False,\n",
    "        skip_saving: Optional[bool] = False,\n",
    "        no_derived_masks: Optional[bool] = False,\n",
    "        device: Optional[str] = None,\n",
    "        quiet: Optional[bool] = False,\n",
    "        verbose: Optional[bool] = False,\n",
    "        license_number: Optional[str] = None\n",
    "    ) -> str:\n",
    "        \"\"\"\n",
    "        Execute TotalSegmentator command.\n",
    "        \n",
    "        Args:\n",
    "            Same as forward method\n",
    "            \n",
    "        Returns:\n",
    "            Command output as string\n",
    "        \"\"\"\n",
    "        # Create output directory if it doesn't exist\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        # Build command for TotalSegmentator\n",
    "        cmd = [\"TotalSegmentator\", \"-i\", input_path, \"-o\", output_dir]\n",
    "        \n",
    "        # Add optional arguments\n",
    "        if output_type:\n",
    "            if output_type.lower() in [\"nifti\", \"dicom\"]:\n",
    "                cmd.extend([\"-ot\", output_type.lower()])\n",
    "            else:\n",
    "                raise ValueError(\"output_type must be 'nifti' or 'dicom'\")\n",
    "        \n",
    "        if multilabel:\n",
    "            cmd.append(\"-ml\")\n",
    "            \n",
    "        if nr_threads_resampling is not None:\n",
    "            cmd.extend([\"-nr\", str(nr_threads_resampling)])\n",
    "            \n",
    "        if nr_threads_saving is not None:\n",
    "            cmd.extend([\"-ns\", str(nr_threads_saving)])\n",
    "        \n",
    "        if fast:\n",
    "            cmd.append(\"-f\")\n",
    "            \n",
    "        if fastest:\n",
    "            cmd.append(\"-ff\")\n",
    "            \n",
    "        if nora_tag:\n",
    "            cmd.extend([\"-t\", nora_tag])\n",
    "            \n",
    "        if preview:\n",
    "            cmd.append(\"-p\")\n",
    "            \n",
    "        if task:\n",
    "            if task in self.AVAILABLE_TASKS:\n",
    "                cmd.extend([\"-ta\", task])\n",
    "            else:\n",
    "                valid_tasks = \", \".join(self.AVAILABLE_TASKS)\n",
    "                raise ValueError(f\"task must be one of: {valid_tasks}\")\n",
    "                \n",
    "        if roi_subset:\n",
    "            # Convert comma-separated string to space-separated list for command line\n",
    "            roi_list = roi_subset.split(\",\")\n",
    "            roi_list = [roi.strip() for roi in roi_list]\n",
    "            cmd.extend([\"-rs\"] + roi_list)\n",
    "            \n",
    "        if roi_subset_robust:\n",
    "            # Convert comma-separated string to space-separated list for command line\n",
    "            roi_robust_list = roi_subset_robust.split(\",\")\n",
    "            roi_robust_list = [roi.strip() for roi in roi_robust_list]\n",
    "            cmd.extend([\"-rsr\"] + roi_robust_list)\n",
    "            \n",
    "        if statistics:\n",
    "            cmd.append(\"-s\")\n",
    "            \n",
    "        if radiomics:\n",
    "            cmd.append(\"-r\")\n",
    "            \n",
    "        if stats_include_incomplete:\n",
    "            cmd.append(\"-sii\")\n",
    "            \n",
    "        if crop_path:\n",
    "            cmd.extend([\"-cp\", crop_path])\n",
    "            \n",
    "        if body_seg:\n",
    "            cmd.append(\"-bs\")\n",
    "            \n",
    "        if force_split:\n",
    "            cmd.append(\"-fs\")\n",
    "            \n",
    "        if skip_saving:\n",
    "            cmd.append(\"-ss\")\n",
    "            \n",
    "        if no_derived_masks:\n",
    "            cmd.append(\"-ndm\")\n",
    "            \n",
    "        if device:\n",
    "            if device.lower() in [\"gpu\", \"cpu\", \"mps\"]:\n",
    "                cmd.extend([\"-d\", device.lower()])\n",
    "            else:\n",
    "                raise ValueError(\"device must be 'gpu', 'cpu', or 'mps'\")\n",
    "                \n",
    "        if quiet:\n",
    "            cmd.append(\"-q\")\n",
    "            \n",
    "        if verbose:\n",
    "            cmd.append(\"-v\")\n",
    "            \n",
    "        if license_number:\n",
    "            cmd.extend([\"-l\", license_number])\n",
    "        \n",
    "        # Run TotalSegmentator\n",
    "        print(f\"Running command: {' '.join(cmd)}\")\n",
    "        process = subprocess.run(cmd, capture_output=True, text=True)\n",
    "        \n",
    "        if process.returncode != 0:\n",
    "            raise RuntimeError(f\"TotalSegmentator failed: {process.stderr}\")\n",
    "        \n",
    "        return process.stdout\n",
    "    \n",
    "    def _get_segmentation_files(self, output_dir: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        Get list of segmentation files in the output directory.\n",
    "        \n",
    "        Args:\n",
    "            output_dir: Path to the output directory\n",
    "            \n",
    "        Returns:\n",
    "            List of segmentation file paths\n",
    "        \"\"\"\n",
    "        files = []\n",
    "        \n",
    "        # Get all files in the output directory\n",
    "        for root, _, filenames in os.walk(output_dir):\n",
    "            for filename in filenames:\n",
    "                # Skip statistics and preview files\n",
    "                if filename in [\"statistics.json\", \"statistics_radiomics.json\", \"preview.png\"]:\n",
    "                    continue\n",
    "                \n",
    "                # Include nifti and dicom files\n",
    "                if filename.endswith(('.nii.gz', '.nii', '.dcm')):\n",
    "                    files.append(os.path.join(root, filename))\n",
    "        \n",
    "        return files\n",
    "\n",
    "totalsegmentator_tool = TotalSegmentatorTool()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification Model Training and Inference Tool (for tabulated data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PyCaretClassificationTool(Tool):\n",
    "    name = \"pycaret_classification\"\n",
    "    description = \"\"\"\n",
    "    This tool uses PyCaret to train and evaluate classification models.\n",
    "    It compares multiple models, tunes the best ones, creates a blended model,\n",
    "    and generates various visualizations and interpretations.\n",
    "    Results are saved to the specified output directory.\n",
    "    \"\"\"\n",
    "    \n",
    "    inputs = {\n",
    "        \"input_path\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"Path to input data file (CSV format)\"\n",
    "        },\n",
    "        \"output_dir\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"Output directory where model and results will be saved\"\n",
    "        },\n",
    "        \"target_column\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"Name of the target column for classification\"\n",
    "        },\n",
    "        \"experiment_name\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"Name of the experiment\",\n",
    "            \"required\": False,\n",
    "            \"nullable\": True\n",
    "        },\n",
    "        \"fold\": {\n",
    "            \"type\": \"integer\",\n",
    "            \"description\": \"Number of cross-validation folds\",\n",
    "            \"required\": False,\n",
    "            \"nullable\": True\n",
    "        },\n",
    "        \"session_id\": {\n",
    "            \"type\": \"integer\",\n",
    "            \"description\": \"Random seed for reproducibility\",\n",
    "            \"required\": False,\n",
    "            \"nullable\": True\n",
    "        },\n",
    "        \"use_gpu\": {\n",
    "            \"type\": \"boolean\",\n",
    "            \"description\": \"Whether to use GPU for training (if available)\",\n",
    "            \"required\": False,\n",
    "            \"nullable\": True\n",
    "        },\n",
    "        \"fix_imbalance\": {\n",
    "            \"type\": \"boolean\",\n",
    "            \"description\": \"Whether to fix class imbalance\",\n",
    "            \"required\": False,\n",
    "            \"nullable\": True\n",
    "        },\n",
    "        \"data_split_stratify\": {\n",
    "            \"type\": \"boolean\",\n",
    "            \"description\": \"Whether to use stratified sampling for data splitting\",\n",
    "            \"required\": False,\n",
    "            \"nullable\": True\n",
    "        },\n",
    "        \"data_split_shuffle\": {\n",
    "            \"type\": \"boolean\",\n",
    "            \"description\": \"Whether to shuffle data before splitting\",\n",
    "            \"required\": False,\n",
    "            \"nullable\": True\n",
    "        },\n",
    "        \"preprocess\": {\n",
    "            \"type\": \"boolean\",\n",
    "            \"description\": \"Whether to apply preprocessing steps\",\n",
    "            \"required\": False,\n",
    "            \"nullable\": True\n",
    "        },\n",
    "        \"ignore_features\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"Comma-separated list of features to ignore during training\",\n",
    "            \"required\": False,\n",
    "            \"nullable\": True\n",
    "        },\n",
    "        \"numeric_features\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"Comma-separated list of numeric features\",\n",
    "            \"required\": False,\n",
    "            \"nullable\": True\n",
    "        },\n",
    "        \"categorical_features\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"Comma-separated list of categorical features\",\n",
    "            \"required\": False,\n",
    "            \"nullable\": True\n",
    "        },\n",
    "        \"date_features\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"Comma-separated list of date features\",\n",
    "            \"required\": False,\n",
    "            \"nullable\": True\n",
    "        },\n",
    "        \"n_select\": {\n",
    "            \"type\": \"integer\",\n",
    "            \"description\": \"Number of top models to select for blending\",\n",
    "            \"required\": False,\n",
    "            \"nullable\": True\n",
    "        },\n",
    "        \"normalize\": {\n",
    "            \"type\": \"boolean\",\n",
    "            \"description\": \"Whether to normalize numeric features\",\n",
    "            \"required\": False,\n",
    "            \"nullable\": True\n",
    "        },\n",
    "        \"transformation\": {\n",
    "            \"type\": \"boolean\",\n",
    "            \"description\": \"Whether to apply transformation to numeric features\",\n",
    "            \"required\": False,\n",
    "            \"nullable\": True\n",
    "        },\n",
    "        \"pca\": {\n",
    "            \"type\": \"boolean\",\n",
    "            \"description\": \"Whether to apply PCA for dimensionality reduction\",\n",
    "            \"required\": False,\n",
    "            \"nullable\": True\n",
    "        },\n",
    "        \"pca_components\": {\n",
    "            \"type\": \"number\",\n",
    "            \"description\": \"Number of PCA components (float between 0-1 or int > 1)\",\n",
    "            \"required\": False,\n",
    "            \"nullable\": True\n",
    "        },\n",
    "        \"include_models\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"Comma-separated list of models to include in comparison\",\n",
    "            \"required\": False,\n",
    "            \"nullable\": True\n",
    "        },\n",
    "        \"exclude_models\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"Comma-separated list of models to exclude from comparison\",\n",
    "            \"required\": False,\n",
    "            \"nullable\": True\n",
    "        },\n",
    "        \"test_data_path\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"Path to test/holdout data for independent evaluation\",\n",
    "            \"required\": False,\n",
    "            \"nullable\": True\n",
    "        },\n",
    "        \"ignore_gpu_errors\": {\n",
    "            \"type\": \"boolean\",\n",
    "            \"description\": \"Whether to ignore GPU-related errors and fall back to CPU\",\n",
    "            \"required\": False,\n",
    "            \"nullable\": True\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    output_type = \"object\"\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_path: str,\n",
    "        output_dir: str,\n",
    "        target_column: str,\n",
    "        experiment_name: Optional[str] = None,\n",
    "        fold: Optional[int] = 10,\n",
    "        session_id: Optional[int] = None,\n",
    "        use_gpu: Optional[bool] = False,\n",
    "        fix_imbalance: Optional[bool] = True,\n",
    "        data_split_stratify: Optional[bool] = True,\n",
    "        data_split_shuffle: Optional[bool] = True,\n",
    "        preprocess: Optional[bool] = True,\n",
    "        ignore_features: Optional[str] = None,\n",
    "        numeric_features: Optional[str] = None,\n",
    "        categorical_features: Optional[str] = None,\n",
    "        date_features: Optional[str] = None,\n",
    "        n_select: Optional[int] = 3,\n",
    "        normalize: Optional[bool] = False,\n",
    "        transformation: Optional[bool] = False,\n",
    "        pca: Optional[bool] = False,\n",
    "        pca_components: Optional[float] = None,\n",
    "        include_models: Optional[str] = None,\n",
    "        exclude_models: Optional[str] = None,\n",
    "        test_data_path: Optional[str] = None,\n",
    "        ignore_gpu_errors: Optional[bool] = True\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Train and evaluate classification models using PyCaret.\n",
    "        \n",
    "        Args:\n",
    "            input_path: Path to input CSV file with training data\n",
    "            output_dir: Directory to save outputs\n",
    "            target_column: Target column name for classification\n",
    "            experiment_name: Name of the experiment\n",
    "            fold: Number of cross-validation folds\n",
    "            session_id: Random seed for reproducibility\n",
    "            use_gpu: Whether to use GPU for training\n",
    "            fix_imbalance: Whether to fix class imbalance\n",
    "            data_split_stratify: Whether to use stratified sampling\n",
    "            data_split_shuffle: Whether to shuffle data before splitting\n",
    "            preprocess: Whether to apply preprocessing steps\n",
    "            ignore_features: Comma-separated list of features to ignore\n",
    "            numeric_features: Comma-separated list of numeric features\n",
    "            categorical_features: Comma-separated list of categorical features\n",
    "            date_features: Comma-separated list of date features\n",
    "            n_select: Number of top models to select for blending\n",
    "            normalize: Whether to normalize numeric features\n",
    "            transformation: Whether to apply transformation to numeric features\n",
    "            pca: Whether to apply PCA for dimensionality reduction\n",
    "            pca_components: Number of PCA components\n",
    "            include_models: Comma-separated list of models to include\n",
    "            exclude_models: Comma-separated list of models to exclude\n",
    "            test_data_path: Path to test/holdout data for independent evaluation\n",
    "            ignore_gpu_errors: Whether to ignore GPU errors and fallback to CPU\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with model training results and file paths\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Setup logging\n",
    "            os.makedirs(output_dir, exist_ok=True)\n",
    "            log_file = os.path.join(output_dir, \"pycaret_classification.log\")\n",
    "            \n",
    "            logging_format = '%(asctime)s - %(levelname)s - %(message)s'\n",
    "            logging.basicConfig(level=logging.INFO, \n",
    "                               format=logging_format,\n",
    "                               handlers=[\n",
    "                                   logging.FileHandler(log_file),\n",
    "                                   logging.StreamHandler()\n",
    "                               ])\n",
    "            \n",
    "            # Generate experiment name if not provided\n",
    "            if experiment_name is None:\n",
    "                timestamp = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "                experiment_name = f\"classification_exp_{timestamp}\"\n",
    "            \n",
    "            # Generate session_id if not provided\n",
    "            if session_id is None:\n",
    "                import random\n",
    "                session_id = random.randint(1, 10000)\n",
    "            \n",
    "            logging.info(f\"Starting classification experiment: {experiment_name}\")\n",
    "            logging.info(f\"Input data: {input_path}\")\n",
    "            logging.info(f\"Output directory: {output_dir}\")\n",
    "            logging.info(f\"Target column: {target_column}\")\n",
    "            \n",
    "            # Import PyCaret's classification module\n",
    "            try:\n",
    "                from pycaret.classification import (\n",
    "                    setup, compare_models, tune_model, blend_models, \n",
    "                    pull, predict_model, save_model, load_model,\n",
    "                    plot_model, interpret_model\n",
    "                )\n",
    "                \n",
    "                # Check PyCaret version for compatibility adjustments\n",
    "                import pycaret\n",
    "                pycaret_version = getattr(pycaret, \"__version__\", \"unknown\")\n",
    "                logging.info(f\"PyCaret version: {pycaret_version}\")\n",
    "                \n",
    "                logging.info(\"PyCaret imported successfully\")\n",
    "            except ImportError as e:\n",
    "                logging.error(f\"Error importing PyCaret: {str(e)}\")\n",
    "                logging.error(\"Please install PyCaret: pip install pycaret\")\n",
    "                raise ImportError(\"PyCaret is required for this tool. Please install it with: pip install pycaret\")\n",
    "            \n",
    "            # Load data\n",
    "            logging.info(f\"Loading data from {input_path}\")\n",
    "            data = pd.read_csv(input_path)\n",
    "            logging.info(f\"Data loaded successfully: {data.shape[0]} rows, {data.shape[1]} columns\")\n",
    "            \n",
    "            # Verify target column exists\n",
    "            if target_column not in data.columns:\n",
    "                raise ValueError(f\"Target column '{target_column}' not found in the dataset\")\n",
    "            \n",
    "            # Prepare parameters for setup\n",
    "            setup_params = {\n",
    "                'data': data,\n",
    "                'target': target_column,\n",
    "                'session_id': session_id,\n",
    "                'experiment_name': experiment_name,\n",
    "                'fold': fold,\n",
    "                'use_gpu': use_gpu,\n",
    "                'fix_imbalance': fix_imbalance,\n",
    "                'preprocess': preprocess,\n",
    "                'data_split_stratify': data_split_stratify,\n",
    "                'data_split_shuffle': data_split_shuffle,\n",
    "                'log_experiment': False  # Set to False to avoid MLflow errors\n",
    "            }\n",
    "            \n",
    "            # Check if it's a compatible version to use experiment_logs\n",
    "            # For now, let's disable experiment logs altogether since it causes issues\n",
    "            # with MLflow in some environments\n",
    "            \"\"\"\n",
    "            try:\n",
    "                import pycaret\n",
    "                pycaret_version = pycaret.__version__\n",
    "                logging.info(f\"PyCaret version: {pycaret_version}\")\n",
    "                \n",
    "                # Check if newer version that supports experiment_logs\n",
    "                from packaging import version\n",
    "                if version.parse(pycaret_version) >= version.parse('2.3.0'):\n",
    "                    setup_params['experiment_logs'] = os.path.join(output_dir, 'logs')\n",
    "            except (ImportError, AttributeError) as e:\n",
    "                logging.warning(f\"Could not determine PyCaret version: {str(e)}\")\n",
    "                logging.warning(\"Some parameters might not be compatible.\")\n",
    "            except Exception as e:\n",
    "                logging.warning(f\"Error checking PyCaret version: {str(e)}\")\n",
    "                # Continue without adding version-specific parameters\n",
    "            \"\"\"\n",
    "            logging.info(\"Disabling experiment logging to avoid MLflow-related errors\")\n",
    "                \n",
    "            # Add optional parameters if provided\n",
    "            if ignore_features:\n",
    "                setup_params['ignore_features'] = [f.strip() for f in ignore_features.split(',')]\n",
    "            \n",
    "            if numeric_features:\n",
    "                setup_params['numeric_features'] = [f.strip() for f in numeric_features.split(',')]\n",
    "            \n",
    "            if categorical_features:\n",
    "                setup_params['categorical_features'] = [f.strip() for f in categorical_features.split(',')]\n",
    "            \n",
    "            if date_features:\n",
    "                setup_params['date_features'] = [f.strip() for f in date_features.split(',')]\n",
    "            \n",
    "            if normalize is not None:\n",
    "                setup_params['normalize'] = normalize\n",
    "                \n",
    "            if transformation is not None:\n",
    "                setup_params['transformation'] = transformation\n",
    "                \n",
    "            if pca is not None:\n",
    "                setup_params['pca'] = pca\n",
    "                \n",
    "            if pca_components is not None:\n",
    "                setup_params['pca_components'] = pca_components\n",
    "            \n",
    "            # Check GPU availability and cuml installation if use_gpu is requested\n",
    "            if use_gpu:\n",
    "                try:\n",
    "                    import cuml\n",
    "                    logging.info(\"RAPIDS cuML is available for GPU acceleration\")\n",
    "                except ImportError:\n",
    "                    logging.warning(\"'cuml' is not installed but use_gpu=True was specified.\")\n",
    "                    if ignore_gpu_errors:\n",
    "                        logging.warning(\"Running on CPU instead. To use GPU, install cuml: pip install cuml\")\n",
    "                        use_gpu = False\n",
    "                        setup_params['use_gpu'] = False\n",
    "                    else:\n",
    "                        raise ImportError(\"GPU acceleration requested but cuml is not installed. Install with: pip install cuml\")\n",
    "            \n",
    "            # Set up the experiment with error handling for parameter compatibility\n",
    "            logging.info(\"Setting up PyCaret experiment\")\n",
    "            logging.info(f\"Setup parameters: {setup_params}\")\n",
    "            \n",
    "            # Try to create setup with default parameters first\n",
    "            try:\n",
    "                s = setup(**setup_params)\n",
    "                logging.info(\"PyCaret setup completed successfully\")\n",
    "            except TypeError as e:\n",
    "                logging.warning(f\"Setup error: {str(e)}\")\n",
    "                if \"unexpected keyword argument\" in str(e):\n",
    "                    # Handle incompatible parameters by removing them and retrying\n",
    "                    error_param = str(e).split(\"argument \")[-1].split(\"'\")[1] if \"'\" in str(e) else str(e).split(\"argument \")[-1].strip()\n",
    "                    logging.warning(f\"Incompatible parameter detected: {error_param}\")\n",
    "                    logging.warning(f\"Removing parameter and retrying setup\")\n",
    "                    \n",
    "                    if error_param in setup_params:\n",
    "                        del setup_params[error_param]\n",
    "                        try:\n",
    "                            s = setup(**setup_params)\n",
    "                            logging.info(\"PyCaret setup completed successfully after parameter adjustment\")\n",
    "                        except Exception as inner_e:\n",
    "                            logging.error(f\"Setup failed even after removing parameter: {str(inner_e)}\")\n",
    "                            # Try a minimal setup as last resort\n",
    "                            minimal_params = {\n",
    "                                'data': data,\n",
    "                                'target': target_column,\n",
    "                                'session_id': session_id\n",
    "                            }\n",
    "                            logging.warning(f\"Attempting minimal setup with just: {minimal_params.keys()}\")\n",
    "                            s = setup(**minimal_params)\n",
    "                            logging.info(\"PyCaret setup completed with minimal parameters\")\n",
    "                    else:\n",
    "                        raise\n",
    "                else:\n",
    "                    raise\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Unexpected setup error: {str(e)}\")\n",
    "                # Try a minimal setup as last resort\n",
    "                minimal_params = {\n",
    "                    'data': data,\n",
    "                    'target': target_column,\n",
    "                    'session_id': session_id\n",
    "                }\n",
    "                logging.warning(f\"Attempting minimal setup with just: {minimal_params.keys()}\")\n",
    "                s = setup(**minimal_params)\n",
    "                logging.info(\"PyCaret setup completed with minimal parameters\")\n",
    "            \n",
    "            # Prepare parameters for compare_models\n",
    "            compare_params = {'n_select': n_select}\n",
    "            \n",
    "            if include_models:\n",
    "                compare_params['include'] = [m.strip() for m in include_models.split(',')]\n",
    "                \n",
    "            if exclude_models:\n",
    "                compare_params['exclude'] = [m.strip() for m in exclude_models.split(',')]\n",
    "            \n",
    "            # Compare baseline models with compatibility handling\n",
    "            logging.info(f\"Comparing models (selecting top {n_select} models)\")\n",
    "            try:\n",
    "                best = compare_models(**compare_params)\n",
    "            except TypeError as e:\n",
    "                if \"unexpected keyword argument\" in str(e):\n",
    "                    # Handle incompatible parameters\n",
    "                    error_param = str(e).split(\"argument \")[-1].split(\"'\")[1] if \"'\" in str(e) else str(e).split(\"argument \")[-1].strip()\n",
    "                    logging.warning(f\"Incompatible parameter detected in compare_models: {error_param}\")\n",
    "                    logging.warning(f\"Removing parameter and retrying\")\n",
    "                    \n",
    "                    if error_param in compare_params:\n",
    "                        del compare_params[error_param]\n",
    "                        best = compare_models(**compare_params)\n",
    "                    else:\n",
    "                        raise\n",
    "                else:\n",
    "                    raise\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error in compare_models: {str(e)}\")\n",
    "                # Try with minimal parameters\n",
    "                logging.warning(\"Attempting compare_models with minimal parameters\")\n",
    "                best = compare_models()\n",
    "            \n",
    "            # If only one model is returned, wrap it in a list\n",
    "            if not isinstance(best, list):\n",
    "                best = [best]\n",
    "                logging.info(\"Only one model was selected\")\n",
    "            \n",
    "            logging.info(f\"Top {len(best)} models selected\")\n",
    "            \n",
    "            # Initialize list to track created files\n",
    "            created_files = []\n",
    "            \n",
    "            # Compare models results\n",
    "            compare_results = pull()\n",
    "            compare_results_path = os.path.join(output_dir, 'model_comparison_results.csv')\n",
    "            compare_results.to_csv(compare_results_path, index=True)\n",
    "            created_files.append(('Model Comparison Results', compare_results_path))\n",
    "            logging.info(f\"Saved model comparison results to {compare_results_path}\")\n",
    "            \n",
    "            # create_plots_for_model function \n",
    "            def create_plots_for_model(model, plots_dir, created_files):\n",
    "                plot_types = [\n",
    "                    'auc', 'confusion_matrix', 'class_report', 'feature', 'boundary',\n",
    "                    'pr', 'error', 'learning', 'manifold', 'calibration', 'vc', 'dimension', \n",
    "                    'feature_all', 'parameter', 'lift', 'gain']\n",
    "                \n",
    "                \n",
    "                model_plots = []\n",
    "                \n",
    "                # Store the original working directory\n",
    "                original_dir = os.getcwd()\n",
    "                \n",
    "                try:\n",
    "                    # Change to the plots directory before creating plots\n",
    "                    os.chdir(plots_dir)\n",
    "                    logging.info(f\"Changed working directory to {plots_dir}\")\n",
    "                    \n",
    "                    # Try to create each plot type\n",
    "                    for plot_type in plot_types:\n",
    "                        try:\n",
    "                            logging.info(f\"Attempting to create {plot_type} plot in {plots_dir}\")\n",
    "                            \n",
    "                            # Try to create the plot with save=True\n",
    "                            plot_model(model, plot=plot_type, save=True)\n",
    "                            \n",
    "                            # Check if file was created\n",
    "                            expected_filename = f\"{plot_type}.png\"\n",
    "                            if os.path.exists(expected_filename):\n",
    "                                full_path = os.path.join(plots_dir, expected_filename)\n",
    "                                model_plots.append((f'Plot: {plot_type}', full_path))\n",
    "                                logging.info(f\"Successfully created {plot_type} plot at {full_path}\")\n",
    "                            else:\n",
    "                                logging.warning(f\"Plot {plot_type} was not found after creation\")\n",
    "                            \n",
    "                        except Exception as e:\n",
    "                            logging.warning(f\"Failed to create {plot_type} plot: {str(e)}\")\n",
    "                \n",
    "                finally:\n",
    "                    # Always restore the original working directory\n",
    "                    os.chdir(original_dir)\n",
    "                    logging.info(f\"Restored working directory to {original_dir}\")\n",
    "                \n",
    "                return model_plots\n",
    "\n",
    "            #create_interpretations_for_model function\n",
    "            def create_interpretations_for_model(model, plots_dir, created_files):\n",
    "                model_interpretations = []\n",
    "                \n",
    "                # Store the original working directory\n",
    "                original_dir = os.getcwd()\n",
    "                \n",
    "                try:\n",
    "                    # Change to the plots directory before creating interpretations\n",
    "                    os.chdir(plots_dir)\n",
    "                    logging.info(f\"Changed working directory to {plots_dir}\")\n",
    "                    \n",
    "                    # Try to create summary interpretation\n",
    "                    try:\n",
    "                        logging.info(\"Attempting to create model interpretation summary\")\n",
    "                        interpret_model(model, plot='summary', save=True)\n",
    "                        \n",
    "                        # Check if file was created\n",
    "                        expected_filename = \"SHAP Summary.png\"\n",
    "                        if os.path.exists(expected_filename):\n",
    "                            full_path = os.path.join(plots_dir, expected_filename)\n",
    "                            model_interpretations.append(('Interpretation: summary', full_path))\n",
    "                            logging.info(f\"Successfully created interpretation summary at {full_path}\")\n",
    "                        else:\n",
    "                            logging.warning(\"Interpretation summary file was not found after creation\")\n",
    "                            \n",
    "                    except Exception as e:\n",
    "                        logging.warning(f\"Failed to create summary interpretation: {str(e)}\")\n",
    "                    \n",
    "                    # Try to create reason plot\n",
    "                    try:\n",
    "                        logging.info(\"Attempting to create model interpretation reason\")\n",
    "                        interpret_model(model, plot='reason', observation=1, save=True)\n",
    "                        \n",
    "                        # Check if file was created\n",
    "                        expected_filename = \"SHAP Reason Code.png\"\n",
    "                        if os.path.exists(expected_filename):\n",
    "                            full_path = os.path.join(plots_dir, expected_filename)\n",
    "                            model_interpretations.append(('Interpretation: reason', full_path))\n",
    "                            logging.info(f\"Successfully created interpretation reason at {full_path}\")\n",
    "                        else:\n",
    "                            logging.warning(\"Interpretation reason file was not found after creation\")\n",
    "                            \n",
    "                    except Exception as e:\n",
    "                        logging.warning(f\"Failed to create reason interpretation: {str(e)}\")\n",
    "                \n",
    "                finally:\n",
    "                    # Always restore the original working directory\n",
    "                    os.chdir(original_dir)\n",
    "                    logging.info(f\"Restored working directory to {original_dir}\")\n",
    "                \n",
    "                return model_interpretations\n",
    "            \n",
    "            # Function to evaluate model on test data \n",
    "            def evaluate_model_on_test_data(model, output_path, model_name):\n",
    "                try:\n",
    "                    logging.info(f\"Evaluating {model_name} on test data using PyCaret's inherent workflow\")\n",
    "                    \n",
    "                    # Use PyCaret's inherent workflow for holdout predictions\n",
    "                    # This uses the test split that was created during setup()\n",
    "                    holdout_pred = predict_model(model)\n",
    "                    \n",
    "                    # Save the predictions\n",
    "                    holdout_pred.to_csv(output_path, index=False)\n",
    "                    logging.info(f\"Saved {model_name} test predictions to {output_path}\")\n",
    "                    return output_path\n",
    "                except Exception as e:\n",
    "                    logging.error(f\"Error evaluating {model_name} on test data: {str(e)}\")\n",
    "                    return None\n",
    "\n",
    "            \n",
    "            # Create a top-level models directory\n",
    "            models_dir = os.path.join(output_dir, \"models\")\n",
    "            os.makedirs(models_dir, exist_ok=True)\n",
    "            \n",
    "            # Dictionary to track all individual model paths\n",
    "            individual_model_paths = {}\n",
    "            model_test_predictions = {}\n",
    "            \n",
    "            # Tune and save each model individually\n",
    "            tuned_models = []\n",
    "            for i, model in enumerate(best):\n",
    "                model_index = i + 1\n",
    "                model_name = f\"tuned_model_{model_index}\"\n",
    "                logging.info(f\"Tuning model {model_index}/{len(best)}\")\n",
    "                \n",
    "                try:\n",
    "                    # Create a dedicated directory for this model\n",
    "                    model_dir = os.path.join(models_dir, model_name)\n",
    "                    os.makedirs(model_dir, exist_ok=True)\n",
    "                    \n",
    "                    # Tune the model\n",
    "                    tuned_model = tune_model(model)\n",
    "                    tuned_models.append(tuned_model)\n",
    "                    \n",
    "                    # Save tuned model results\n",
    "                    tuned_results = pull()\n",
    "                    tuned_results_path = os.path.join(model_dir, f'{model_name}_results.csv')\n",
    "                    tuned_results.to_csv(tuned_results_path, index=True)\n",
    "                    created_files.append((f'{model_name} Results', tuned_results_path))\n",
    "                    logging.info(f\"Saved {model_name} results to {tuned_results_path}\")\n",
    "                    \n",
    "                    # Save the model itself\n",
    "                    model_path = os.path.join(model_dir, f'{model_name}')\n",
    "                    try:\n",
    "                        save_model(tuned_model, model_path)\n",
    "                        created_files.append((f'{model_name} Saved Model', model_path))\n",
    "                        individual_model_paths[model_name] = model_path\n",
    "                        logging.info(f\"Saved {model_name} to {model_path}\")\n",
    "                    except Exception as e:\n",
    "                        logging.error(f\"Error saving {model_name}: {str(e)}\")\n",
    "                    \n",
    "                    # Create plots directory for this model\n",
    "                    plots_dir = os.path.join(model_dir, \"plots\")\n",
    "                    os.makedirs(plots_dir, exist_ok=True)\n",
    "                    \n",
    "                    # Generate plots for this model\n",
    "                    logging.info(f\"Generating plots for {model_name}\")\n",
    "                    model_plots = create_plots_for_model(tuned_model, plots_dir, created_files)\n",
    "                    created_files.extend(model_plots)\n",
    "                    \n",
    "                    # Generate interpretations for this model\n",
    "                    logging.info(f\"Generating interpretations for {model_name}\")\n",
    "                    model_interpretations = create_interpretations_for_model(tuned_model, plots_dir, created_files)\n",
    "                    created_files.extend(model_interpretations)\n",
    "                    \n",
    "                    # Evaluate on holdout set using PyCaret's inherent workflow\n",
    "                    test_pred_path = os.path.join(model_dir, f'independent_eval_results_{model_name}.csv')\n",
    "                    test_result = evaluate_model_on_test_data(tuned_model, test_pred_path, model_name)\n",
    "                    if test_result:\n",
    "                        created_files.append((f'{model_name} Test Predictions', test_result))\n",
    "                        model_test_predictions[model_name] = test_result\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    logging.error(f\"Error processing {model_name}: {str(e)}\")\n",
    "            \n",
    "            # Create a blended model if we have multiple tuned models\n",
    "            blended_model = None\n",
    "            blended_model_path = None\n",
    "            blended_test_pred_path = None\n",
    "            \n",
    "            if len(tuned_models) > 1:\n",
    "                logging.info(\"Creating blended model from tuned models\")\n",
    "                try:\n",
    "                    # Create dedicated directory for blended model\n",
    "                    blended_dir = os.path.join(models_dir, \"blended_model\")\n",
    "                    os.makedirs(blended_dir, exist_ok=True)\n",
    "                    \n",
    "                    # Create the blended model\n",
    "                    blended_model = blend_models(tuned_models)\n",
    "                    \n",
    "                    # Save blended model results\n",
    "                    blended_results = pull()\n",
    "                    blended_results_path = os.path.join(blended_dir, 'blended_model_results.csv')\n",
    "                    blended_results.to_csv(blended_results_path, index=True)\n",
    "                    created_files.append(('Blended Model Results', blended_results_path))\n",
    "                    logging.info(f\"Saved blended model results to {blended_results_path}\")\n",
    "                    \n",
    "                    # Save the blended model\n",
    "                    blended_model_path = os.path.join(blended_dir, 'blended_model')\n",
    "                    try:\n",
    "                        save_model(blended_model, blended_model_path)\n",
    "                        created_files.append(('Blended Model Saved Model', blended_model_path))\n",
    "                        individual_model_paths[\"blended_model\"] = blended_model_path\n",
    "                        logging.info(f\"Saved blended model to {blended_model_path}\")\n",
    "                    except Exception as e:\n",
    "                        logging.error(f\"Error saving blended model: {str(e)}\")\n",
    "                    \n",
    "                    # Create plots directory for blended model\n",
    "                    plots_dir = os.path.join(blended_dir, \"plots\")\n",
    "                    os.makedirs(plots_dir, exist_ok=True)\n",
    "                    \n",
    "                    # Generate plots for blended model\n",
    "                    logging.info(\"Generating plots for blended model\")\n",
    "                    blended_plots = create_plots_for_model(blended_model, plots_dir, created_files)\n",
    "                    created_files.extend(blended_plots)\n",
    "                    \n",
    "                    # Generate interpretations for blended model\n",
    "                    logging.info(\"Generating interpretations for blended model\")\n",
    "                    blended_interpretations = create_interpretations_for_model(blended_model, plots_dir, created_files)\n",
    "                    created_files.extend(blended_interpretations)\n",
    "                    \n",
    "                    # Evaluate on holdout set using PyCaret's inherent workflow\n",
    "                    blended_test_pred_path = os.path.join(blended_dir, 'independent_eval_results_blended_model.csv')\n",
    "                    test_result = evaluate_model_on_test_data(blended_model, blended_test_pred_path, \"blended_model\")\n",
    "                    if test_result:\n",
    "                        created_files.append(('Blended Model Test Predictions', test_result))\n",
    "                        model_test_predictions[\"blended_model\"] = test_result\n",
    "                    \n",
    "                    final_model = blended_model\n",
    "                except Exception as e:\n",
    "                    logging.error(f\"Error creating blended model: {str(e)}\")\n",
    "                    logging.info(\"Using the best tuned model as the final model\")\n",
    "                    final_model = tuned_models[0]\n",
    "            else:\n",
    "                logging.info(\"Only one model available, using it as the final model\")\n",
    "                final_model = tuned_models[0] if tuned_models else best[0]\n",
    "            \n",
    "            # Save the final model to the main output directory as well\n",
    "            final_model_path = os.path.join(output_dir, f'{experiment_name}_final_model')\n",
    "            try:\n",
    "                save_model(final_model, final_model_path)\n",
    "                created_files.append(('Final Model', final_model_path))\n",
    "                logging.info(f\"Saved final model to {final_model_path}\")\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error saving final model: {str(e)}\")\n",
    "            \n",
    "            # Create summary report with links to all models and evaluations\n",
    "            try:\n",
    "                summary_path = os.path.join(output_dir, \"model_summary.csv\")\n",
    "                summary_data = []\n",
    "                \n",
    "                # Add individual models to summary\n",
    "                for i, model in enumerate(tuned_models):\n",
    "                    model_name = f\"tuned_model_{i+1}\"\n",
    "                    model_path = individual_model_paths.get(model_name, \"Not saved\")\n",
    "                    test_pred_path = model_test_predictions.get(model_name, \"Not available\")\n",
    "                    \n",
    "                    summary_data.append({\n",
    "                        \"Model\": model_name,\n",
    "                        \"Type\": \"Tuned Individual Model\",\n",
    "                        \"Model Path\": model_path,\n",
    "                        \"Test Predictions\": test_pred_path\n",
    "                    })\n",
    "                \n",
    "                # Add blended model to summary if it exists\n",
    "                if blended_model is not None:\n",
    "                    model_path = individual_model_paths.get(\"blended_model\", \"Not saved\")\n",
    "                    test_pred_path = model_test_predictions.get(\"blended_model\", \"Not available\")\n",
    "                    \n",
    "                    summary_data.append({\n",
    "                        \"Model\": \"blended_model\",\n",
    "                        \"Type\": \"Blended Model\",\n",
    "                        \"Model Path\": model_path,\n",
    "                        \"Test Predictions\": test_pred_path\n",
    "                    })\n",
    "                \n",
    "                # Write summary to CSV\n",
    "                summary_df = pd.DataFrame(summary_data)\n",
    "                summary_df.to_csv(summary_path, index=False)\n",
    "                created_files.append(('Model Summary', summary_path))\n",
    "                logging.info(f\"Created model summary at {summary_path}\")\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error creating model summary: {str(e)}\")\n",
    "            \n",
    "            logging.info(\"Classification modeling completed successfully\")\n",
    "            \n",
    "            return {\n",
    "                \"status\": \"success\",\n",
    "                \"experiment_name\": experiment_name,\n",
    "                \"input_path\": input_path,\n",
    "                \"output_dir\": output_dir,\n",
    "                \"final_model_path\": final_model_path,\n",
    "                \"individual_model_paths\": individual_model_paths,\n",
    "                \"model_test_predictions\": model_test_predictions,\n",
    "                \"model_summary\": summary_path if 'summary_path' in locals() else None,\n",
    "                \"log_file\": log_file,\n",
    "                \"created_files\": created_files\n",
    "            }\n",
    "        \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error in PyCaret classification: {str(e)}\", exc_info=True)\n",
    "            return {\n",
    "                \"status\": \"error\",\n",
    "                \"error_message\": str(e),\n",
    "                \"input_path\": input_path,\n",
    "                \"output_dir\": output_dir\n",
    "            }\n",
    "\n",
    "class PyCaretInferenceTool(Tool):\n",
    "    name = \"pycaret_inference\"\n",
    "    description = \"\"\"\n",
    "    This tool uses a saved PyCaret classification model to make predictions on new data.\n",
    "    It can also calculate performance metrics if ground truth data is available.\n",
    "    \"\"\"\n",
    "    \n",
    "    inputs = {\n",
    "        \"input_path\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"Path to input data file (CSV format) for inference\"\n",
    "        },\n",
    "        \"model_path\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"Path to the saved PyCaret model\"\n",
    "        },\n",
    "        \"output_dir\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"Directory where prediction results will be saved (if not specified, uses same directory as input file)\",\n",
    "            \"required\": False,\n",
    "            \"nullable\": True\n",
    "        },\n",
    "        \"ground_truth_column\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"Name of the column containing ground truth values (if available for metrics calculation)\",\n",
    "            \"required\": False,\n",
    "            \"nullable\": True\n",
    "        },\n",
    "        \"prediction_filename\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"Name for the output prediction file (default: predictions.csv)\",\n",
    "            \"required\": False,\n",
    "            \"nullable\": True\n",
    "        },\n",
    "        \"verbose\": {\n",
    "            \"type\": \"boolean\",\n",
    "            \"description\": \"Whether to print detailed logs\",\n",
    "            \"required\": False,\n",
    "            \"nullable\": True\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    output_type = \"object\"\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_path: str,\n",
    "        model_path: str,\n",
    "        output_dir: Optional[str] = None,\n",
    "        ground_truth_column: Optional[str] = None,\n",
    "        prediction_filename: Optional[str] = \"predictions.csv\",\n",
    "        verbose: Optional[bool] = True\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Run inference using a saved PyCaret model on new data.\n",
    "        \n",
    "        Args:\n",
    "            input_path: Path to input CSV file with data for inference\n",
    "            model_path: Path to the saved PyCaret model\n",
    "            output_dir: Directory to save prediction outputs (default: same directory as input file)\n",
    "            ground_truth_column: Name of the column containing ground truth values (if available)\n",
    "            prediction_filename: Name for the output prediction file\n",
    "            verbose: Whether to print detailed logs\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with inference results and file paths\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Set up logging\n",
    "            logging_format = '%(asctime)s - %(levelname)s - %(message)s'\n",
    "            logging_level = logging.INFO if verbose else logging.WARNING\n",
    "            \n",
    "            # Clear any existing handlers to avoid duplicates\n",
    "            logging.getLogger().handlers = []\n",
    "            \n",
    "            logging.basicConfig(level=logging_level, \n",
    "                               format=logging_format,\n",
    "                               handlers=[logging.StreamHandler()])\n",
    "            \n",
    "            # Set output directory (default to input file directory if not specified)\n",
    "            if output_dir is None:\n",
    "                output_dir = os.path.dirname(input_path)\n",
    "                if not output_dir:  # If input_path doesn't have a directory component\n",
    "                    output_dir = \".\"\n",
    "            \n",
    "            # Create output directory if it doesn't exist\n",
    "            os.makedirs(output_dir, exist_ok=True)\n",
    "            \n",
    "            # Set up log file in output directory\n",
    "            log_file = os.path.join(output_dir, \"pycaret_inference.log\")\n",
    "            file_handler = logging.FileHandler(log_file)\n",
    "            file_handler.setFormatter(logging.Formatter(logging_format))\n",
    "            logging.getLogger().addHandler(file_handler)\n",
    "            \n",
    "            logging.info(f\"Starting inference using model: {model_path}\")\n",
    "            logging.info(f\"Input data: {input_path}\")\n",
    "            logging.info(f\"Output directory: {output_dir}\")\n",
    "            \n",
    "            # Import PyCaret's classification module\n",
    "            try:\n",
    "                from pycaret.classification import load_model, predict_model\n",
    "                logging.info(\"PyCaret imported successfully\")\n",
    "            except ImportError as e:\n",
    "                logging.error(f\"Error importing PyCaret: {str(e)}\")\n",
    "                logging.error(\"Please install PyCaret: pip install pycaret\")\n",
    "                raise ImportError(\"PyCaret is required for this tool. Please install it with: pip install pycaret\")\n",
    "            \n",
    "            # Load the data\n",
    "            logging.info(f\"Loading data from {input_path}\")\n",
    "            try:\n",
    "                data = pd.read_csv(input_path)\n",
    "                logging.info(f\"Data loaded successfully: {data.shape[0]} rows, {data.shape[1]} columns\")\n",
    "                logging.info(f\"Data columns: {data.columns.tolist()}\")\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error loading data: {str(e)}\")\n",
    "                return {\n",
    "                    \"status\": \"error\",\n",
    "                    \"error_message\": f\"Failed to load data: {str(e)}\",\n",
    "                    \"input_path\": input_path\n",
    "                }\n",
    "            \n",
    "            # Check if ground truth column exists if specified\n",
    "            has_ground_truth = False\n",
    "            if ground_truth_column:\n",
    "                if ground_truth_column in data.columns:\n",
    "                    has_ground_truth = True\n",
    "                    logging.info(f\"Ground truth column '{ground_truth_column}' found in dataset\")\n",
    "                    # Store original ground truth data before any modifications\n",
    "                    original_ground_truth = data[ground_truth_column].copy()\n",
    "                else:\n",
    "                    logging.warning(f\"Ground truth column '{ground_truth_column}' not found in the dataset. Metrics won't be calculated.\")\n",
    "            \n",
    "            # Fix model path - remove .pkl extension if present since PyCaret adds it automatically\n",
    "            fixed_model_path = model_path\n",
    "            if fixed_model_path.endswith('.pkl'):\n",
    "                fixed_model_path = fixed_model_path[:-4]\n",
    "                logging.info(f\"Removed .pkl extension from model path, using: {fixed_model_path}\")\n",
    "            \n",
    "            # Check if the model directory exists\n",
    "            model_dir = os.path.dirname(fixed_model_path)\n",
    "            if not os.path.exists(model_dir):\n",
    "                logging.error(f\"Model directory does not exist: {model_dir}\")\n",
    "                return {\n",
    "                    \"status\": \"error\",\n",
    "                    \"error_message\": f\"Model directory not found: {model_dir}\",\n",
    "                    \"model_path\": model_path\n",
    "                }\n",
    "            \n",
    "            # Check if model file or directory exists\n",
    "            if not os.path.exists(fixed_model_path) and not os.path.exists(fixed_model_path + '.pkl'):\n",
    "                # Try to find the model in the directory by listing all files\n",
    "                model_dir = os.path.dirname(fixed_model_path)\n",
    "                model_basename = os.path.basename(fixed_model_path)\n",
    "                \n",
    "                logging.warning(f\"Model not found at: {fixed_model_path} or {fixed_model_path + '.pkl'}\")\n",
    "                logging.info(f\"Searching for model in directory: {model_dir}\")\n",
    "                \n",
    "                if os.path.exists(model_dir):\n",
    "                    files = os.listdir(model_dir)\n",
    "                    logging.info(f\"Files in directory: {files}\")\n",
    "                    \n",
    "                    # Try to find files that match the model basename\n",
    "                    potential_models = [f for f in files if f.startswith(model_basename)]\n",
    "                    if potential_models:\n",
    "                        potential_model = os.path.join(model_dir, potential_models[0])\n",
    "                        logging.info(f\"Found potential model: {potential_model}\")\n",
    "                        fixed_model_path = potential_model\n",
    "                        if fixed_model_path.endswith('.pkl'):\n",
    "                            fixed_model_path = fixed_model_path[:-4]\n",
    "                \n",
    "            # Load the model with enhanced error reporting\n",
    "            logging.info(f\"Loading model from {fixed_model_path}\")\n",
    "            try:\n",
    "                model = load_model(fixed_model_path)\n",
    "                logging.info(\"Model loaded successfully\")\n",
    "            except FileNotFoundError as e:\n",
    "                logging.error(f\"File not found error: {str(e)}\")\n",
    "                # Try to list available files in the directory\n",
    "                model_dir = os.path.dirname(fixed_model_path)\n",
    "                if os.path.exists(model_dir):\n",
    "                    files = os.listdir(model_dir)\n",
    "                    logging.error(f\"Files available in the directory: {files}\")\n",
    "                return {\n",
    "                    \"status\": \"error\",\n",
    "                    \"error_message\": f\"Failed to load model (file not found): {str(e)}\",\n",
    "                    \"model_path\": model_path,\n",
    "                    \"fixed_model_path\": fixed_model_path\n",
    "                }\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error loading model: {str(e)}\")\n",
    "                return {\n",
    "                    \"status\": \"error\",\n",
    "                    \"error_message\": f\"Failed to load model: {str(e)}\",\n",
    "                    \"model_path\": model_path,\n",
    "                    \"fixed_model_path\": fixed_model_path\n",
    "                }\n",
    "            \n",
    "            # Make predictions\n",
    "            logging.info(\"Making predictions\")\n",
    "            try:\n",
    "                # Try to get the raw scores (probabilities) for metrics\n",
    "                try:\n",
    "                    predictions = predict_model(model, data=data, raw_score=True)\n",
    "                    logging.info(\"Predictions with raw scores generated successfully\")\n",
    "                except Exception as e:\n",
    "                    logging.warning(f\"Error generating raw scores: {str(e)}. Falling back to standard predictions.\")\n",
    "                    predictions = predict_model(model, data=data)\n",
    "                    logging.info(\"Standard predictions generated successfully\")\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error making predictions: {str(e)}\")\n",
    "                return {\n",
    "                    \"status\": \"error\",\n",
    "                    \"error_message\": f\"Failed to make predictions: {str(e)}\"\n",
    "                }\n",
    "            \n",
    "            # Log the column names to help debug\n",
    "            logging.info(f\"Prediction columns: {predictions.columns.tolist()}\")\n",
    "            \n",
    "            # Save predictions\n",
    "            predictions_path = os.path.join(output_dir, prediction_filename)\n",
    "            try:\n",
    "                predictions.to_csv(predictions_path, index=False)\n",
    "                logging.info(f\"Predictions saved to {predictions_path}\")\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error saving predictions: {str(e)}\")\n",
    "                return {\n",
    "                    \"status\": \"error\",\n",
    "                    \"error_message\": f\"Failed to save predictions: {str(e)}\",\n",
    "                    \"predictions\": predictions  # Still return predictions in memory even if saving failed\n",
    "                }\n",
    "            \n",
    "            # Calculate metrics if ground truth is available\n",
    "            metrics = {}\n",
    "            metrics_df = None\n",
    "            comparison_path = None\n",
    "            confusion_matrix_path = None\n",
    "            \n",
    "            if has_ground_truth:\n",
    "                logging.info(f\"Calculating performance metrics using '{ground_truth_column}' as ground truth\")\n",
    "                try:\n",
    "                    # Get actual values using the stored ground truth\n",
    "                    y_true = original_ground_truth\n",
    "                    \n",
    "                    # Determine prediction column (PyCaret usually adds 'Label' column)\n",
    "                    pred_col_candidates = ['Label', 'prediction_label', 'predicted_y']\n",
    "                    pred_col = None\n",
    "                    \n",
    "                    for col in pred_col_candidates:\n",
    "                        if col in predictions.columns:\n",
    "                            pred_col = col\n",
    "                            break\n",
    "                    \n",
    "                    if pred_col is None:\n",
    "                        # Fall back to the last column if no standard name is found\n",
    "                        pred_col = predictions.columns[-1]\n",
    "                        logging.warning(f\"No standard prediction column found. Using '{pred_col}' as the prediction column.\")\n",
    "                    \n",
    "                    logging.info(f\"Using '{pred_col}' as prediction column for metrics\")\n",
    "                    \n",
    "                    y_pred = predictions[pred_col]\n",
    "                    \n",
    "                    # Calculate accuracy\n",
    "                    accuracy = accuracy_score(y_true, y_pred)\n",
    "                    metrics['accuracy'] = accuracy\n",
    "                    logging.info(f\"Accuracy: {accuracy:.4f}\")\n",
    "                    \n",
    "                    # Create a DataFrame for prediction vs actual comparison\n",
    "                    comparison_df = pd.DataFrame({\n",
    "                        'actual': y_true,\n",
    "                        'predicted': y_pred\n",
    "                    })\n",
    "                    \n",
    "                    # Get class labels to determine binary or multiclass\n",
    "                    unique_classes = np.unique(y_true)\n",
    "                    is_binary = len(unique_classes) == 2\n",
    "                    \n",
    "                    # Calculate confusion matrix and save it\n",
    "                    cm = confusion_matrix(y_true, y_pred)\n",
    "                    cm_df = pd.DataFrame(cm)\n",
    "                    \n",
    "                    # Add class labels if available\n",
    "                    if hasattr(model, 'classes_'):\n",
    "                        cm_df.index = model.classes_\n",
    "                        cm_df.columns = model.classes_\n",
    "                        cm_df.index.name = 'Actual'\n",
    "                        cm_df.columns.name = 'Predicted'\n",
    "                    else:\n",
    "                        cm_df.index = [f'Actual_{i}' for i in range(len(cm_df))]\n",
    "                        cm_df.columns = [f'Predicted_{i}' for i in range(len(cm_df.columns))]\n",
    "                    \n",
    "                    confusion_matrix_path = os.path.join(output_dir, \"confusion_matrix.csv\")\n",
    "                    cm_df.to_csv(confusion_matrix_path)\n",
    "                    logging.info(f\"Confusion matrix saved to {confusion_matrix_path}\")\n",
    "                    \n",
    "                    # For binary classification, calculate additional metrics\n",
    "                    if is_binary:\n",
    "                        logging.info(\"Binary classification detected, calculating additional metrics\")\n",
    "                        \n",
    "                        # Get unique classes and determine positive class\n",
    "                        classes = sorted(unique_classes)\n",
    "                        pos_class = classes[1]  # Usually the higher value is considered positive class\n",
    "                        logging.info(f\"Classes: {classes}, using {pos_class} as positive class\")\n",
    "                        \n",
    "                        # Find probability columns\n",
    "                        prob_cols = [col for col in predictions.columns if col.startswith('Score_')]\n",
    "                        \n",
    "                        # If no Score_ columns, look for other probability columns\n",
    "                        if not prob_cols:\n",
    "                            prob_cols = [col for col in predictions.columns \n",
    "                                        if 'probability' in col.lower() or 'score' in col.lower()]\n",
    "                        \n",
    "                        # Log available probability columns to help debugging\n",
    "                        if prob_cols:\n",
    "                            logging.info(f\"Available probability columns: {prob_cols}\")\n",
    "                            \n",
    "                            # Try to find column for positive class probability\n",
    "                            pos_class_cols = [col for col in prob_cols \n",
    "                                            if str(pos_class) in col or '1' in col or 'positive' in col.lower()]\n",
    "                            \n",
    "                            if pos_class_cols:\n",
    "                                # Prefer columns that seem to match positive class\n",
    "                                prob_col = pos_class_cols[0]\n",
    "                                logging.info(f\"Found column matching positive class: {prob_col}\")\n",
    "                            else:\n",
    "                                # Fallback to first probability column\n",
    "                                prob_col = prob_cols[0]\n",
    "                                logging.info(f\"No column matching positive class found, using first probability column: {prob_col}\")\n",
    "                                \n",
    "                            logging.info(f\"Using '{prob_col}' for AUC calculation (positive class: {pos_class})\")\n",
    "                            \n",
    "                            try:\n",
    "                                y_prob = predictions[prob_col]\n",
    "                                \n",
    "                                # Add probability to comparison DataFrame\n",
    "                                comparison_df['probability'] = y_prob\n",
    "                                \n",
    "                                # Calculate AUC\n",
    "                                auc = roc_auc_score(y_true, y_prob)\n",
    "                                metrics['auc'] = auc\n",
    "                                logging.info(f\"AUC: {auc:.4f}\")\n",
    "                            except Exception as e:\n",
    "                                logging.warning(f\"AUC calculation failed: {str(e)}\")\n",
    "                                # Try alternative approach with predict_proba if available\n",
    "                                try:\n",
    "                                    if hasattr(model, 'predict_proba'):\n",
    "                                        logging.info(\"Attempting AUC calculation using model.predict_proba\")\n",
    "                                        proba = model.predict_proba(data)\n",
    "                                        # Find the index of the positive class\n",
    "                                        if hasattr(model, 'classes_'):\n",
    "                                            pos_idx = np.where(model.classes_ == pos_class)[0][0]\n",
    "                                            y_prob = proba[:, pos_idx]\n",
    "                                        else:\n",
    "                                            # If classes_ not available, assume second column is positive class\n",
    "                                            y_prob = proba[:, 1]\n",
    "                                            \n",
    "                                        # Add probability to comparison DataFrame\n",
    "                                        comparison_df['probability'] = y_prob\n",
    "                                        \n",
    "                                        auc = roc_auc_score(y_true, y_prob)\n",
    "                                        metrics['auc'] = auc\n",
    "                                        logging.info(f\"AUC (using predict_proba): {auc:.4f}\")\n",
    "                                except Exception as inner_e:\n",
    "                                    logging.warning(f\"Alternative AUC calculation also failed: {str(inner_e)}\")\n",
    "                        else:\n",
    "                            logging.warning(\"No probability columns found for AUC calculation\")\n",
    "                        \n",
    "                        # Calculate other binary classification metrics\n",
    "                        try:\n",
    "                            # Extract confusion matrix values for binary case\n",
    "                            tn, fp, fn, tp = cm.ravel()\n",
    "                            \n",
    "                            # Calculate precision, recall, f1\n",
    "                            precision = precision_score(y_true, y_pred)\n",
    "                            recall = recall_score(y_true, y_pred)  # Same as sensitivity\n",
    "                            f1 = f1_score(y_true, y_pred)\n",
    "                            \n",
    "                            # Calculate specificity\n",
    "                            specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "                            \n",
    "                            # Store metrics\n",
    "                            metrics['precision'] = precision\n",
    "                            metrics['recall'] = recall\n",
    "                            metrics['sensitivity'] = recall  # Sensitivity is the same as recall\n",
    "                            metrics['specificity'] = specificity\n",
    "                            metrics['f1'] = f1\n",
    "                            \n",
    "                            # Log all metrics\n",
    "                            logging.info(f\"Precision: {precision:.4f}\")\n",
    "                            logging.info(f\"Recall/Sensitivity: {recall:.4f}\")\n",
    "                            logging.info(f\"Specificity: {specificity:.4f}\")\n",
    "                            logging.info(f\"F1 Score: {f1:.4f}\")\n",
    "                            \n",
    "                            # Add confusion matrix details for easier interpretation\n",
    "                            metrics['true_positives'] = int(tp)\n",
    "                            metrics['false_positives'] = int(fp)\n",
    "                            metrics['true_negatives'] = int(tn)\n",
    "                            metrics['false_negatives'] = int(fn)\n",
    "                            \n",
    "                        except Exception as e:\n",
    "                            logging.warning(f\"Could not calculate some binary metrics: {str(e)}\")\n",
    "                    else:\n",
    "                        # For multiclass, add appropriate metrics\n",
    "                        logging.info(\"Multiclass classification detected\")\n",
    "                        \n",
    "                        try:\n",
    "                            # Multiclass precision, recall, f1 with average\n",
    "                            precision_macro = precision_score(y_true, y_pred, average='macro')\n",
    "                            recall_macro = recall_score(y_true, y_pred, average='macro')\n",
    "                            f1_macro = f1_score(y_true, y_pred, average='macro')\n",
    "                            \n",
    "                            metrics['precision_macro'] = precision_macro\n",
    "                            metrics['recall_macro'] = recall_macro\n",
    "                            metrics['f1_macro'] = f1_macro\n",
    "                            \n",
    "                            logging.info(f\"Precision (macro): {precision_macro:.4f}\")\n",
    "                            logging.info(f\"Recall (macro): {recall_macro:.4f}\")\n",
    "                            logging.info(f\"F1 Score (macro): {f1_macro:.4f}\")\n",
    "                            \n",
    "                            # Try to calculate multiclass AUC if applicable\n",
    "                            try:\n",
    "                                # Check for probability columns\n",
    "                                prob_cols = [col for col in predictions.columns if col.startswith('Score_')]\n",
    "                                if len(prob_cols) > 1:\n",
    "                                    # Get probabilities for each class\n",
    "                                    class_probs = predictions[prob_cols].values\n",
    "                                    \n",
    "                                    # Calculate one-vs-rest AUC\n",
    "                                    from sklearn.preprocessing import label_binarize\n",
    "                                    from sklearn.metrics import roc_auc_score\n",
    "                                    \n",
    "                                    # Convert y_true to one-hot encoding\n",
    "                                    classes = sorted(unique_classes)\n",
    "                                    y_true_bin = label_binarize(y_true, classes=classes)\n",
    "                                    \n",
    "                                    # If only two classes, reshape to work with roc_auc_score\n",
    "                                    if len(classes) == 2:\n",
    "                                        y_true_bin = y_true_bin.reshape(-1, 1)\n",
    "                                    \n",
    "                                    # Calculate AUC for multiclass\n",
    "                                    if len(classes) > 2:\n",
    "                                        auc_multiclass = roc_auc_score(y_true_bin, class_probs, multi_class='ovr', average='macro')\n",
    "                                        metrics['auc_macro'] = auc_multiclass\n",
    "                                        logging.info(f\"AUC (macro): {auc_multiclass:.4f}\")\n",
    "                            except Exception as e:\n",
    "                                logging.warning(f\"Could not calculate multiclass AUC: {str(e)}\")\n",
    "                                \n",
    "                        except Exception as e:\n",
    "                            logging.warning(f\"Could not calculate some multiclass metrics: {str(e)}\")\n",
    "                    \n",
    "                    # Save the prediction vs actual comparison\n",
    "                    comparison_path = os.path.join(output_dir, \"prediction_vs_actual.csv\")\n",
    "                    comparison_df.to_csv(comparison_path, index=False)\n",
    "                    logging.info(f\"Prediction vs actual comparison saved to {comparison_path}\")\n",
    "                    \n",
    "                    # Create a DataFrame for the metrics report\n",
    "                    metrics_df = pd.DataFrame([metrics])\n",
    "                    \n",
    "                    # Save metrics to CSV file\n",
    "                    metrics_csv_path = os.path.join(output_dir, \"metrics_report.csv\")\n",
    "                    metrics_df.to_csv(metrics_csv_path, index=False)\n",
    "                    logging.info(f\"Metrics report saved to CSV: {metrics_csv_path}\")\n",
    "                    \n",
    "                    # Save metrics to JSON file (for compatibility)\n",
    "                    metrics_json_path = os.path.join(output_dir, \"metrics.json\")\n",
    "                    with open(metrics_json_path, 'w') as f:\n",
    "                        json.dump(metrics, f, indent=4)\n",
    "                    logging.info(f\"Performance metrics saved to JSON: {metrics_json_path}\")\n",
    "                    \n",
    "                    logging.info(f\"Metrics summary: {metrics}\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    logging.error(f\"Error calculating metrics: {str(e)}\", exc_info=True)\n",
    "                    # Still continue, as predictions were made\n",
    "            \n",
    "            return {\n",
    "                \"status\": \"success\",\n",
    "                \"predictions_path\": predictions_path,\n",
    "                \"log_file\": log_file,\n",
    "                \"metrics\": metrics if metrics else None,\n",
    "                \"metrics_df\": metrics_df.to_dict('records')[0] if metrics_df is not None else None,\n",
    "                \"metrics_csv_path\": os.path.join(output_dir, \"metrics_report.csv\") if metrics_df is not None else None,\n",
    "                \"metrics_json_path\": os.path.join(output_dir, \"metrics.json\") if metrics else None,\n",
    "                \"comparison_path\": comparison_path,\n",
    "                \"confusion_matrix_path\": confusion_matrix_path,\n",
    "                \"input_path\": input_path,\n",
    "                \"model_path\": model_path,\n",
    "                \"fixed_model_path\": fixed_model_path,\n",
    "                \"output_dir\": output_dir,\n",
    "                \"num_predictions\": len(predictions) if predictions is not None else 0,\n",
    "                \"has_ground_truth\": has_ground_truth\n",
    "            }\n",
    "        \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Unhandled error in PyCaret inference: {str(e)}\", exc_info=True)\n",
    "            return {\n",
    "                \"status\": \"error\",\n",
    "                \"error_message\": str(e),\n",
    "                \"input_path\": input_path,\n",
    "                \"model_path\": model_path\n",
    "            }\n",
    "\n",
    "\n",
    "pycaret_class_inference_tool = PyCaretInferenceTool()\n",
    "pycaret_class_training_tool = PyCaretClassificationTool()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regression Model Training and Inference Tool (for tabulated data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PyCaretRegressionInferenceTool(Tool):\n",
    "    name = \"pycaret_regression_inference\"\n",
    "    description = \"\"\"\n",
    "    This tool uses a saved PyCaret regression model to make predictions on new data.\n",
    "    It can also calculate performance metrics if ground truth data is available.\n",
    "    \"\"\"\n",
    "    \n",
    "    inputs = {\n",
    "        \"input_path\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"Path to input data file (CSV format) for inference\"\n",
    "        },\n",
    "        \"model_path\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"Path to the saved PyCaret model\"\n",
    "        },\n",
    "        \"output_dir\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"Directory where prediction results will be saved (if not specified, uses same directory as input file)\",\n",
    "            \"required\": False,\n",
    "            \"nullable\": True\n",
    "        },\n",
    "        \"ground_truth_column\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"Name of the column containing ground truth values (if available for metrics calculation)\",\n",
    "            \"required\": False,\n",
    "            \"nullable\": True\n",
    "        },\n",
    "        \"prediction_filename\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"Name for the output prediction file (default: predictions.csv)\",\n",
    "            \"required\": False,\n",
    "            \"nullable\": True\n",
    "        },\n",
    "        \"verbose\": {\n",
    "            \"type\": \"boolean\",\n",
    "            \"description\": \"Whether to print detailed logs\",\n",
    "            \"required\": False,\n",
    "            \"nullable\": True\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    output_type = \"object\"\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_path: str,\n",
    "        model_path: str,\n",
    "        output_dir: Optional[str] = None,\n",
    "        ground_truth_column: Optional[str] = None,\n",
    "        prediction_filename: Optional[str] = \"predictions.csv\",\n",
    "        verbose: Optional[bool] = True\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Run inference using a saved PyCaret regression model on new data.\n",
    "        \n",
    "        Args:\n",
    "            input_path: Path to input CSV file with data for inference\n",
    "            model_path: Path to the saved PyCaret model\n",
    "            output_dir: Directory to save prediction outputs (default: same directory as input file)\n",
    "            ground_truth_column: Name of the column containing ground truth values (if available)\n",
    "            prediction_filename: Name for the output prediction file\n",
    "            verbose: Whether to print detailed logs\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with inference results and file paths\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Set up logging\n",
    "            logging_format = '%(asctime)s - %(levelname)s - %(message)s'\n",
    "            logging_level = logging.INFO if verbose else logging.WARNING\n",
    "            \n",
    "            # Clear any existing handlers to avoid duplicates\n",
    "            logging.getLogger().handlers = []\n",
    "            \n",
    "            logging.basicConfig(level=logging_level, \n",
    "                               format=logging_format,\n",
    "                               handlers=[logging.StreamHandler()])\n",
    "            \n",
    "            # Set output directory (default to input file directory if not specified)\n",
    "            if output_dir is None:\n",
    "                output_dir = os.path.dirname(input_path)\n",
    "                if not output_dir:  # If input_path doesn't have a directory component\n",
    "                    output_dir = \".\"\n",
    "            \n",
    "            # Create output directory if it doesn't exist\n",
    "            os.makedirs(output_dir, exist_ok=True)\n",
    "            \n",
    "            # Set up log file in output directory\n",
    "            log_file = os.path.join(output_dir, \"pycaret_regression_inference.log\")\n",
    "            file_handler = logging.FileHandler(log_file)\n",
    "            file_handler.setFormatter(logging.Formatter(logging_format))\n",
    "            logging.getLogger().addHandler(file_handler)\n",
    "            \n",
    "            logging.info(f\"Starting inference using model: {model_path}\")\n",
    "            logging.info(f\"Input data: {input_path}\")\n",
    "            logging.info(f\"Output directory: {output_dir}\")\n",
    "            \n",
    "            # Import PyCaret's regression module\n",
    "            try:\n",
    "                from pycaret.regression import load_model, predict_model\n",
    "                logging.info(\"PyCaret imported successfully\")\n",
    "            except ImportError as e:\n",
    "                logging.error(f\"Error importing PyCaret: {str(e)}\")\n",
    "                logging.error(\"Please install PyCaret: pip install pycaret\")\n",
    "                raise ImportError(\"PyCaret is required for this tool. Please install it with: pip install pycaret\")\n",
    "            \n",
    "            # Load the data\n",
    "            logging.info(f\"Loading data from {input_path}\")\n",
    "            try:\n",
    "                data = pd.read_csv(input_path)\n",
    "                logging.info(f\"Data loaded successfully: {data.shape[0]} rows, {data.shape[1]} columns\")\n",
    "                logging.info(f\"Data columns: {data.columns.tolist()}\")\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error loading data: {str(e)}\")\n",
    "                return {\n",
    "                    \"status\": \"error\",\n",
    "                    \"error_message\": f\"Failed to load data: {str(e)}\",\n",
    "                    \"input_path\": input_path\n",
    "                }\n",
    "            \n",
    "            # Check if ground truth column exists if specified\n",
    "            has_ground_truth = False\n",
    "            if ground_truth_column:\n",
    "                if ground_truth_column in data.columns:\n",
    "                    has_ground_truth = True\n",
    "                    logging.info(f\"Ground truth column '{ground_truth_column}' found in dataset\")\n",
    "                    # Store original ground truth data before any modifications\n",
    "                    original_ground_truth = data[ground_truth_column].copy()\n",
    "                else:\n",
    "                    logging.warning(f\"Ground truth column '{ground_truth_column}' not found in the dataset. Metrics won't be calculated.\")\n",
    "            \n",
    "            # Fix model path - remove .pkl extension if present since PyCaret adds it automatically\n",
    "            fixed_model_path = model_path\n",
    "            if fixed_model_path.endswith('.pkl'):\n",
    "                fixed_model_path = fixed_model_path[:-4]\n",
    "                logging.info(f\"Removed .pkl extension from model path, using: {fixed_model_path}\")\n",
    "            \n",
    "            # Check if the model directory exists\n",
    "            model_dir = os.path.dirname(fixed_model_path)\n",
    "            if not os.path.exists(model_dir):\n",
    "                logging.error(f\"Model directory does not exist: {model_dir}\")\n",
    "                return {\n",
    "                    \"status\": \"error\",\n",
    "                    \"error_message\": f\"Model directory not found: {model_dir}\",\n",
    "                    \"model_path\": model_path\n",
    "                }\n",
    "            \n",
    "            # Check if model file or directory exists\n",
    "            if not os.path.exists(fixed_model_path) and not os.path.exists(fixed_model_path + '.pkl'):\n",
    "                # Try to find the model in the directory by listing all files\n",
    "                model_dir = os.path.dirname(fixed_model_path)\n",
    "                model_basename = os.path.basename(fixed_model_path)\n",
    "                \n",
    "                logging.warning(f\"Model not found at: {fixed_model_path} or {fixed_model_path + '.pkl'}\")\n",
    "                logging.info(f\"Searching for model in directory: {model_dir}\")\n",
    "                \n",
    "                if os.path.exists(model_dir):\n",
    "                    files = os.listdir(model_dir)\n",
    "                    logging.info(f\"Files in directory: {files}\")\n",
    "                    \n",
    "                    # Try to find files that match the model basename\n",
    "                    potential_models = [f for f in files if f.startswith(model_basename)]\n",
    "                    if potential_models:\n",
    "                        potential_model = os.path.join(model_dir, potential_models[0])\n",
    "                        logging.info(f\"Found potential model: {potential_model}\")\n",
    "                        fixed_model_path = potential_model\n",
    "                        if fixed_model_path.endswith('.pkl'):\n",
    "                            fixed_model_path = fixed_model_path[:-4]\n",
    "                \n",
    "            # Load the model with enhanced error reporting\n",
    "            logging.info(f\"Loading model from {fixed_model_path}\")\n",
    "            try:\n",
    "                model = load_model(fixed_model_path)\n",
    "                logging.info(\"Model loaded successfully\")\n",
    "            except FileNotFoundError as e:\n",
    "                logging.error(f\"File not found error: {str(e)}\")\n",
    "                # Try to list available files in the directory\n",
    "                model_dir = os.path.dirname(fixed_model_path)\n",
    "                if os.path.exists(model_dir):\n",
    "                    files = os.listdir(model_dir)\n",
    "                    logging.error(f\"Files available in the directory: {files}\")\n",
    "                return {\n",
    "                    \"status\": \"error\",\n",
    "                    \"error_message\": f\"Failed to load model (file not found): {str(e)}\",\n",
    "                    \"model_path\": model_path,\n",
    "                    \"fixed_model_path\": fixed_model_path\n",
    "                }\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error loading model: {str(e)}\")\n",
    "                return {\n",
    "                    \"status\": \"error\",\n",
    "                    \"error_message\": f\"Failed to load model: {str(e)}\",\n",
    "                    \"model_path\": model_path,\n",
    "                    \"fixed_model_path\": fixed_model_path\n",
    "                }\n",
    "            \n",
    "            # Make predictions\n",
    "            logging.info(\"Making predictions\")\n",
    "            try:\n",
    "                predictions = predict_model(model, data=data)\n",
    "                logging.info(\"Predictions generated successfully\")\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error making predictions: {str(e)}\")\n",
    "                return {\n",
    "                    \"status\": \"error\",\n",
    "                    \"error_message\": f\"Failed to make predictions: {str(e)}\"\n",
    "                }\n",
    "            \n",
    "            # Log the column names to help debug\n",
    "            logging.info(f\"Prediction columns: {predictions.columns.tolist()}\")\n",
    "            \n",
    "            # Save predictions\n",
    "            predictions_path = os.path.join(output_dir, prediction_filename)\n",
    "            try:\n",
    "                predictions.to_csv(predictions_path, index=False)\n",
    "                logging.info(f\"Predictions saved to {predictions_path}\")\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error saving predictions: {str(e)}\")\n",
    "                return {\n",
    "                    \"status\": \"error\",\n",
    "                    \"error_message\": f\"Failed to save predictions: {str(e)}\",\n",
    "                    \"predictions\": predictions  # Still return predictions in memory even if saving failed\n",
    "                }\n",
    "            \n",
    "            # Calculate metrics if ground truth is available\n",
    "            metrics = {}\n",
    "            metrics_df = None\n",
    "            comparison_path = None\n",
    "            residuals_path = None\n",
    "            \n",
    "            if has_ground_truth:\n",
    "                logging.info(f\"Calculating performance metrics using '{ground_truth_column}' as ground truth\")\n",
    "                try:\n",
    "                    # Get actual values using the stored ground truth\n",
    "                    y_true = original_ground_truth\n",
    "                    \n",
    "                    # Determine prediction column (PyCaret usually adds 'Label' or 'prediction_label' column)\n",
    "                    pred_col_candidates = ['Label', 'prediction_label', 'predicted_y', 'Prediction']\n",
    "                    pred_col = None\n",
    "                    \n",
    "                    for col in pred_col_candidates:\n",
    "                        if col in predictions.columns:\n",
    "                            pred_col = col\n",
    "                            break\n",
    "                    \n",
    "                    if pred_col is None:\n",
    "                        # Fall back to the last column if no standard name is found\n",
    "                        pred_col = predictions.columns[-1]\n",
    "                        logging.warning(f\"No standard prediction column found. Using '{pred_col}' as the prediction column.\")\n",
    "                    \n",
    "                    logging.info(f\"Using '{pred_col}' as prediction column for metrics\")\n",
    "                    \n",
    "                    y_pred = predictions[pred_col]\n",
    "                    \n",
    "                    # Create a DataFrame for prediction vs actual comparison\n",
    "                    comparison_df = pd.DataFrame({\n",
    "                        'actual': y_true,\n",
    "                        'predicted': y_pred\n",
    "                    })\n",
    "                    \n",
    "                    # Calculate residuals\n",
    "                    comparison_df['residual'] = y_true - y_pred\n",
    "                    comparison_df['abs_residual'] = np.abs(comparison_df['residual'])\n",
    "                    comparison_df['squared_residual'] = comparison_df['residual'] ** 2\n",
    "                    \n",
    "                    # Calculate percent error where actual != 0\n",
    "                    comparison_df['percent_error'] = np.nan\n",
    "                    nonzero_idx = y_true != 0\n",
    "                    if np.any(nonzero_idx):\n",
    "                        comparison_df.loc[nonzero_idx, 'percent_error'] = (\n",
    "                            np.abs(y_true[nonzero_idx] - y_pred[nonzero_idx]) / np.abs(y_true[nonzero_idx])\n",
    "                        ) * 100\n",
    "                    \n",
    "                    # Calculate regression metrics\n",
    "                    mse = mean_squared_error(y_true, y_pred)\n",
    "                    rmse = np.sqrt(mse)\n",
    "                    mae = mean_absolute_error(y_true, y_pred)\n",
    "                    r2 = r2_score(y_true, y_pred)\n",
    "                    explained_var = explained_variance_score(y_true, y_pred)\n",
    "                    \n",
    "                    # Try to calculate MAPE with handling for zero values\n",
    "                    try:\n",
    "                        # Calculate MAPE only on non-zero actual values\n",
    "                        mape = mean_absolute_percentage_error(y_true[nonzero_idx], y_pred[nonzero_idx]) * 100\n",
    "                    except Exception as e:\n",
    "                        logging.warning(f\"Could not calculate MAPE: {str(e)}\")\n",
    "                        mape = np.nan\n",
    "                    \n",
    "                    # Store metrics\n",
    "                    metrics['mse'] = mse\n",
    "                    metrics['rmse'] = rmse\n",
    "                    metrics['mae'] = mae\n",
    "                    metrics['r2'] = r2\n",
    "                    metrics['explained_variance'] = explained_var\n",
    "                    \n",
    "                    if not np.isnan(mape):\n",
    "                        metrics['mape'] = mape\n",
    "                    \n",
    "                    # Additional descriptive statistics on residuals\n",
    "                    metrics['mean_residual'] = np.mean(comparison_df['residual'])\n",
    "                    metrics['median_residual'] = np.median(comparison_df['residual'])\n",
    "                    metrics['min_residual'] = np.min(comparison_df['residual'])\n",
    "                    metrics['max_residual'] = np.max(comparison_df['residual'])\n",
    "                    metrics['std_residual'] = np.std(comparison_df['residual'])\n",
    "                    \n",
    "                    # Log all metrics\n",
    "                    logging.info(f\"MSE: {mse:.4f}\")\n",
    "                    logging.info(f\"RMSE: {rmse:.4f}\")\n",
    "                    logging.info(f\"MAE: {mae:.4f}\")\n",
    "                    logging.info(f\"R: {r2:.4f}\")\n",
    "                    logging.info(f\"Explained Variance: {explained_var:.4f}\")\n",
    "                    if not np.isnan(mape):\n",
    "                        logging.info(f\"MAPE: {mape:.4f}%\")\n",
    "                    \n",
    "                    # Create a residual analysis table\n",
    "                    residual_bins = pd.cut(comparison_df['residual'], bins=10)\n",
    "                    residual_analysis = comparison_df.groupby(residual_bins)['residual'].agg(['count', 'mean', 'min', 'max'])\n",
    "                    residual_analysis = residual_analysis.reset_index()\n",
    "                    residual_analysis['bin'] = residual_analysis['residual'].astype(str)\n",
    "                    residual_analysis = residual_analysis.drop('residual', axis=1)\n",
    "                    \n",
    "                    # Save residual analysis\n",
    "                    residuals_path = os.path.join(output_dir, \"residual_analysis.csv\")\n",
    "                    residual_analysis.to_csv(residuals_path, index=False)\n",
    "                    logging.info(f\"Residual analysis saved to {residuals_path}\")\n",
    "                    \n",
    "                    # Save the prediction vs actual comparison\n",
    "                    comparison_path = os.path.join(output_dir, \"prediction_vs_actual.csv\")\n",
    "                    comparison_df.to_csv(comparison_path, index=False)\n",
    "                    logging.info(f\"Prediction vs actual comparison saved to {comparison_path}\")\n",
    "                    \n",
    "                    # Create a DataFrame for the metrics report\n",
    "                    metrics_df = pd.DataFrame([metrics])\n",
    "                    \n",
    "                    # Save metrics to CSV file\n",
    "                    metrics_csv_path = os.path.join(output_dir, \"metrics_report.csv\")\n",
    "                    metrics_df.to_csv(metrics_csv_path, index=False)\n",
    "                    logging.info(f\"Metrics report saved to CSV: {metrics_csv_path}\")\n",
    "                    \n",
    "                    # Save metrics to JSON file (for compatibility)\n",
    "                    metrics_json_path = os.path.join(output_dir, \"metrics.json\")\n",
    "                    with open(metrics_json_path, 'w') as f:\n",
    "                        json.dump(metrics, f, indent=4)\n",
    "                    logging.info(f\"Performance metrics saved to JSON: {metrics_json_path}\")\n",
    "                    \n",
    "                    logging.info(f\"Metrics summary: {metrics}\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    logging.error(f\"Error calculating metrics: {str(e)}\", exc_info=True)\n",
    "                    # Still continue, as predictions were made\n",
    "            \n",
    "            return {\n",
    "                \"status\": \"success\",\n",
    "                \"predictions_path\": predictions_path,\n",
    "                \"log_file\": log_file,\n",
    "                \"metrics\": metrics if metrics else None,\n",
    "                \"metrics_df\": metrics_df.to_dict('records')[0] if metrics_df is not None else None,\n",
    "                \"metrics_csv_path\": os.path.join(output_dir, \"metrics_report.csv\") if metrics_df is not None else None,\n",
    "                \"metrics_json_path\": os.path.join(output_dir, \"metrics.json\") if metrics else None,\n",
    "                \"comparison_path\": comparison_path,\n",
    "                \"residuals_path\": residuals_path,\n",
    "                \"input_path\": input_path,\n",
    "                \"model_path\": model_path,\n",
    "                \"fixed_model_path\": fixed_model_path,\n",
    "                \"output_dir\": output_dir,\n",
    "                \"num_predictions\": len(predictions) if predictions is not None else 0,\n",
    "                \"has_ground_truth\": has_ground_truth\n",
    "            }\n",
    "        \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Unhandled error in PyCaret regression inference: {str(e)}\", exc_info=True)\n",
    "            return {\n",
    "                \"status\": \"error\",\n",
    "                \"error_message\": str(e),\n",
    "                \"input_path\": input_path,\n",
    "                \"model_path\": model_path\n",
    "            }\n",
    "\n",
    "\n",
    "class PyCaretRegressionTool(Tool):\n",
    "    name = \"pycaret_regression\"\n",
    "    description = \"\"\"\n",
    "    This tool uses PyCaret to train and evaluate regression models.\n",
    "    It compares multiple models, tunes the best ones, creates a blended model,\n",
    "    and generates various visualizations and interpretations.\n",
    "    Results are saved to the specified output directory.\n",
    "    \"\"\"\n",
    "    \n",
    "    inputs = {\n",
    "        \"input_path\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"Path to input data file (CSV format)\"\n",
    "        },\n",
    "        \"output_dir\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"Output directory where model and results will be saved\"\n",
    "        },\n",
    "        \"target_column\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"Name of the target column for regression\"\n",
    "        },\n",
    "        \"experiment_name\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"Name of the experiment\",\n",
    "            \"required\": False,\n",
    "            \"nullable\": True\n",
    "        },\n",
    "        \"fold\": {\n",
    "            \"type\": \"integer\",\n",
    "            \"description\": \"Number of cross-validation folds\",\n",
    "            \"required\": False,\n",
    "            \"nullable\": True\n",
    "        },\n",
    "        \"session_id\": {\n",
    "            \"type\": \"integer\",\n",
    "            \"description\": \"Random seed for reproducibility\",\n",
    "            \"required\": False,\n",
    "            \"nullable\": True\n",
    "        },\n",
    "        \"use_gpu\": {\n",
    "            \"type\": \"boolean\",\n",
    "            \"description\": \"Whether to use GPU for training (if available)\",\n",
    "            \"required\": False,\n",
    "            \"nullable\": True\n",
    "        },\n",
    "        \"data_split_stratify\": {\n",
    "            \"type\": \"boolean\",\n",
    "            \"description\": \"Whether to use stratified sampling for data splitting\",\n",
    "            \"required\": False,\n",
    "            \"nullable\": True\n",
    "        },\n",
    "        \"data_split_shuffle\": {\n",
    "            \"type\": \"boolean\",\n",
    "            \"description\": \"Whether to shuffle data before splitting\",\n",
    "            \"required\": False,\n",
    "            \"nullable\": True\n",
    "        },\n",
    "        \"preprocess\": {\n",
    "            \"type\": \"boolean\",\n",
    "            \"description\": \"Whether to apply preprocessing steps\",\n",
    "            \"required\": False,\n",
    "            \"nullable\": True\n",
    "        },\n",
    "        \"ignore_features\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"Comma-separated list of features to ignore during training\",\n",
    "            \"required\": False,\n",
    "            \"nullable\": True\n",
    "        },\n",
    "        \"numeric_features\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"Comma-separated list of numeric features\",\n",
    "            \"required\": False,\n",
    "            \"nullable\": True\n",
    "        },\n",
    "        \"categorical_features\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"Comma-separated list of categorical features\",\n",
    "            \"required\": False,\n",
    "            \"nullable\": True\n",
    "        },\n",
    "        \"date_features\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"Comma-separated list of date features\",\n",
    "            \"required\": False,\n",
    "            \"nullable\": True\n",
    "        },\n",
    "        \"n_select\": {\n",
    "            \"type\": \"integer\",\n",
    "            \"description\": \"Number of top models to select for blending\",\n",
    "            \"required\": False,\n",
    "            \"nullable\": True\n",
    "        },\n",
    "        \"normalize\": {\n",
    "            \"type\": \"boolean\",\n",
    "            \"description\": \"Whether to normalize numeric features\",\n",
    "            \"required\": False,\n",
    "            \"nullable\": True\n",
    "        },\n",
    "        \"transformation\": {\n",
    "            \"type\": \"boolean\",\n",
    "            \"description\": \"Whether to apply transformation to numeric features\",\n",
    "            \"required\": False,\n",
    "            \"nullable\": True\n",
    "        },\n",
    "        \"pca\": {\n",
    "            \"type\": \"boolean\",\n",
    "            \"description\": \"Whether to apply PCA for dimensionality reduction\",\n",
    "            \"required\": False,\n",
    "            \"nullable\": True\n",
    "        },\n",
    "        \"pca_components\": {\n",
    "            \"type\": \"number\",\n",
    "            \"description\": \"Number of PCA components (float between 0-1 or int > 1)\",\n",
    "            \"required\": False,\n",
    "            \"nullable\": True\n",
    "        },\n",
    "        \"include_models\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"Comma-separated list of models to include in comparison\",\n",
    "            \"required\": False,\n",
    "            \"nullable\": True\n",
    "        },\n",
    "        \"exclude_models\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"Comma-separated list of models to exclude from comparison\",\n",
    "            \"required\": False,\n",
    "            \"nullable\": True\n",
    "        },\n",
    "        \"test_data_path\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"Path to test/holdout data for independent evaluation\",\n",
    "            \"required\": False,\n",
    "            \"nullable\": True\n",
    "        },\n",
    "        \"ignore_gpu_errors\": {\n",
    "            \"type\": \"boolean\",\n",
    "            \"description\": \"Whether to ignore GPU-related errors and fall back to CPU\",\n",
    "            \"required\": False,\n",
    "            \"nullable\": True\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    output_type = \"object\"\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_path: str,\n",
    "        output_dir: str,\n",
    "        target_column: str,\n",
    "        experiment_name: Optional[str] = None,\n",
    "        fold: Optional[int] = 10,\n",
    "        session_id: Optional[int] = None,\n",
    "        use_gpu: Optional[bool] = False,\n",
    "        data_split_stratify: Optional[bool] = False,  # Usually False for regression\n",
    "        data_split_shuffle: Optional[bool] = True,\n",
    "        preprocess: Optional[bool] = True,\n",
    "        ignore_features: Optional[str] = None,\n",
    "        numeric_features: Optional[str] = None,\n",
    "        categorical_features: Optional[str] = None,\n",
    "        date_features: Optional[str] = None,\n",
    "        n_select: Optional[int] = 3,\n",
    "        normalize: Optional[bool] = True,  # Usually True for regression\n",
    "        transformation: Optional[bool] = True,  # Usually True for regression\n",
    "        pca: Optional[bool] = False,\n",
    "        pca_components: Optional[float] = None,\n",
    "        include_models: Optional[str] = None,\n",
    "        exclude_models: Optional[str] = None,\n",
    "        test_data_path: Optional[str] = None,\n",
    "        ignore_gpu_errors: Optional[bool] = True\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Train and evaluate regression models using PyCaret.\n",
    "        \n",
    "        Args:\n",
    "            input_path: Path to input CSV file with training data\n",
    "            output_dir: Directory to save outputs\n",
    "            target_column: Target column name for regression\n",
    "            experiment_name: Name of the experiment\n",
    "            fold: Number of cross-validation folds\n",
    "            session_id: Random seed for reproducibility\n",
    "            use_gpu: Whether to use GPU for training\n",
    "            data_split_stratify: Whether to use stratified sampling\n",
    "            data_split_shuffle: Whether to shuffle data before splitting\n",
    "            preprocess: Whether to apply preprocessing steps\n",
    "            ignore_features: Comma-separated list of features to ignore\n",
    "            numeric_features: Comma-separated list of numeric features\n",
    "            categorical_features: Comma-separated list of categorical features\n",
    "            date_features: Comma-separated list of date features\n",
    "            n_select: Number of top models to select for blending\n",
    "            normalize: Whether to normalize numeric features\n",
    "            transformation: Whether to apply transformation to numeric features\n",
    "            pca: Whether to apply PCA for dimensionality reduction\n",
    "            pca_components: Number of PCA components\n",
    "            include_models: Comma-separated list of models to include\n",
    "            exclude_models: Comma-separated list of models to exclude\n",
    "            test_data_path: Path to test/holdout data for independent evaluation\n",
    "            ignore_gpu_errors: Whether to ignore GPU errors and fallback to CPU\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with model training results and file paths\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Setup logging\n",
    "            os.makedirs(output_dir, exist_ok=True)\n",
    "            log_file = os.path.join(output_dir, \"pycaret_regression.log\")\n",
    "            \n",
    "            logging_format = '%(asctime)s - %(levelname)s - %(message)s'\n",
    "            logging.basicConfig(level=logging.INFO, \n",
    "                               format=logging_format,\n",
    "                               handlers=[\n",
    "                                   logging.FileHandler(log_file),\n",
    "                                   logging.StreamHandler()\n",
    "                               ])\n",
    "            \n",
    "            # Generate experiment name if not provided\n",
    "            if experiment_name is None:\n",
    "                timestamp = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "                experiment_name = f\"regression_exp_{timestamp}\"\n",
    "            \n",
    "            # Generate session_id if not provided\n",
    "            if session_id is None:\n",
    "                import random\n",
    "                session_id = random.randint(1, 10000)\n",
    "            \n",
    "            logging.info(f\"Starting regression experiment: {experiment_name}\")\n",
    "            logging.info(f\"Input data: {input_path}\")\n",
    "            logging.info(f\"Output directory: {output_dir}\")\n",
    "            logging.info(f\"Target column: {target_column}\")\n",
    "            \n",
    "            # Import PyCaret's regression module\n",
    "            try:\n",
    "                from pycaret.regression import (\n",
    "                    setup, compare_models, tune_model, blend_models, \n",
    "                    pull, predict_model, save_model, load_model,\n",
    "                    plot_model, interpret_model\n",
    "                )\n",
    "                \n",
    "                # Check PyCaret version for compatibility adjustments\n",
    "                import pycaret\n",
    "                pycaret_version = getattr(pycaret, \"__version__\", \"unknown\")\n",
    "                logging.info(f\"PyCaret version: {pycaret_version}\")\n",
    "                \n",
    "                logging.info(\"PyCaret imported successfully\")\n",
    "            except ImportError as e:\n",
    "                logging.error(f\"Error importing PyCaret: {str(e)}\")\n",
    "                logging.error(\"Please install PyCaret: pip install pycaret\")\n",
    "                raise ImportError(\"PyCaret is required for this tool. Please install it with: pip install pycaret\")\n",
    "            \n",
    "            # Load data\n",
    "            logging.info(f\"Loading data from {input_path}\")\n",
    "            data = pd.read_csv(input_path)\n",
    "            logging.info(f\"Data loaded successfully: {data.shape[0]} rows, {data.shape[1]} columns\")\n",
    "            \n",
    "            # Verify target column exists\n",
    "            if target_column not in data.columns:\n",
    "                raise ValueError(f\"Target column '{target_column}' not found in the dataset\")\n",
    "            \n",
    "            # Prepare parameters for setup\n",
    "            setup_params = {\n",
    "                'data': data,\n",
    "                'target': target_column,\n",
    "                'session_id': session_id,\n",
    "                'experiment_name': experiment_name,\n",
    "                'fold': fold,\n",
    "                'use_gpu': use_gpu,\n",
    "                'preprocess': preprocess,\n",
    "                'data_split_shuffle': data_split_shuffle,\n",
    "                'log_experiment': False  # Set to False to avoid MLflow errors\n",
    "            }\n",
    "            \n",
    "            # Add stratify only if it's supported in regression context\n",
    "            if data_split_stratify is not None:\n",
    "                setup_params['data_split_stratify'] = data_split_stratify\n",
    "            \n",
    "            # Add optional parameters if provided\n",
    "            if ignore_features:\n",
    "                setup_params['ignore_features'] = [f.strip() for f in ignore_features.split(',')]\n",
    "            \n",
    "            if numeric_features:\n",
    "                setup_params['numeric_features'] = [f.strip() for f in numeric_features.split(',')]\n",
    "            \n",
    "            if categorical_features:\n",
    "                setup_params['categorical_features'] = [f.strip() for f in categorical_features.split(',')]\n",
    "            \n",
    "            if date_features:\n",
    "                setup_params['date_features'] = [f.strip() for f in date_features.split(',')]\n",
    "            \n",
    "            if normalize is not None:\n",
    "                setup_params['normalize'] = normalize\n",
    "                \n",
    "            if transformation is not None:\n",
    "                setup_params['transformation'] = transformation\n",
    "                \n",
    "            if pca is not None:\n",
    "                setup_params['pca'] = pca\n",
    "                \n",
    "            if pca_components is not None:\n",
    "                setup_params['pca_components'] = pca_components\n",
    "            \n",
    "            # Check GPU availability and cuml installation if use_gpu is requested\n",
    "            if use_gpu:\n",
    "                try:\n",
    "                    import cuml\n",
    "                    logging.info(\"RAPIDS cuML is available for GPU acceleration\")\n",
    "                except ImportError:\n",
    "                    logging.warning(\"'cuml' is not installed but use_gpu=True was specified.\")\n",
    "                    if ignore_gpu_errors:\n",
    "                        logging.warning(\"Running on CPU instead. To use GPU, install cuml: pip install cuml\")\n",
    "                        use_gpu = False\n",
    "                        setup_params['use_gpu'] = False\n",
    "                    else:\n",
    "                        raise ImportError(\"GPU acceleration requested but cuml is not installed. Install with: pip install cuml\")\n",
    "            \n",
    "            # Set up the experiment with error handling for parameter compatibility\n",
    "            logging.info(\"Setting up PyCaret experiment\")\n",
    "            logging.info(f\"Setup parameters: {setup_params}\")\n",
    "            \n",
    "            # Try to create setup with default parameters first\n",
    "            try:\n",
    "                s = setup(**setup_params)\n",
    "                logging.info(\"PyCaret setup completed successfully\")\n",
    "            except TypeError as e:\n",
    "                logging.warning(f\"Setup error: {str(e)}\")\n",
    "                if \"unexpected keyword argument\" in str(e):\n",
    "                    # Handle incompatible parameters by removing them and retrying\n",
    "                    error_param = str(e).split(\"argument \")[-1].split(\"'\")[1] if \"'\" in str(e) else str(e).split(\"argument \")[-1].strip()\n",
    "                    logging.warning(f\"Incompatible parameter detected: {error_param}\")\n",
    "                    logging.warning(f\"Removing parameter and retrying setup\")\n",
    "                    \n",
    "                    if error_param in setup_params:\n",
    "                        del setup_params[error_param]\n",
    "                        try:\n",
    "                            s = setup(**setup_params)\n",
    "                            logging.info(\"PyCaret setup completed successfully after parameter adjustment\")\n",
    "                        except Exception as inner_e:\n",
    "                            logging.error(f\"Setup failed even after removing parameter: {str(inner_e)}\")\n",
    "                            # Try a minimal setup as last resort\n",
    "                            minimal_params = {\n",
    "                                'data': data,\n",
    "                                'target': target_column,\n",
    "                                'session_id': session_id\n",
    "                            }\n",
    "                            logging.warning(f\"Attempting minimal setup with just: {minimal_params.keys()}\")\n",
    "                            s = setup(**minimal_params)\n",
    "                            logging.info(\"PyCaret setup completed with minimal parameters\")\n",
    "                    else:\n",
    "                        raise\n",
    "                else:\n",
    "                    raise\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Unexpected setup error: {str(e)}\")\n",
    "                # Try a minimal setup as last resort\n",
    "                minimal_params = {\n",
    "                    'data': data,\n",
    "                    'target': target_column,\n",
    "                    'session_id': session_id\n",
    "                }\n",
    "                logging.warning(f\"Attempting minimal setup with just: {minimal_params.keys()}\")\n",
    "                s = setup(**minimal_params)\n",
    "                logging.info(\"PyCaret setup completed with minimal parameters\")\n",
    "            \n",
    "            # Prepare parameters for compare_models\n",
    "            compare_params = {'n_select': n_select}\n",
    "            \n",
    "            if include_models:\n",
    "                compare_params['include'] = [m.strip() for m in include_models.split(',')]\n",
    "                \n",
    "            if exclude_models:\n",
    "                compare_params['exclude'] = [m.strip() for m in exclude_models.split(',')]\n",
    "            \n",
    "            # Compare baseline models with compatibility handling\n",
    "            logging.info(f\"Comparing models (selecting top {n_select} models)\")\n",
    "            try:\n",
    "                best = compare_models(**compare_params)\n",
    "            except TypeError as e:\n",
    "                if \"unexpected keyword argument\" in str(e):\n",
    "                    # Handle incompatible parameters\n",
    "                    error_param = str(e).split(\"argument \")[-1].split(\"'\")[1] if \"'\" in str(e) else str(e).split(\"argument \")[-1].strip()\n",
    "                    logging.warning(f\"Incompatible parameter detected in compare_models: {error_param}\")\n",
    "                    logging.warning(f\"Removing parameter and retrying\")\n",
    "                    \n",
    "                    if error_param in compare_params:\n",
    "                        del compare_params[error_param]\n",
    "                        best = compare_models(**compare_params)\n",
    "                    else:\n",
    "                        raise\n",
    "                else:\n",
    "                    raise\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error in compare_models: {str(e)}\")\n",
    "                # Try with minimal parameters\n",
    "                logging.warning(\"Attempting compare_models with minimal parameters\")\n",
    "                best = compare_models()\n",
    "            \n",
    "            # If only one model is returned, wrap it in a list\n",
    "            if not isinstance(best, list):\n",
    "                best = [best]\n",
    "                logging.info(\"Only one model was selected\")\n",
    "            \n",
    "            logging.info(f\"Top {len(best)} models selected\")\n",
    "            \n",
    "            # Initialize list to track created files\n",
    "            created_files = []\n",
    "            \n",
    "            # Compare models results\n",
    "            compare_results = pull()\n",
    "            compare_results_path = os.path.join(output_dir, 'model_comparison_results.csv')\n",
    "            compare_results.to_csv(compare_results_path, index=True)\n",
    "            created_files.append(('Model Comparison Results', compare_results_path))\n",
    "            logging.info(f\"Saved model comparison results to {compare_results_path}\")\n",
    "            \n",
    "            # Create a function to safely save plots\n",
    "            def save_plot_safely(plot_obj, file_path, plot_type, created_files):\n",
    "                try:\n",
    "                    # Save the figure with explicit filepath\n",
    "                    plot_obj.savefig(file_path, bbox_inches='tight', dpi=300)\n",
    "                    created_files.append((f'Plot: {plot_type}', file_path))\n",
    "                    logging.info(f\"Created {plot_type} plot at {file_path}\")\n",
    "                    return True\n",
    "                except Exception as e:\n",
    "                    logging.warning(f\"Could not save {plot_type} plot to {file_path}: {str(e)}\")\n",
    "                    return False\n",
    "            \n",
    "            # Define a function to create plots for a model\n",
    "            def create_plots_for_model(model, plots_dir, created_files):\n",
    "                # Store the original working directory\n",
    "                original_dir = os.getcwd()\n",
    "                \n",
    "                try:\n",
    "                    # Change to the plots directory before creating plots\n",
    "                    os.chdir(plots_dir)\n",
    "                    logging.info(f\"Changed working directory to {plots_dir}\")\n",
    "                    \n",
    "                    # Regression-specific plot types\n",
    "                    plot_types = [\n",
    "                        'residuals', 'error', 'cooks', 'learning',\n",
    "                        'vc', 'feature', 'feature_all', 'parameter']\n",
    "                    \n",
    "                    \n",
    "                    model_plots = []\n",
    "                    \n",
    "                    # Try to create each plot type\n",
    "                    for plot_type in plot_types:\n",
    "                        try:\n",
    "                            logging.info(f\"Attempting to create {plot_type} plot in {plots_dir}\")\n",
    "                            \n",
    "                            # Try to create the plot with save=True\n",
    "                            plot_model(model, plot=plot_type, save=True)\n",
    "                            \n",
    "                            # Check if file was created\n",
    "                            expected_filename = f\"{plot_type}.png\"\n",
    "                            if os.path.exists(expected_filename):\n",
    "                                full_path = os.path.join(plots_dir, expected_filename)\n",
    "                                model_plots.append((f'Plot: {plot_type}', full_path))\n",
    "                                logging.info(f\"Successfully created {plot_type} plot at {full_path}\")\n",
    "                            else:\n",
    "                                logging.warning(f\"Plot {plot_type} was not found after creation\")\n",
    "                            \n",
    "                        except Exception as e:\n",
    "                            logging.warning(f\"Failed to create {plot_type} plot: {str(e)}\")\n",
    "                    \n",
    "                    return model_plots\n",
    "                \n",
    "                finally:\n",
    "                    # Always restore the original working directory\n",
    "                    os.chdir(original_dir)\n",
    "                    logging.info(f\"Restored working directory to {original_dir}\")\n",
    "            \n",
    "            # Function to generate model interpretations\n",
    "            def create_interpretations_for_model(model, plots_dir, created_files):\n",
    "                # Store the original working directory\n",
    "                original_dir = os.getcwd()\n",
    "                \n",
    "                try:\n",
    "                    # Change to the plots directory before creating interpretations\n",
    "                    os.chdir(plots_dir)\n",
    "                    logging.info(f\"Changed working directory to {plots_dir}\")\n",
    "                    \n",
    "                    model_interpretations = []\n",
    "                    \n",
    "                    # Try to create summary interpretation\n",
    "                    try:\n",
    "                        logging.info(\"Attempting to create model interpretation summary\")\n",
    "                        interpret_model(model, plot='summary', save=True)\n",
    "                        \n",
    "                        # Check if file was created\n",
    "                        expected_filename = \"SHAP Summary.png\"\n",
    "                        if os.path.exists(expected_filename):\n",
    "                            full_path = os.path.join(plots_dir, expected_filename)\n",
    "                            model_interpretations.append(('Interpretation: summary', full_path))\n",
    "                            logging.info(f\"Successfully created interpretation summary at {full_path}\")\n",
    "                        else:\n",
    "                            logging.warning(\"Interpretation summary file was not found after creation\")\n",
    "                            \n",
    "                    except Exception as e:\n",
    "                        logging.warning(f\"Failed to create summary interpretation: {str(e)}\")\n",
    "                    \n",
    "                    # Try to create correlation interpretation\n",
    "                    try:\n",
    "                        logging.info(\"Attempting to create model interpretation correlation\")\n",
    "                        interpret_model(model, plot='correlation', save=True)\n",
    "                        \n",
    "                        # Check if file was created\n",
    "                        expected_filename = \"SHAP Correlation.png\"\n",
    "                        if os.path.exists(expected_filename):\n",
    "                            full_path = os.path.join(plots_dir, expected_filename)\n",
    "                            model_interpretations.append(('Interpretation: correlation', full_path))\n",
    "                            logging.info(f\"Successfully created interpretation correlation at {full_path}\")\n",
    "                        else:\n",
    "                            logging.warning(\"Interpretation correlation file was not found after creation\")\n",
    "                            \n",
    "                    except Exception as e:\n",
    "                        logging.warning(f\"Failed to create correlation interpretation: {str(e)}\")\n",
    "                    \n",
    "                    return model_interpretations\n",
    "                \n",
    "                finally:\n",
    "                    # Always restore the original working directory\n",
    "                    os.chdir(original_dir)\n",
    "                    logging.info(f\"Restored working directory to {original_dir}\")\n",
    "            \n",
    "            # Function to evaluate model on test data\n",
    "            def evaluate_model_on_test_data(model, output_path, model_name):\n",
    "                try:\n",
    "                    logging.info(f\"Evaluating {model_name} on test data using PyCaret's inherent workflow\")\n",
    "                    \n",
    "                    # Use PyCaret's inherent workflow for holdout predictions\n",
    "                    # This uses the test split that was created during setup()\n",
    "                    holdout_pred = predict_model(model)\n",
    "                    \n",
    "                    # Save the predictions\n",
    "                    holdout_pred.to_csv(output_path, index=False)\n",
    "                    logging.info(f\"Saved {model_name} test predictions to {output_path}\")\n",
    "                    return output_path\n",
    "                except Exception as e:\n",
    "                    logging.error(f\"Error evaluating {model_name} on test data: {str(e)}\")\n",
    "                    return None\n",
    "            \n",
    "            # Create a top-level models directory\n",
    "            models_dir = os.path.join(output_dir, \"models\")\n",
    "            os.makedirs(models_dir, exist_ok=True)\n",
    "            \n",
    "            # Dictionary to track all individual model paths\n",
    "            individual_model_paths = {}\n",
    "            model_test_predictions = {}\n",
    "            \n",
    "            # Tune and save each model individually\n",
    "            tuned_models = []\n",
    "            for i, model in enumerate(best):\n",
    "                model_index = i + 1\n",
    "                model_name = f\"tuned_model_{model_index}\"\n",
    "                logging.info(f\"Tuning model {model_index}/{len(best)}\")\n",
    "                \n",
    "                try:\n",
    "                    # Create a dedicated directory for this model\n",
    "                    model_dir = os.path.join(models_dir, model_name)\n",
    "                    os.makedirs(model_dir, exist_ok=True)\n",
    "                    \n",
    "                    # Tune the model\n",
    "                    tuned_model = tune_model(model)\n",
    "                    tuned_models.append(tuned_model)\n",
    "                    \n",
    "                    # Save tuned model results\n",
    "                    tuned_results = pull()\n",
    "                    tuned_results_path = os.path.join(model_dir, f'{model_name}_results.csv')\n",
    "                    tuned_results.to_csv(tuned_results_path, index=True)\n",
    "                    created_files.append((f'{model_name} Results', tuned_results_path))\n",
    "                    logging.info(f\"Saved {model_name} results to {tuned_results_path}\")\n",
    "                    \n",
    "                    # Save the model itself\n",
    "                    model_path = os.path.join(model_dir, f'{model_name}')\n",
    "                    try:\n",
    "                        save_model(tuned_model, model_path)\n",
    "                        created_files.append((f'{model_name} Saved Model', model_path))\n",
    "                        individual_model_paths[model_name] = model_path\n",
    "                        logging.info(f\"Saved {model_name} to {model_path}\")\n",
    "                    except Exception as e:\n",
    "                        logging.error(f\"Error saving {model_name}: {str(e)}\")\n",
    "                    \n",
    "                    # Create plots directory for this model\n",
    "                    plots_dir = os.path.join(model_dir, \"plots\")\n",
    "                    os.makedirs(plots_dir, exist_ok=True)\n",
    "                    \n",
    "                    # Generate plots for this model\n",
    "                    logging.info(f\"Generating plots for {model_name}\")\n",
    "                    model_plots = create_plots_for_model(tuned_model, plots_dir, created_files)\n",
    "                    created_files.extend(model_plots)\n",
    "                    \n",
    "                    # Generate interpretations for this model\n",
    "                    logging.info(f\"Generating interpretations for {model_name}\")\n",
    "                    model_interpretations = create_interpretations_for_model(tuned_model, plots_dir, created_files)\n",
    "                    created_files.extend(model_interpretations)\n",
    "                    \n",
    "                    # Evaluate on holdout set using PyCaret's inherent workflow\n",
    "                    test_pred_path = os.path.join(model_dir, f'independent_eval_results_{model_name}.csv')\n",
    "                    test_result = evaluate_model_on_test_data(tuned_model, test_pred_path, model_name)\n",
    "                    if test_result:\n",
    "                        created_files.append((f'{model_name} Test Predictions', test_result))\n",
    "                        model_test_predictions[model_name] = test_result\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    logging.error(f\"Error processing {model_name}: {str(e)}\")\n",
    "            \n",
    "            # Create a blended model if we have multiple tuned models\n",
    "            blended_model = None\n",
    "            blended_model_path = None\n",
    "            blended_test_pred_path = None\n",
    "            \n",
    "            if len(tuned_models) > 1:\n",
    "                logging.info(\"Creating blended model from tuned models\")\n",
    "                try:\n",
    "                    # Create dedicated directory for blended model\n",
    "                    blended_dir = os.path.join(models_dir, \"blended_model\")\n",
    "                    os.makedirs(blended_dir, exist_ok=True)\n",
    "                    \n",
    "                    # Create the blended model\n",
    "                    blended_model = blend_models(tuned_models)\n",
    "                    \n",
    "                    # Save blended model results\n",
    "                    blended_results = pull()\n",
    "                    blended_results_path = os.path.join(blended_dir, 'blended_model_results.csv')\n",
    "                    blended_results.to_csv(blended_results_path, index=True)\n",
    "                    created_files.append(('Blended Model Results', blended_results_path))\n",
    "                    logging.info(f\"Saved blended model results to {blended_results_path}\")\n",
    "                    \n",
    "                    # Save the blended model\n",
    "                    blended_model_path = os.path.join(blended_dir, 'blended_model')\n",
    "                    try:\n",
    "                        save_model(blended_model, blended_model_path)\n",
    "                        created_files.append(('Blended Model Saved Model', blended_model_path))\n",
    "                        individual_model_paths[\"blended_model\"] = blended_model_path\n",
    "                        logging.info(f\"Saved blended model to {blended_model_path}\")\n",
    "                    except Exception as e:\n",
    "                        logging.error(f\"Error saving blended model: {str(e)}\")\n",
    "                    \n",
    "                    # Create plots directory for blended model\n",
    "                    plots_dir = os.path.join(blended_dir, \"plots\")\n",
    "                    os.makedirs(plots_dir, exist_ok=True)\n",
    "                    \n",
    "                    # Generate plots for blended model\n",
    "                    logging.info(\"Generating plots for blended model\")\n",
    "                    blended_plots = create_plots_for_model(blended_model, plots_dir, created_files)\n",
    "                    created_files.extend(blended_plots)\n",
    "                    \n",
    "                    # Generate interpretations for blended model\n",
    "                    logging.info(\"Generating interpretations for blended model\")\n",
    "                    blended_interpretations = create_interpretations_for_model(blended_model, plots_dir, created_files)\n",
    "                    created_files.extend(blended_interpretations)\n",
    "                    \n",
    "                    # Evaluate on holdout set using PyCaret's inherent workflow\n",
    "                    blended_test_pred_path = os.path.join(blended_dir, 'independent_eval_results_blended_model.csv')\n",
    "                    test_result = evaluate_model_on_test_data(blended_model, blended_test_pred_path, \"blended_model\")\n",
    "                    if test_result:\n",
    "                        created_files.append(('Blended Model Test Predictions', test_result))\n",
    "                        model_test_predictions[\"blended_model\"] = test_result\n",
    "                    \n",
    "                    final_model = blended_model\n",
    "                except Exception as e:\n",
    "                    logging.error(f\"Error creating blended model: {str(e)}\")\n",
    "                    logging.info(\"Using the best tuned model as the final model\")\n",
    "                    final_model = tuned_models[0]\n",
    "            else:\n",
    "                logging.info(\"Only one model available, using it as the final model\")\n",
    "                final_model = tuned_models[0] if tuned_models else best[0]\n",
    "            \n",
    "            # Save the final model to the main output directory as well\n",
    "            final_model_path = os.path.join(output_dir, f'{experiment_name}_final_model')\n",
    "            try:\n",
    "                save_model(final_model, final_model_path)\n",
    "                created_files.append(('Final Model', final_model_path))\n",
    "                logging.info(f\"Saved final model to {final_model_path}\")\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error saving final model: {str(e)}\")\n",
    "            \n",
    "            # Create summary report with links to all models and evaluations\n",
    "            try:\n",
    "                summary_path = os.path.join(output_dir, \"model_summary.csv\")\n",
    "                summary_data = []\n",
    "                \n",
    "                # Add individual models to summary\n",
    "                for i, model in enumerate(tuned_models):\n",
    "                    model_name = f\"tuned_model_{i+1}\"\n",
    "                    model_path = individual_model_paths.get(model_name, \"Not saved\")\n",
    "                    test_pred_path = model_test_predictions.get(model_name, \"Not available\")\n",
    "                    \n",
    "                    summary_data.append({\n",
    "                        \"Model\": model_name,\n",
    "                        \"Type\": \"Tuned Individual Model\",\n",
    "                        \"Model Path\": model_path,\n",
    "                        \"Test Predictions\": test_pred_path\n",
    "                    })\n",
    "                \n",
    "                # Add blended model to summary if it exists\n",
    "                if blended_model is not None:\n",
    "                    model_path = individual_model_paths.get(\"blended_model\", \"Not saved\")\n",
    "                    test_pred_path = model_test_predictions.get(\"blended_model\", \"Not available\")\n",
    "                    \n",
    "                    summary_data.append({\n",
    "                        \"Model\": \"blended_model\",\n",
    "                        \"Type\": \"Blended Model\",\n",
    "                        \"Model Path\": model_path,\n",
    "                        \"Test Predictions\": test_pred_path\n",
    "                    })\n",
    "                \n",
    "                # Write summary to CSV\n",
    "                summary_df = pd.DataFrame(summary_data)\n",
    "                summary_df.to_csv(summary_path, index=False)\n",
    "                created_files.append(('Model Summary', summary_path))\n",
    "                logging.info(f\"Created model summary at {summary_path}\")\n",
    "            except Exception as e:\n",
    "                    logging.error(f\"Error creating model summary: {str(e)}\")\n",
    "\n",
    "            logging.info(\"Regression modeling completed successfully\")\n",
    "\n",
    "            return {\n",
    "                \"status\": \"success\",\n",
    "                \"experiment_name\": experiment_name,\n",
    "                \"input_path\": input_path,\n",
    "                \"output_dir\": output_dir,\n",
    "                \"final_model_path\": final_model_path,\n",
    "                \"individual_model_paths\": individual_model_paths,\n",
    "                \"model_test_predictions\": model_test_predictions,\n",
    "                \"model_summary\": summary_path if 'summary_path' in locals() else None,\n",
    "                \"log_file\": log_file,\n",
    "                \"created_files\": created_files\n",
    "            }\n",
    "\n",
    "        except Exception as e:\n",
    "                logging.error(f\"Error in PyCaret regression: {str(e)}\", exc_info=True)\n",
    "                return {\n",
    "                    \"status\": \"error\",\n",
    "                    \"error_message\": str(e),\n",
    "                    \"input_path\": input_path,\n",
    "                    \"output_dir\": output_dir\n",
    "                }\n",
    "\n",
    "pycaret_regression_training_tool = PyCaretRegressionTool()\n",
    "pycaret_regression_inference_tool = PyCaretRegressionInferenceTool()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image Classification Model Training and Inference Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MedicalImageDataset(Dataset):\n",
    "    \"\"\"Dataset class for medical images that works with both ResNet and Inception V3 models.\"\"\"\n",
    "    \n",
    "    def __init__(self, image_dir, labels_file=None, transform=None, is_test=False, image_size=None):\n",
    "        \"\"\"\n",
    "        Initialize the dataset.\n",
    "        \n",
    "        Args:\n",
    "            image_dir (str): Directory containing the images.\n",
    "            labels_file (str, optional): Path to CSV file with image names and labels.\n",
    "            transform (callable, optional): Transform to apply to the images.\n",
    "            is_test (bool, optional): Whether this is a test dataset without labels.\n",
    "            image_size (tuple, optional): Size for fallback images when loading fails.\n",
    "                                         Default: (224, 224) for ResNet, (299, 299) for Inception V3.\n",
    "        \"\"\"\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "        self.is_test = is_test\n",
    "        \n",
    "        # Default image sizes based on architecture\n",
    "        if image_size is None:\n",
    "            # Default to ResNet size as it's more common\n",
    "            self.image_size = (224, 224)\n",
    "        else:\n",
    "            self.image_size = image_size\n",
    "        \n",
    "        if not is_test and labels_file is not None:\n",
    "            self.labels_df = pd.read_csv(labels_file)\n",
    "            \n",
    "            # Handle both naming conventions (filename and image_name)\n",
    "            if 'filename' in self.labels_df.columns:\n",
    "                self.filename_col = 'filename'\n",
    "            elif 'image_name' in self.labels_df.columns:\n",
    "                self.filename_col = 'image_name'\n",
    "            else:\n",
    "                raise ValueError(\"CSV file must contain either 'filename' or 'image_name' column\")\n",
    "            \n",
    "            # Ensure filenames don't have directory paths\n",
    "            self.labels_df[self.filename_col] = self.labels_df[self.filename_col].apply(\n",
    "                lambda x: os.path.basename(x) if isinstance(x, str) else x\n",
    "            )\n",
    "        elif is_test:\n",
    "            # For test set without labels, just list all images in the directory\n",
    "            self.image_files = [f for f in os.listdir(image_dir) \n",
    "                            if os.path.isfile(os.path.join(image_dir, f)) and \n",
    "                            f.lower().endswith(('png', 'jpg', 'jpeg', 'bmp', 'tif', 'tiff'))]\n",
    "    \n",
    "    def __len__(self):\n",
    "        if self.is_test:\n",
    "            return len(self.image_files)\n",
    "        return len(self.labels_df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "            \n",
    "        if self.is_test:\n",
    "            img_name = self.image_files[idx]\n",
    "            img_path = os.path.join(self.image_dir, img_name)\n",
    "            try:\n",
    "                image = Image.open(img_path).convert('RGB')\n",
    "            except Exception as e:\n",
    "                logging.warning(f\"Error opening image {img_path}: {e}\")\n",
    "                image = Image.new('RGB', self.image_size, color='black')\n",
    "                \n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "                \n",
    "            return image, img_name  # Return filename for prediction output\n",
    "        else:\n",
    "            img_name = self.labels_df.iloc[idx][self.filename_col]\n",
    "            img_path = os.path.join(self.image_dir, img_name)\n",
    "            \n",
    "            try:\n",
    "                image = Image.open(img_path).convert('RGB')\n",
    "            except Exception as e:\n",
    "                # If image can't be opened, create a black image as placeholder\n",
    "                logging.warning(f\"Error opening image {img_path}: {e}\")\n",
    "                image = Image.new('RGB', self.image_size, color='black')\n",
    "                \n",
    "            label = self.labels_df.iloc[idx]['label'] \n",
    "            \n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "                \n",
    "            return image, label\n",
    "               \n",
    "class PyTorchResNetTrainingTool(Tool):\n",
    "    name = \"pytorch_resnet_training\"\n",
    "    description = \"\"\"\n",
    "    This tool trains a ResNet model using PyTorch for medical image classification.\n",
    "    It can train from scratch or fine-tune a pre-trained model, and includes validation metrics.\n",
    "    \"\"\"\n",
    "    \n",
    "    inputs = {\n",
    "        \"data_dir\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"Directory containing dataset with training and validation folders\"\n",
    "        },\n",
    "        \"output_dir\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"Directory where the trained model and results will be saved\"\n",
    "        },\n",
    "        \"num_classes\": {\n",
    "            \"type\": \"integer\",\n",
    "            \"description\": \"Number of classes for classification\"\n",
    "        },\n",
    "        \"model_type\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"ResNet model type: resnet18, resnet34, resnet50, resnet101, or resnet152\",\n",
    "            \"required\": False,\n",
    "            \"nullable\": True\n",
    "        },\n",
    "        \"num_epochs\": {\n",
    "            \"type\": \"integer\",\n",
    "            \"description\": \"Number of training epochs\",\n",
    "            \"required\": False,\n",
    "            \"nullable\": True\n",
    "        },\n",
    "        \"batch_size\": {\n",
    "            \"type\": \"integer\",\n",
    "            \"description\": \"Batch size for training\",\n",
    "            \"required\": False,\n",
    "            \"nullable\": True\n",
    "        },\n",
    "        \"pretrained\": {\n",
    "            \"type\": \"boolean\",\n",
    "            \"description\": \"Whether to use pretrained weights\",\n",
    "            \"required\": False,\n",
    "            \"nullable\": True\n",
    "        },\n",
    "        \"early_stopping\": {\n",
    "            \"type\": \"boolean\", \n",
    "            \"description\": \"Whether to use early stopping\",\n",
    "            \"required\": False,\n",
    "            \"nullable\": True\n",
    "        },\n",
    "        \"patience\": {\n",
    "            \"type\": \"integer\",\n",
    "            \"description\": \"Number of epochs to wait before early stopping\",\n",
    "            \"required\": False,\n",
    "            \"nullable\": True\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    output_type = \"object\"\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        data_dir: str,\n",
    "        output_dir: str,\n",
    "        num_classes: int,\n",
    "        model_type: Optional[str] = \"resnet50\",\n",
    "        num_epochs: Optional[int] = 10,\n",
    "        batch_size: Optional[int] = 16,\n",
    "        pretrained: Optional[bool] = True,\n",
    "        early_stopping: Optional[bool] = True,\n",
    "        patience: Optional[int] = 5\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Train a ResNet model for medical image classification.\n",
    "        \n",
    "        Args:\n",
    "            data_dir: Directory containing dataset with train and val subdirectories\n",
    "            output_dir: Directory to save model and results\n",
    "            num_classes: Number of classes for classification\n",
    "            model_type: ResNet variant (resnet18, resnet34, resnet50, resnet101, resnet152)\n",
    "            num_epochs: Number of training epochs\n",
    "            batch_size: Batch size for training\n",
    "            pretrained: Whether to use pretrained weights\n",
    "            early_stopping: Whether to use early stopping\n",
    "            patience: Number of epochs to wait before early stopping\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with training results and model paths\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Set up logging\n",
    "            os.makedirs(output_dir, exist_ok=True)\n",
    "            log_file = os.path.join(output_dir, \"training.log\")\n",
    "            logging.basicConfig(\n",
    "                level=logging.INFO,\n",
    "                format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "                handlers=[\n",
    "                    logging.FileHandler(log_file),\n",
    "                    logging.StreamHandler()\n",
    "                ]\n",
    "            )\n",
    "            \n",
    "            # Log configuration\n",
    "            logging.info(f\"Starting ResNet training with configuration:\")\n",
    "            logging.info(f\"Model type: {model_type}\")\n",
    "            logging.info(f\"Number of classes: {num_classes}\")\n",
    "            logging.info(f\"Pretrained: {pretrained}\")\n",
    "            logging.info(f\"Early stopping: {early_stopping}\")\n",
    "            if early_stopping:\n",
    "                logging.info(f\"Patience: {patience}\")\n",
    "            logging.info(f\"Data directory: {data_dir}\")\n",
    "            \n",
    "            # Check if CUDA is available\n",
    "            device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "            logging.info(f\"Using device: {device}\")\n",
    "            \n",
    "            # Define transformations\n",
    "            train_transform = transforms.Compose([\n",
    "                transforms.Resize((224, 224)),\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.RandomRotation(10),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "            ])\n",
    "            \n",
    "            val_transform = transforms.Compose([\n",
    "                transforms.Resize((224, 224)),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "            ])\n",
    "            \n",
    "            # Setup directories\n",
    "            train_dir = os.path.join(data_dir, 'train')\n",
    "            val_dir = os.path.join(data_dir, 'val')\n",
    "            test_dir = os.path.join(data_dir, 'test')\n",
    "            \n",
    "            if not os.path.exists(train_dir):\n",
    "                raise ValueError(f\"Training directory not found: {train_dir}\")\n",
    "            \n",
    "            # Check if validation directory exists, otherwise use a split from training\n",
    "            use_train_val_split = not os.path.exists(val_dir)\n",
    "            \n",
    "            # Check if data directory has a labels.csv file or if images are in class subdirectories\n",
    "            train_labels_file = os.path.join(data_dir, 'labels.csv')\n",
    "            use_csv_labels = os.path.exists(train_labels_file)\n",
    "            \n",
    "            # Create datasets based on the data organization\n",
    "            if use_csv_labels:\n",
    "                # Using CSV file with image paths and labels\n",
    "                logging.info(f\"Using labels from CSV file: {train_labels_file}\")\n",
    "                train_dataset = MedicalImageDataset(\n",
    "                    image_dir=train_dir,\n",
    "                    labels_file=train_labels_file,\n",
    "                    transform=train_transform\n",
    "                )\n",
    "                \n",
    "                if not use_train_val_split:\n",
    "                    val_labels_file = os.path.join(data_dir, 'val_labels.csv')\n",
    "                    if not os.path.exists(val_labels_file):\n",
    "                        val_labels_file = train_labels_file  # Fallback to same labels file\n",
    "                    \n",
    "                    val_dataset = MedicalImageDataset(\n",
    "                        image_dir=val_dir,\n",
    "                        labels_file=val_labels_file,\n",
    "                        transform=val_transform\n",
    "                    )\n",
    "            else:\n",
    "                # Using directory structure (each class in its own subdirectory)\n",
    "                logging.info(\"Using directory structure for class labels\")\n",
    "                train_dataset = datasets.ImageFolder(\n",
    "                    root=train_dir,\n",
    "                    transform=train_transform\n",
    "                )\n",
    "                \n",
    "                if not use_train_val_split:\n",
    "                    val_dataset = datasets.ImageFolder(\n",
    "                        root=val_dir,\n",
    "                        transform=val_transform\n",
    "                    )\n",
    "            \n",
    "            # Split training data if no validation directory\n",
    "            if use_train_val_split:\n",
    "                train_size = int(0.8 * len(train_dataset))\n",
    "                val_size = len(train_dataset) - train_size\n",
    "                train_dataset, val_dataset = torch.utils.data.random_split(\n",
    "                    train_dataset, [train_size, val_size]\n",
    "                )\n",
    "            \n",
    "            logging.info(f\"Training dataset size: {len(train_dataset)}\")\n",
    "            logging.info(f\"Validation dataset size: {len(val_dataset)}\")\n",
    "            \n",
    "            # Create data loaders\n",
    "            train_loader = DataLoader(\n",
    "                train_dataset, \n",
    "                batch_size=batch_size, \n",
    "                shuffle=True, \n",
    "                num_workers=4\n",
    "            )\n",
    "            \n",
    "            val_loader = DataLoader(\n",
    "                val_dataset,\n",
    "                batch_size=batch_size,\n",
    "                shuffle=False,\n",
    "                num_workers=4\n",
    "            )\n",
    "            \n",
    "            # Create model\n",
    "            logging.info(f\"Creating {model_type} model...\")\n",
    "            \n",
    "            # Dictionary of available ResNet models and their weights\n",
    "            resnet_models = {\n",
    "                'resnet18': (models.resnet18, models.ResNet18_Weights.IMAGENET1K_V1),\n",
    "                'resnet34': (models.resnet34, models.ResNet34_Weights.IMAGENET1K_V1),\n",
    "                'resnet50': (models.resnet50, models.ResNet50_Weights.IMAGENET1K_V1),\n",
    "                'resnet101': (models.resnet101, models.ResNet101_Weights.IMAGENET1K_V1),\n",
    "                'resnet152': (models.resnet152, models.ResNet152_Weights.IMAGENET1K_V1)\n",
    "            }\n",
    "            \n",
    "            if model_type not in resnet_models:\n",
    "                logging.warning(f\"Invalid model type: {model_type}. Using resnet50 instead.\")\n",
    "                model_type = 'resnet50'\n",
    "            \n",
    "            # Get model function and weights    \n",
    "            model_fn, weights = resnet_models[model_type]\n",
    "            \n",
    "            # Create model with or without pretrained weights\n",
    "            if pretrained:\n",
    "                model = model_fn(weights=weights)\n",
    "                logging.info(f\"Using pretrained weights from ImageNet\")\n",
    "            else:\n",
    "                model = model_fn(weights=None)\n",
    "                logging.info(f\"Initializing model with random weights\")\n",
    "            \n",
    "            # Modify the last layer for our number of classes\n",
    "            in_features = model.fc.in_features\n",
    "            model.fc = nn.Linear(in_features, num_classes)\n",
    "            \n",
    "            # Move model to device\n",
    "            model = model.to(device)\n",
    "            \n",
    "            # Loss function and optimizer\n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "            optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "            \n",
    "            # Learning rate scheduler (removed verbose parameter)\n",
    "            scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "                optimizer, 'min', patience=2, factor=0.5\n",
    "            )\n",
    "            \n",
    "            # Track best model\n",
    "            best_val_loss = float('inf')\n",
    "            best_val_acc = 0.0\n",
    "            patience_counter = 0\n",
    "            \n",
    "            # Training history\n",
    "            history = {\n",
    "                'train_loss': [],\n",
    "                'val_loss': [],\n",
    "                'train_acc': [],\n",
    "                'val_acc': [],\n",
    "                'learning_rates': []\n",
    "            }\n",
    "            \n",
    "            # Training loop\n",
    "            logging.info(\"Starting training...\")\n",
    "            start_time = time.time()\n",
    "            \n",
    "            for epoch in range(num_epochs):\n",
    "                # Training phase\n",
    "                model.train()\n",
    "                train_loss = 0.0\n",
    "                train_correct = 0\n",
    "                train_total = 0\n",
    "                \n",
    "                for inputs, labels in train_loader:\n",
    "                    inputs, labels = inputs.to(device), labels.to(device)\n",
    "                    \n",
    "                    # Zero the gradients\n",
    "                    optimizer.zero_grad()\n",
    "                    \n",
    "                    # Forward pass\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    \n",
    "                    # Backward pass and optimize\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    \n",
    "                    # Statistics\n",
    "                    train_loss += loss.item() * inputs.size(0)\n",
    "                    _, predicted = torch.max(outputs, 1)\n",
    "                    train_total += labels.size(0)\n",
    "                    train_correct += (predicted == labels).sum().item()\n",
    "                \n",
    "                epoch_train_loss = train_loss / train_total\n",
    "                epoch_train_acc = 100 * train_correct / train_total\n",
    "                \n",
    "                # Validation phase\n",
    "                model.eval()\n",
    "                val_loss = 0.0\n",
    "                val_correct = 0\n",
    "                val_total = 0\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    for inputs, labels in val_loader:\n",
    "                        inputs, labels = inputs.to(device), labels.to(device)\n",
    "                        \n",
    "                        outputs = model(inputs)\n",
    "                        loss = criterion(outputs, labels)\n",
    "                        \n",
    "                        val_loss += loss.item() * inputs.size(0)\n",
    "                        _, predicted = torch.max(outputs, 1)\n",
    "                        val_total += labels.size(0)\n",
    "                        val_correct += (predicted == labels).sum().item()\n",
    "                \n",
    "                epoch_val_loss = val_loss / val_total\n",
    "                epoch_val_acc = 100 * val_correct / val_total\n",
    "                \n",
    "                # Update learning rate\n",
    "                scheduler.step(epoch_val_loss)\n",
    "                \n",
    "                # Get current learning rate\n",
    "                current_lr = optimizer.param_groups[0]['lr']\n",
    "                \n",
    "                # Save history\n",
    "                history['train_loss'].append(epoch_train_loss)\n",
    "                history['val_loss'].append(epoch_val_loss)\n",
    "                history['train_acc'].append(epoch_train_acc)\n",
    "                history['val_acc'].append(epoch_val_acc)\n",
    "                history['learning_rates'].append(current_lr)\n",
    "                \n",
    "                # Log progress\n",
    "                logging.info(f\"Epoch {epoch+1}/{num_epochs} - \"\n",
    "                           f\"Train Loss: {epoch_train_loss:.4f}, \"\n",
    "                           f\"Train Acc: {epoch_train_acc:.2f}%, \"\n",
    "                           f\"Val Loss: {epoch_val_loss:.4f}, \"\n",
    "                           f\"Val Acc: {epoch_val_acc:.2f}%, \"\n",
    "                           f\"LR: {current_lr:.6f}\")\n",
    "                \n",
    "                # Check for improvement\n",
    "                if epoch_val_acc > best_val_acc:\n",
    "                    best_val_acc = epoch_val_acc\n",
    "                    best_val_loss = epoch_val_loss\n",
    "                    patience_counter = 0\n",
    "                    \n",
    "                    # Save best model\n",
    "                    best_model_path = os.path.join(output_dir, \"best_model.pt\")\n",
    "                    torch.save({\n",
    "                        'epoch': epoch,\n",
    "                        'model_state_dict': model.state_dict(),\n",
    "                        'optimizer_state_dict': optimizer.state_dict(),\n",
    "                        'loss': best_val_loss,\n",
    "                        'accuracy': best_val_acc\n",
    "                    }, best_model_path)\n",
    "                    logging.info(f\"Saved best model with accuracy: {best_val_acc:.2f}%\")\n",
    "                else:\n",
    "                    patience_counter += 1\n",
    "                \n",
    "                # Early stopping if enabled\n",
    "                if early_stopping and patience_counter >= patience:\n",
    "                    logging.info(f\"Early stopping triggered after {epoch+1} epochs\")\n",
    "                    break\n",
    "            \n",
    "            # Calculate training time\n",
    "            total_time = time.time() - start_time\n",
    "            logging.info(f\"Training completed in {total_time:.2f} seconds\")\n",
    "            \n",
    "            # Save final model\n",
    "            final_model_path = os.path.join(output_dir, \"final_model.pt\")\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': epoch_val_loss,\n",
    "                'accuracy': epoch_val_acc\n",
    "            }, final_model_path)\n",
    "            \n",
    "            # If test directory exists, evaluate on test set\n",
    "            if os.path.exists(test_dir):\n",
    "                logging.info(\"Evaluating model on test set...\")\n",
    "                # Create test dataset and dataloader\n",
    "                if use_csv_labels:\n",
    "                    test_labels_file = os.path.join(data_dir, 'test_labels.csv')\n",
    "                    if not os.path.exists(test_labels_file):\n",
    "                        test_labels_file = train_labels_file  # Fallback to same labels file\n",
    "                    \n",
    "                    test_dataset = MedicalImageDataset(\n",
    "                        image_dir=test_dir,\n",
    "                        labels_file=test_labels_file,\n",
    "                        transform=val_transform  # Use the same transform as validation\n",
    "                    )\n",
    "                else:\n",
    "                    test_dataset = datasets.ImageFolder(\n",
    "                        root=test_dir,\n",
    "                        transform=val_transform\n",
    "                    )\n",
    "                \n",
    "                test_loader = DataLoader(\n",
    "                    test_dataset,\n",
    "                    batch_size=batch_size,\n",
    "                    shuffle=False,\n",
    "                    num_workers=4\n",
    "                )\n",
    "                \n",
    "                # Evaluate on test set\n",
    "                model.eval()\n",
    "                test_loss = 0.0\n",
    "                test_correct = 0\n",
    "                test_total = 0\n",
    "                all_preds = []\n",
    "                all_labels = []\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    for inputs, labels in test_loader:\n",
    "                        inputs, labels = inputs.to(device), labels.to(device)\n",
    "                        \n",
    "                        outputs = model(inputs)\n",
    "                        loss = criterion(outputs, labels)\n",
    "                        \n",
    "                        test_loss += loss.item() * inputs.size(0)\n",
    "                        _, predicted = torch.max(outputs, 1)\n",
    "                        test_total += labels.size(0)\n",
    "                        test_correct += (predicted == labels).sum().item()\n",
    "                        \n",
    "                        # Store predictions and labels for metrics\n",
    "                        all_preds.extend(predicted.cpu().numpy())\n",
    "                        all_labels.extend(labels.cpu().numpy())\n",
    "                \n",
    "                test_acc = 100 * test_correct / test_total\n",
    "                logging.info(f\"Test accuracy: {test_acc:.2f}%\")\n",
    "                \n",
    "                # Calculate and log detailed metrics\n",
    "                metrics = {}\n",
    "                metrics['accuracy'] = accuracy_score(all_labels, all_preds)\n",
    "                metrics['precision_macro'] = precision_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "                metrics['recall_macro'] = recall_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "                metrics['f1_macro'] = f1_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "                \n",
    "                # Add per-class metrics if there are multiple classes\n",
    "                if num_classes > 1:\n",
    "                    metrics['precision_per_class'] = precision_score(all_labels, all_preds, average=None, zero_division=0).tolist()\n",
    "                    metrics['recall_per_class'] = recall_score(all_labels, all_preds, average=None, zero_division=0).tolist()\n",
    "                    metrics['f1_per_class'] = f1_score(all_labels, all_preds, average=None, zero_division=0).tolist()\n",
    "                \n",
    "                # Save metrics to JSON\n",
    "                metrics_path = os.path.join(output_dir, \"test_metrics.json\")\n",
    "                with open(metrics_path, 'w') as f:\n",
    "                    json.dump(metrics, f, indent=4)\n",
    "                \n",
    "                # Generate confusion matrix\n",
    "                cm = confusion_matrix(all_labels, all_preds)\n",
    "                plt.figure(figsize=(10, 8))\n",
    "                sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "                plt.xlabel('Predicted')\n",
    "                plt.ylabel('True')\n",
    "                plt.title('Confusion Matrix')\n",
    "                cm_path = os.path.join(output_dir, \"confusion_matrix.png\")\n",
    "                plt.savefig(cm_path)\n",
    "                plt.close()\n",
    "            \n",
    "            # Save training history\n",
    "            history_path = os.path.join(output_dir, \"training_history.json\")\n",
    "            with open(history_path, 'w') as f:\n",
    "                json.dump(history, f, indent=4)\n",
    "            \n",
    "            # Create plots\n",
    "            plt.figure(figsize=(15, 5))\n",
    "            \n",
    "            plt.subplot(1, 3, 1)\n",
    "            plt.plot(history['train_loss'], label='Training Loss')\n",
    "            plt.plot(history['val_loss'], label='Validation Loss')\n",
    "            plt.xlabel('Epoch')\n",
    "            plt.ylabel('Loss')\n",
    "            plt.legend()\n",
    "            plt.title('Loss Curves')\n",
    "            \n",
    "            plt.subplot(1, 3, 2)\n",
    "            plt.plot(history['train_acc'], label='Training Accuracy')\n",
    "            plt.plot(history['val_acc'], label='Validation Accuracy')\n",
    "            plt.xlabel('Epoch')\n",
    "            plt.ylabel('Accuracy (%)')\n",
    "            plt.legend()\n",
    "            plt.title('Accuracy Curves')\n",
    "            \n",
    "            plt.subplot(1, 3, 3)\n",
    "            plt.plot(history['learning_rates'])\n",
    "            plt.xlabel('Epoch')\n",
    "            plt.ylabel('Learning Rate')\n",
    "            plt.title('Learning Rate Schedule')\n",
    "            plt.yscale('log')\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plots_path = os.path.join(output_dir, \"training_plots.png\")\n",
    "            plt.savefig(plots_path)\n",
    "            \n",
    "            # Save model configuration\n",
    "            config = {\n",
    "                'model_type': model_type,\n",
    "                'num_classes': num_classes,\n",
    "                'image_size': 224,\n",
    "                'pretrained': pretrained,\n",
    "                'early_stopping': early_stopping,\n",
    "                'patience': patience if early_stopping else None,\n",
    "                'best_epoch': epoch,\n",
    "                'best_accuracy': best_val_acc,\n",
    "                'training_epochs': epoch + 1,\n",
    "                'early_stopped': early_stopping and patience_counter >= patience,\n",
    "                'batch_size': batch_size\n",
    "            }\n",
    "            \n",
    "            config_path = os.path.join(output_dir, \"model_config.json\")\n",
    "            with open(config_path, 'w') as f:\n",
    "                json.dump(config, f, indent=4)\n",
    "            \n",
    "            # Return results\n",
    "            return {\n",
    "                \"status\": \"success\",\n",
    "                \"best_model_path\": best_model_path,\n",
    "                \"final_model_path\": final_model_path,\n",
    "                \"config_path\": config_path,\n",
    "                \"plots_path\": plots_path,\n",
    "                \"history_path\": history_path,\n",
    "                \"test_metrics_path\": metrics_path if 'metrics_path' in locals() else None,\n",
    "                \"confusion_matrix_path\": cm_path if 'cm_path' in locals() else None,\n",
    "                \"best_accuracy\": best_val_acc,\n",
    "                \"test_accuracy\": test_acc if 'test_acc' in locals() else None,\n",
    "                \"training_time_seconds\": total_time,\n",
    "                \"epochs_completed\": epoch + 1,\n",
    "                \"early_stopped\": early_stopping and patience_counter >= patience,\n",
    "                \"early_stopping_used\": early_stopping\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error during training: {str(e)}\", exc_info=True)\n",
    "            return {\n",
    "                \"status\": \"error\",\n",
    "                \"error_message\": str(e),\n",
    "                \"data_dir\": data_dir,\n",
    "                \"output_dir\": output_dir\n",
    "            }\n",
    "\n",
    "class PyTorchResNetInferenceTool(Tool):\n",
    "    name = \"pytorch_resnet_inference\"\n",
    "    description = \"\"\"\n",
    "    This tool uses a trained PyTorch ResNet model to perform inference on new images.\n",
    "    It can also calculate performance metrics if ground truth labels are provided.\n",
    "    \"\"\"\n",
    "    \n",
    "    inputs = {\n",
    "        \"image_dir\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"Directory containing images for inference\"\n",
    "        },\n",
    "        \"model_path\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"Path to the trained model file (.pt format)\"\n",
    "        },\n",
    "        \"output_dir\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"Directory where prediction results will be saved\"\n",
    "        },\n",
    "        \"config_path\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"Path to model configuration JSON file (optional)\",\n",
    "            \"required\": False,\n",
    "            \"nullable\": True\n",
    "        },\n",
    "        \"ground_truth_file\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"CSV file with image filenames and ground truth labels for evaluation (optional)\",\n",
    "            \"required\": False,\n",
    "            \"nullable\": True\n",
    "        },\n",
    "        \"class_names\": {\n",
    "            \"type\": \"array\",\n",
    "            \"description\": \"List of class names corresponding to model output indices (optional)\",\n",
    "            \"required\": False,\n",
    "            \"nullable\": True\n",
    "        },\n",
    "        \"num_classes\": {\n",
    "            \"type\": \"integer\",\n",
    "            \"description\": \"Number of classes (needed if not provided in config file)\",\n",
    "            \"required\": False,\n",
    "            \"nullable\": True\n",
    "        },\n",
    "        \"model_type\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"ResNet model type (will use config value if not specified)\",\n",
    "            \"required\": False,\n",
    "            \"nullable\": True\n",
    "        },\n",
    "        \"batch_size\": {\n",
    "            \"type\": \"integer\",\n",
    "            \"description\": \"Batch size for inference\",\n",
    "            \"required\": False,\n",
    "            \"nullable\": True\n",
    "        },\n",
    "        \"case_column\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"Name of the column in ground truth file that contains case/file identifiers\",\n",
    "            \"required\": False,\n",
    "            \"nullable\": True\n",
    "        },\n",
    "        \"label_column\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"Name of the column in ground truth file that contains the labels\",\n",
    "            \"required\": False,\n",
    "            \"nullable\": True\n",
    "        },\n",
    "        \"file_extension\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"File extension to add to case IDs if they don't already have one\",\n",
    "            \"required\": False,\n",
    "            \"nullable\": True\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    output_type = \"object\"\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        image_dir: str,\n",
    "        model_path: str,\n",
    "        output_dir: str,\n",
    "        config_path: Optional[str] = None,\n",
    "        ground_truth_file: Optional[str] = None,\n",
    "        class_names: Optional[List[str]] = None,\n",
    "        num_classes: Optional[int] = None,\n",
    "        model_type: Optional[str] = None,\n",
    "        batch_size: Optional[int] = 32,\n",
    "        case_column: Optional[str] = \"case\",\n",
    "        label_column: Optional[str] = \"label\",\n",
    "        file_extension: Optional[str] = \".png\"\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Run inference using a trained PyTorch ResNet model on new images.\n",
    "        \n",
    "        Args:\n",
    "            image_dir: Directory containing images for inference\n",
    "            model_path: Path to the trained model file (.pt format)\n",
    "            output_dir: Directory to save prediction outputs\n",
    "            config_path: Path to model configuration JSON file (optional)\n",
    "            ground_truth_file: CSV file with filenames and ground truth labels (optional)\n",
    "            class_names: List of class names corresponding to model output indices\n",
    "            num_classes: Number of classes (needed if not in config file)\n",
    "            model_type: ResNet model type (will use config value if not specified)\n",
    "            batch_size: Batch size for inference\n",
    "            case_column: Name of the column in ground truth file with case/file IDs\n",
    "            label_column: Name of the column in ground truth file with labels\n",
    "            file_extension: File extension to add to case IDs if needed\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with inference results and file paths\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Set up logging\n",
    "            os.makedirs(output_dir, exist_ok=True)\n",
    "            log_file = os.path.join(output_dir, \"inference.log\")\n",
    "            logging.basicConfig(\n",
    "                level=logging.INFO,\n",
    "                format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "                handlers=[\n",
    "                    logging.FileHandler(log_file),\n",
    "                    logging.StreamHandler()\n",
    "                ]\n",
    "            )\n",
    "            \n",
    "            # Load model configuration if provided\n",
    "            model_config = {}\n",
    "            if config_path and os.path.exists(config_path):\n",
    "                logging.info(f\"Loading model configuration from {config_path}\")\n",
    "                try:\n",
    "                    with open(config_path, 'r') as f:\n",
    "                        model_config = json.load(f)\n",
    "                    logging.info(f\"Model configuration loaded: {model_config}\")\n",
    "                except Exception as e:\n",
    "                    logging.warning(f\"Error loading model configuration: {str(e)}. Will use provided parameters.\")\n",
    "            \n",
    "            # Set parameters, with priority to explicit parameters over config values\n",
    "            if model_type is None:\n",
    "                model_type = model_config.get('model_type', 'resnet50')\n",
    "            \n",
    "            if num_classes is None:\n",
    "                num_classes = model_config.get('num_classes')\n",
    "                if num_classes is None:\n",
    "                    if class_names:\n",
    "                        num_classes = len(class_names)\n",
    "                    else:\n",
    "                        raise ValueError(\"Number of classes must be provided either directly, in config file, or through class_names\")\n",
    "            \n",
    "            image_size = model_config.get('image_size', 224)\n",
    "            \n",
    "            # Log inference settings\n",
    "            logging.info(f\"Starting ResNet inference with settings:\")\n",
    "            logging.info(f\"Model path: {model_path}\")\n",
    "            logging.info(f\"Model type: {model_type}\")\n",
    "            logging.info(f\"Number of classes: {num_classes}\")\n",
    "            logging.info(f\"Batch size: {batch_size}\")\n",
    "            logging.info(f\"Image directory: {image_dir}\")\n",
    "            logging.info(f\"Has ground truth: {ground_truth_file is not None}\")\n",
    "            \n",
    "            # Check if CUDA is available\n",
    "            device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "            logging.info(f\"Using device: {device}\")\n",
    "            \n",
    "            # Define image transformation\n",
    "            transform = transforms.Compose([\n",
    "                transforms.Resize((image_size, image_size)),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "            ])\n",
    "            \n",
    "            # Create model\n",
    "            logging.info(f\"Creating {model_type} model architecture...\")\n",
    "            \n",
    "            # Dictionary of available ResNet models\n",
    "            resnet_models = {\n",
    "                'resnet18': models.resnet18,\n",
    "                'resnet34': models.resnet34,\n",
    "                'resnet50': models.resnet50,\n",
    "                'resnet101': models.resnet101,\n",
    "                'resnet152': models.resnet152\n",
    "            }\n",
    "            \n",
    "            if model_type not in resnet_models:\n",
    "                logging.warning(f\"Invalid model type: {model_type}. Using resnet50 instead.\")\n",
    "                model_type = 'resnet50'\n",
    "                \n",
    "            model_fn = resnet_models[model_type]\n",
    "            model = model_fn(pretrained=False)\n",
    "            \n",
    "            # Modify the last layer for our number of classes\n",
    "            in_features = model.fc.in_features\n",
    "            model.fc = nn.Linear(in_features, num_classes)\n",
    "            \n",
    "            # Load trained weights\n",
    "            logging.info(f\"Loading model weights from {model_path}\")\n",
    "            try:\n",
    "                checkpoint = torch.load(model_path, map_location=device)\n",
    "                \n",
    "                # Handle different checkpoint formats\n",
    "                if 'model_state_dict' in checkpoint:\n",
    "                    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "                    logging.info(f\"Model loaded from checkpoint at epoch {checkpoint.get('epoch', 'unknown')}\")\n",
    "                else:\n",
    "                    model.load_state_dict(checkpoint)\n",
    "                    logging.info(\"Model loaded directly from state dict\")\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error loading model weights: {str(e)}\")\n",
    "                return {\n",
    "                    \"status\": \"error\",\n",
    "                    \"error_message\": f\"Failed to load model weights: {str(e)}\",\n",
    "                    \"model_path\": model_path\n",
    "                }\n",
    "            \n",
    "            # Move model to device and set to evaluation mode\n",
    "            model = model.to(device)\n",
    "            model.eval()\n",
    "            \n",
    "            # Load ground truth labels if provided\n",
    "            ground_truth = {}\n",
    "            if ground_truth_file and os.path.exists(ground_truth_file):\n",
    "                logging.info(f\"Loading ground truth data from {ground_truth_file}\")\n",
    "                try:\n",
    "                    gt_df = pd.read_csv(ground_truth_file)\n",
    "                    \n",
    "                    # Check required columns\n",
    "                    if case_column not in gt_df.columns or label_column not in gt_df.columns:\n",
    "                        raise ValueError(f\"Ground truth file must contain '{case_column}' and '{label_column}' columns\")\n",
    "                    \n",
    "                    # Create mapping of case ID to filename\n",
    "                    case_to_filename = {}\n",
    "                    \n",
    "                    # Create dictionary mapping filename to label\n",
    "                    for _, row in gt_df.iterrows():\n",
    "                        # Get case ID and convert to string\n",
    "                        case_id = str(row[case_column])\n",
    "                        \n",
    "                        # Add file extension if needed\n",
    "                        if not any(case_id.lower().endswith(ext) for ext in \n",
    "                                  ['.png', '.jpg', '.jpeg', '.bmp', '.tif', '.tiff']):\n",
    "                            filename = f\"{case_id}{file_extension}\"\n",
    "                        else:\n",
    "                            filename = case_id\n",
    "                        \n",
    "                        # Store the mapping and ground truth\n",
    "                        case_to_filename[case_id] = filename\n",
    "                        ground_truth[filename] = int(row[label_column])\n",
    "                        \n",
    "                    logging.info(f\"Loaded {len(ground_truth)} ground truth labels\")\n",
    "                    \n",
    "                    # Create a reverse mapping from filename to case ID for results\n",
    "                    filename_to_case = {v: k for k, v in case_to_filename.items()}\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    logging.error(f\"Error loading ground truth file: {str(e)}\")\n",
    "                    logging.warning(\"Will continue without ground truth evaluation\")\n",
    "                    ground_truth = {}\n",
    "            \n",
    "            # Get list of all image files\n",
    "            image_files = [f for f in os.listdir(image_dir) \n",
    "                         if os.path.isfile(os.path.join(image_dir, f)) and \n",
    "                         f.lower().endswith(('png', 'jpg', 'jpeg', 'bmp', 'tif', 'tiff'))]\n",
    "            \n",
    "            if not image_files:\n",
    "                logging.error(f\"No valid image files found in {image_dir}\")\n",
    "                return {\n",
    "                    \"status\": \"error\",\n",
    "                    \"error_message\": f\"No valid image files found in directory\",\n",
    "                    \"image_dir\": image_dir\n",
    "                }\n",
    "            \n",
    "            logging.info(f\"Found {len(image_files)} images for inference\")\n",
    "            \n",
    "            # Process images in batches for efficiency\n",
    "            all_predictions = []\n",
    "            batch_idx = 0\n",
    "            start_time = time.time()\n",
    "            \n",
    "            with torch.no_grad():  # No gradient computation needed for inference\n",
    "                for batch_start in range(0, len(image_files), batch_size):\n",
    "                    batch_end = min(batch_start + batch_size, len(image_files))\n",
    "                    batch_files = image_files[batch_start:batch_end]\n",
    "                    batch_imgs = []\n",
    "                    valid_files = []\n",
    "                    \n",
    "                    # Load and preprocess images in this batch\n",
    "                    for img_file in batch_files:\n",
    "                        try:\n",
    "                            img_path = os.path.join(image_dir, img_file)\n",
    "                            image = Image.open(img_path).convert('RGB')\n",
    "                            image_tensor = transform(image)\n",
    "                            batch_imgs.append(image_tensor)\n",
    "                            valid_files.append(img_file)\n",
    "                        except Exception as e:\n",
    "                            logging.warning(f\"Error processing image {img_file}: {str(e)}\")\n",
    "                            all_predictions.append({\n",
    "                                'filename': img_file,\n",
    "                                'error': str(e)\n",
    "                            })\n",
    "                    \n",
    "                    if not batch_imgs:  # Skip if no valid images in batch\n",
    "                        continue\n",
    "                    \n",
    "                    # Stack tensors into batch\n",
    "                    batch_tensor = torch.stack(batch_imgs).to(device)\n",
    "                    \n",
    "                    # Run inference\n",
    "                    outputs = model(batch_tensor)\n",
    "                    probabilities = torch.nn.functional.softmax(outputs, dim=1)\n",
    "                    \n",
    "                    # Process results\n",
    "                    batch_probs = probabilities.cpu().numpy()\n",
    "                    batch_preds = np.argmax(batch_probs, axis=1)\n",
    "                    \n",
    "                    # Store predictions\n",
    "                    for i, (filename, pred_class, probs) in enumerate(zip(valid_files, batch_preds, batch_probs)):\n",
    "                        # Get corresponding case ID if available\n",
    "                        case_id = filename_to_case.get(filename, filename) if 'filename_to_case' in locals() else filename\n",
    "                        \n",
    "                        result = {\n",
    "                            'filename': filename,\n",
    "                            'case_id': case_id,\n",
    "                            'predicted_class': int(pred_class),\n",
    "                            'confidence': float(probs[pred_class])\n",
    "                        }\n",
    "                        \n",
    "                        # Add ground truth if available\n",
    "                        if filename in ground_truth:\n",
    "                            result['true_label'] = int(ground_truth[filename])\n",
    "                            result['correct'] = result['predicted_class'] == result['true_label']\n",
    "                        \n",
    "                        # Add class name if available\n",
    "                        if class_names and pred_class < len(class_names):\n",
    "                            result['predicted_class_name'] = class_names[pred_class]\n",
    "                            \n",
    "                        # Add probabilities for each class\n",
    "                        for class_idx, prob in enumerate(probs):\n",
    "                            if class_names and class_idx < len(class_names):\n",
    "                                result[f'prob_{class_names[class_idx]}'] = float(prob)\n",
    "                            else:\n",
    "                                result[f'prob_class_{class_idx}'] = float(prob)\n",
    "                        \n",
    "                        all_predictions.append(result)\n",
    "                    \n",
    "                    batch_idx += 1\n",
    "                    if batch_idx % 10 == 0:\n",
    "                        logging.info(f\"Processed {batch_end}/{len(image_files)} images\")\n",
    "            \n",
    "            processing_time = time.time() - start_time\n",
    "            logging.info(f\"Inference completed in {processing_time:.2f} seconds\")\n",
    "            \n",
    "            # Create results dataframe\n",
    "            predictions_df = pd.DataFrame(all_predictions)\n",
    "            \n",
    "            # Save predictions to CSV\n",
    "            csv_path = os.path.join(output_dir, \"predictions.csv\")\n",
    "            predictions_df.to_csv(csv_path, index=False)\n",
    "            logging.info(f\"Saved predictions to {csv_path}\")\n",
    "            \n",
    "            # Calculate metrics if ground truth is available\n",
    "            metrics = {}\n",
    "            metrics_path = None\n",
    "            cm_path = None\n",
    "            roc_path = None\n",
    "            \n",
    "            if ground_truth and any('true_label' in p for p in all_predictions):\n",
    "                logging.info(\"Calculating performance metrics...\")\n",
    "                \n",
    "                # Filter predictions with ground truth\n",
    "                valid_preds = [p for p in all_predictions if 'true_label' in p]\n",
    "                \n",
    "                # Extract true labels and predictions\n",
    "                y_true = [p['true_label'] for p in valid_preds]\n",
    "                y_pred = [p['predicted_class'] for p in valid_preds]\n",
    "                \n",
    "                # Get unique classes in sorted order\n",
    "                unique_classes = sorted(list(set(y_true)))\n",
    "                \n",
    "                # Basic metrics\n",
    "                metrics['accuracy'] = accuracy_score(y_true, y_pred)\n",
    "                \n",
    "                try:\n",
    "                    # Multi-class metrics\n",
    "                    metrics['precision_macro'] = precision_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "                    metrics['recall_macro'] = recall_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "                    metrics['f1_macro'] = f1_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "                    \n",
    "                    # Per-class metrics\n",
    "                    class_report = classification_report(y_true, y_pred, output_dict=True, zero_division=0)\n",
    "                    \n",
    "                    # Extract per-class metrics\n",
    "                    for class_idx in unique_classes:\n",
    "                        class_key = str(class_idx)\n",
    "                        if class_key in class_report:\n",
    "                            class_metrics = class_report[class_key]\n",
    "                            class_name = class_names[class_idx] if class_names and class_idx < len(class_names) else f\"class_{class_idx}\"\n",
    "                            \n",
    "                            metrics[f'precision_{class_name}'] = class_metrics['precision']\n",
    "                            metrics[f'recall_{class_name}'] = class_metrics['recall']\n",
    "                            metrics[f'f1_{class_name}'] = class_metrics['f1-score']\n",
    "                            \n",
    "                            # Calculate sensitivity and specificity for each class\n",
    "                            # Sensitivity = recall\n",
    "                            metrics[f'sensitivity_{class_name}'] = class_metrics['recall']\n",
    "                            \n",
    "                            # Specificity = TN / (TN + FP)\n",
    "                            y_true_binary = [1 if y == class_idx else 0 for y in y_true]\n",
    "                            y_pred_binary = [1 if y == class_idx else 0 for y in y_pred]\n",
    "                            tn, fp, fn, tp = confusion_matrix(y_true_binary, y_pred_binary, labels=[0, 1]).ravel()\n",
    "                            specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "                            metrics[f'specificity_{class_name}'] = specificity\n",
    "                    \n",
    "                    # Calculate AUC and ROC curves if we have probability outputs\n",
    "                    if len(unique_classes) == 2:  # Binary classification\n",
    "                        # Extract probabilities for positive class\n",
    "                        class_idx = max(unique_classes)  # Assuming 1 is positive class in binary case\n",
    "                        if class_names and class_idx < len(class_names):\n",
    "                            prob_col = f'prob_{class_names[class_idx]}'\n",
    "                        else:\n",
    "                            prob_col = f'prob_class_{class_idx}'\n",
    "                        \n",
    "                        if prob_col in predictions_df.columns:\n",
    "                            y_score = predictions_df.loc[predictions_df['true_label'].notna(), prob_col].values\n",
    "                            y_true_for_auc = [y for i, y in enumerate(y_true)]\n",
    "                            \n",
    "                            # Calculate ROC curve and AUC\n",
    "                            metrics['auc'] = roc_auc_score(y_true_for_auc, y_score)\n",
    "                            fpr, tpr, _ = roc_curve(y_true_for_auc, y_score)\n",
    "                            \n",
    "                            # Plot ROC curve\n",
    "                            plt.figure(figsize=(8, 8))\n",
    "                            plt.plot(fpr, tpr, color='darkorange', lw=2, \n",
    "                                    label=f'ROC curve (area = {metrics[\"auc\"]:.3f})')\n",
    "                            plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "                            plt.xlim([0.0, 1.0])\n",
    "                            plt.ylim([0.0, 1.05])\n",
    "                            plt.xlabel('False Positive Rate')\n",
    "                            plt.ylabel('True Positive Rate')\n",
    "                            plt.title('Receiver Operating Characteristic')\n",
    "                            plt.legend(loc=\"lower right\")\n",
    "                            \n",
    "                            roc_path = os.path.join(output_dir, \"roc_curve.png\")\n",
    "                            plt.savefig(roc_path)\n",
    "                            plt.close()\n",
    "                    elif len(unique_classes) > 2:  # Multi-class\n",
    "                        # For multi-class, calculate one-vs-rest AUC for each class\n",
    "                        for class_idx in unique_classes:\n",
    "                            if class_names and class_idx < len(class_names):\n",
    "                                prob_col = f'prob_{class_names[class_idx]}'\n",
    "                                class_name = class_names[class_idx]\n",
    "                            else:\n",
    "                                prob_col = f'prob_class_{class_idx}'\n",
    "                                class_name = f\"class_{class_idx}\"\n",
    "                            \n",
    "                            if prob_col in predictions_df.columns:\n",
    "                                # Create binary labels for this class\n",
    "                                y_true_binary = [1 if y == class_idx else 0 for y in y_true]\n",
    "                                y_score = predictions_df.loc[predictions_df['true_label'].notna(), prob_col].values\n",
    "                                \n",
    "                                try:\n",
    "                                    # Calculate AUC\n",
    "                                    class_auc = roc_auc_score(y_true_binary, y_score)\n",
    "                                    metrics[f'auc_{class_name}'] = class_auc\n",
    "                                except Exception as e:\n",
    "                                    logging.warning(f\"Could not calculate AUC for {class_name}: {str(e)}\")\n",
    "                except Exception as e:\n",
    "                    logging.warning(f\"Error calculating some metrics: {str(e)}\")\n",
    "                \n",
    "                # Log all metrics\n",
    "                logging.info(\"Performance metrics:\")\n",
    "                for metric_name, metric_value in metrics.items():\n",
    "                    logging.info(f\"{metric_name}: {metric_value:.4f}\")\n",
    "                \n",
    "                # Create confusion matrix\n",
    "                cm = confusion_matrix(y_true, y_pred, labels=unique_classes)\n",
    "                \n",
    "                # Set up class labels for the plot\n",
    "                if class_names:\n",
    "                    labels = [class_names[i] if i < len(class_names) else f\"class_{i}\" for i in unique_classes]\n",
    "                else:\n",
    "                    labels = [f\"class_{i}\" for i in unique_classes]\n",
    "                \n",
    "                plt.figure(figsize=(10, 8))\n",
    "                sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)\n",
    "                plt.xlabel('Predicted')\n",
    "                plt.ylabel('True')\n",
    "                plt.title('Confusion Matrix')\n",
    "                cm_path = os.path.join(output_dir, \"confusion_matrix.png\")\n",
    "                plt.savefig(cm_path)\n",
    "                plt.close()\n",
    "                \n",
    "                # Save metrics to JSON\n",
    "                metrics_path = os.path.join(output_dir, \"metrics.json\")\n",
    "                with open(metrics_path, 'w') as f:\n",
    "                    json.dump(metrics, f, indent=4)\n",
    "                \n",
    "                # Create a readable metrics summary as CSV\n",
    "                metrics_summary = []\n",
    "                for metric_name, metric_value in metrics.items():\n",
    "                    metrics_summary.append({\n",
    "                        'metric': metric_name,\n",
    "                        'value': metric_value\n",
    "                    })\n",
    "                pd.DataFrame(metrics_summary).to_csv(\n",
    "                    os.path.join(output_dir, \"metrics_summary.csv\"), index=False\n",
    "                )\n",
    "            \n",
    "            # Return results\n",
    "            return {\n",
    "                \"status\": \"success\",\n",
    "                \"predictions_path\": csv_path,\n",
    "                \"log_file\": log_file,\n",
    "                \"metrics\": metrics if metrics else None,\n",
    "                \"metrics_path\": metrics_path,\n",
    "                \"confusion_matrix_path\": cm_path,\n",
    "                \"roc_curve_path\": roc_path if 'roc_path' in locals() else None,\n",
    "                \"image_dir\": image_dir,\n",
    "                \"model_path\": model_path,\n",
    "                \"output_dir\": output_dir,\n",
    "                \"num_images_processed\": len(all_predictions),\n",
    "                \"num_images_with_errors\": sum(1 for p in all_predictions if 'error' in p),\n",
    "                \"has_ground_truth\": bool(ground_truth),\n",
    "                \"processing_time_seconds\": processing_time\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error during inference: {str(e)}\", exc_info=True)\n",
    "            return {\n",
    "                \"status\": \"error\",\n",
    "                \"error_message\": str(e),\n",
    "                \"image_dir\": image_dir,\n",
    "                \"model_path\": model_path,\n",
    "                \"output_dir\": output_dir\n",
    "            }\n",
    "\n",
    "class PyTorchInceptionV3TrainingTool(Tool):\n",
    "    name = \"pytorch_inception_v3_training\"\n",
    "    description = \"\"\"\n",
    "    This tool trains an Inception V3 model using PyTorch for medical image classification.\n",
    "    It can train from scratch or fine-tune a pre-trained model, and includes validation metrics.\n",
    "    \"\"\"\n",
    "    \n",
    "    inputs = {\n",
    "        \"data_dir\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"Directory containing dataset with training and validation folders\"\n",
    "        },\n",
    "        \"output_dir\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"Directory where the trained model and results will be saved\"\n",
    "        },\n",
    "        \"num_classes\": {\n",
    "            \"type\": \"integer\",\n",
    "            \"description\": \"Number of classes for classification\"\n",
    "        },\n",
    "        \"num_epochs\": {\n",
    "            \"type\": \"integer\",\n",
    "            \"description\": \"Number of training epochs\",\n",
    "            \"required\": False,\n",
    "            \"nullable\": True\n",
    "        },\n",
    "        \"batch_size\": {\n",
    "            \"type\": \"integer\",\n",
    "            \"description\": \"Batch size for training\",\n",
    "            \"required\": False,\n",
    "            \"nullable\": True\n",
    "        },\n",
    "        \"pretrained\": {\n",
    "            \"type\": \"boolean\",\n",
    "            \"description\": \"Whether to use pretrained weights\",\n",
    "            \"required\": False,\n",
    "            \"nullable\": True\n",
    "        },\n",
    "        \"early_stopping\": {\n",
    "            \"type\": \"boolean\", \n",
    "            \"description\": \"Whether to use early stopping\",\n",
    "            \"required\": False,\n",
    "            \"nullable\": True\n",
    "        },\n",
    "        \"patience\": {\n",
    "            \"type\": \"integer\",\n",
    "            \"description\": \"Number of epochs to wait before early stopping\",\n",
    "            \"required\": False,\n",
    "            \"nullable\": True\n",
    "        },\n",
    "        \"aux_logits\": {\n",
    "            \"type\": \"boolean\",\n",
    "            \"description\": \"Whether to use auxiliary logits during training (Inception specific)\",\n",
    "            \"required\": False,\n",
    "            \"nullable\": True\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    output_type = \"object\"\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        data_dir: str,\n",
    "        output_dir: str,\n",
    "        num_classes: int,\n",
    "        num_epochs: Optional[int] = 10,\n",
    "        batch_size: Optional[int] = 16,\n",
    "        pretrained: Optional[bool] = True,\n",
    "        early_stopping: Optional[bool] = True,\n",
    "        patience: Optional[int] = 5,\n",
    "        aux_logits: Optional[bool] = True\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Train an Inception V3 model for medical image classification.\n",
    "        \n",
    "        Args:\n",
    "            data_dir: Directory containing dataset with train and val subdirectories\n",
    "            output_dir: Directory to save model and results\n",
    "            num_classes: Number of classes for classification\n",
    "            num_epochs: Number of training epochs\n",
    "            batch_size: Batch size for training\n",
    "            pretrained: Whether to use pretrained weights\n",
    "            early_stopping: Whether to use early stopping\n",
    "            patience: Number of epochs to wait before early stopping\n",
    "            aux_logits: Whether to use auxiliary logits during training (Inception specific)\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with training results and model paths\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Define the image size for Inception V3\n",
    "            inception_image_size = (299, 299)\n",
    "            \n",
    "            # Set up logging\n",
    "            os.makedirs(output_dir, exist_ok=True)\n",
    "            log_file = os.path.join(output_dir, \"training.log\")\n",
    "            logging.basicConfig(\n",
    "                level=logging.INFO,\n",
    "                format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "                handlers=[\n",
    "                    logging.FileHandler(log_file),\n",
    "                    logging.StreamHandler()\n",
    "                ]\n",
    "            )\n",
    "            \n",
    "            # Log configuration\n",
    "            logging.info(f\"Starting Inception V3 training with configuration:\")\n",
    "            logging.info(f\"Number of classes: {num_classes}\")\n",
    "            logging.info(f\"Pretrained: {pretrained}\")\n",
    "            logging.info(f\"Early stopping: {early_stopping}\")\n",
    "            logging.info(f\"Auxiliary logits: {aux_logits}\")\n",
    "            if early_stopping:\n",
    "                logging.info(f\"Patience: {patience}\")\n",
    "            logging.info(f\"Data directory: {data_dir}\")\n",
    "            \n",
    "            # Check if CUDA is available\n",
    "            device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "            logging.info(f\"Using device: {device}\")\n",
    "            \n",
    "            # Define transformations - Inception V3 requires 299x299 input\n",
    "            train_transform = transforms.Compose([\n",
    "                transforms.Resize(inception_image_size),  # Inception V3 requires 299x299 input\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.RandomRotation(10),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "            ])\n",
    "            \n",
    "            val_transform = transforms.Compose([\n",
    "                transforms.Resize(inception_image_size),  # Inception V3 requires 299x299 input\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "            ])\n",
    "            \n",
    "            # Setup directories\n",
    "            train_dir = os.path.join(data_dir, 'train')\n",
    "            val_dir = os.path.join(data_dir, 'val')\n",
    "            test_dir = os.path.join(data_dir, 'test')\n",
    "            \n",
    "            if not os.path.exists(train_dir):\n",
    "                raise ValueError(f\"Training directory not found: {train_dir}\")\n",
    "            \n",
    "            # Check if validation directory exists, otherwise use a split from training\n",
    "            use_train_val_split = not os.path.exists(val_dir)\n",
    "            \n",
    "            # Check if data directory has a labels.csv file or if images are in class subdirectories\n",
    "            train_labels_file = os.path.join(data_dir, 'labels.csv')\n",
    "            use_csv_labels = os.path.exists(train_labels_file)\n",
    "            \n",
    "            # Create datasets based on the data organization\n",
    "            if use_csv_labels:\n",
    "                # Using CSV file with image paths and labels\n",
    "                logging.info(f\"Using labels from CSV file: {train_labels_file}\")\n",
    "                train_dataset = MedicalImageDataset(\n",
    "                    image_dir=train_dir,\n",
    "                    labels_file=train_labels_file,\n",
    "                    transform=train_transform,\n",
    "                    is_test=False,\n",
    "                    image_size=inception_image_size  # Inception V3 requires 299x299\n",
    "                )\n",
    "                \n",
    "                if not use_train_val_split:\n",
    "                    val_labels_file = os.path.join(data_dir, 'val_labels.csv')\n",
    "                    if not os.path.exists(val_labels_file):\n",
    "                        val_labels_file = train_labels_file  # Fallback to same labels file\n",
    "                    \n",
    "                    val_dataset = MedicalImageDataset(\n",
    "                        image_dir=val_dir,\n",
    "                        labels_file=val_labels_file,\n",
    "                        transform=val_transform,\n",
    "                        is_test=False,\n",
    "                        image_size=inception_image_size  # Inception V3 requires 299x299\n",
    "                    )\n",
    "            else:\n",
    "                # Using directory structure (each class in its own subdirectory)\n",
    "                logging.info(\"Using directory structure for class labels\")\n",
    "                train_dataset = datasets.ImageFolder(\n",
    "                    root=train_dir,\n",
    "                    transform=train_transform\n",
    "                )\n",
    "                \n",
    "                if not use_train_val_split:\n",
    "                    val_dataset = datasets.ImageFolder(\n",
    "                        root=val_dir,\n",
    "                        transform=val_transform\n",
    "                    )\n",
    "            \n",
    "            # Split training data if no validation directory\n",
    "            if use_train_val_split:\n",
    "                train_size = int(0.8 * len(train_dataset))\n",
    "                val_size = len(train_dataset) - train_size\n",
    "                train_dataset, val_dataset = torch.utils.data.random_split(\n",
    "                    train_dataset, [train_size, val_size]\n",
    "                )\n",
    "            \n",
    "            logging.info(f\"Training dataset size: {len(train_dataset)}\")\n",
    "            logging.info(f\"Validation dataset size: {len(val_dataset)}\")\n",
    "            \n",
    "            # Create data loaders\n",
    "            train_loader = DataLoader(\n",
    "                train_dataset, \n",
    "                batch_size=batch_size, \n",
    "                shuffle=True, \n",
    "                num_workers=4\n",
    "            )\n",
    "            \n",
    "            val_loader = DataLoader(\n",
    "                val_dataset,\n",
    "                batch_size=batch_size,\n",
    "                shuffle=False,\n",
    "                num_workers=4\n",
    "            )\n",
    "            \n",
    "            # Create Inception V3 model\n",
    "            logging.info(\"Creating Inception V3 model...\")\n",
    "            \n",
    "            # Initialize model with or without pretrained weights\n",
    "            if pretrained:\n",
    "                model = models.inception_v3(weights=models.Inception_V3_Weights.IMAGENET1K_V1, aux_logits=aux_logits)\n",
    "                logging.info(\"Using pretrained weights from ImageNet\")\n",
    "            else:\n",
    "                model = models.inception_v3(weights=None, aux_logits=aux_logits)\n",
    "                logging.info(\"Initializing model with random weights\")\n",
    "            \n",
    "            # Modify the classifier for our number of classes\n",
    "            # Inception V3 has two outputs if aux_logits=True\n",
    "            # Replace the main classifier\n",
    "            in_features = model.fc.in_features\n",
    "            model.fc = nn.Linear(in_features, num_classes)\n",
    "            \n",
    "            # Replace the auxiliary classifier if aux_logits is enabled\n",
    "            if aux_logits:\n",
    "                in_features_aux = model.AuxLogits.fc.in_features\n",
    "                model.AuxLogits.fc = nn.Linear(in_features_aux, num_classes)\n",
    "            \n",
    "            # Move model to device\n",
    "            model = model.to(device)\n",
    "            \n",
    "            # Loss function and optimizer\n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "            optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "            \n",
    "            # Learning rate scheduler\n",
    "            scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "                optimizer, 'min', patience=2, factor=0.5\n",
    "            )\n",
    "            \n",
    "            # Track best model\n",
    "            best_val_loss = float('inf')\n",
    "            best_val_acc = 0.0\n",
    "            patience_counter = 0\n",
    "            \n",
    "            # Training history\n",
    "            history = {\n",
    "                'train_loss': [],\n",
    "                'val_loss': [],\n",
    "                'train_acc': [],\n",
    "                'val_acc': [],\n",
    "                'learning_rates': []\n",
    "            }\n",
    "            \n",
    "            # Training loop\n",
    "            logging.info(\"Starting training...\")\n",
    "            start_time = time.time()\n",
    "            \n",
    "            for epoch in range(num_epochs):\n",
    "                # Training phase\n",
    "                model.train()\n",
    "                train_loss = 0.0\n",
    "                train_correct = 0\n",
    "                train_total = 0\n",
    "                \n",
    "                for inputs, labels in train_loader:\n",
    "                    inputs, labels = inputs.to(device), labels.to(device)\n",
    "                    \n",
    "                    # Zero the gradients\n",
    "                    optimizer.zero_grad()\n",
    "                    \n",
    "                    # Forward pass\n",
    "                    # Inception v3 returns tuple (output, aux_output) when training if aux_logits=True\n",
    "                    if aux_logits:\n",
    "                        outputs, aux_outputs = model(inputs)\n",
    "                        loss1 = criterion(outputs, labels)\n",
    "                        loss2 = criterion(aux_outputs, labels)\n",
    "                        # During training, the auxiliary classifier's loss is weighted by 0.3\n",
    "                        loss = loss1 + 0.3 * loss2\n",
    "                    else:\n",
    "                        outputs = model(inputs)\n",
    "                        loss = criterion(outputs, labels)\n",
    "                    \n",
    "                    # Backward pass and optimize\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    \n",
    "                    # Statistics\n",
    "                    train_loss += loss.item() * inputs.size(0)\n",
    "                    _, predicted = torch.max(outputs, 1)\n",
    "                    train_total += labels.size(0)\n",
    "                    train_correct += (predicted == labels).sum().item()\n",
    "                \n",
    "                epoch_train_loss = train_loss / train_total\n",
    "                epoch_train_acc = 100 * train_correct / train_total\n",
    "                \n",
    "                # Validation phase\n",
    "                model.eval()\n",
    "                val_loss = 0.0\n",
    "                val_correct = 0\n",
    "                val_total = 0\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    for inputs, labels in val_loader:\n",
    "                        inputs, labels = inputs.to(device), labels.to(device)\n",
    "                        \n",
    "                        # During evaluation, only the main output is used\n",
    "                        outputs = model(inputs)\n",
    "                        \n",
    "                        # Handle the case where model returns a tuple (happens during evaluation with aux_logits=True)\n",
    "                        if isinstance(outputs, tuple):\n",
    "                            outputs = outputs[0]\n",
    "                        \n",
    "                        loss = criterion(outputs, labels)\n",
    "                        \n",
    "                        val_loss += loss.item() * inputs.size(0)\n",
    "                        _, predicted = torch.max(outputs, 1)\n",
    "                        val_total += labels.size(0)\n",
    "                        val_correct += (predicted == labels).sum().item()\n",
    "                \n",
    "                epoch_val_loss = val_loss / val_total\n",
    "                epoch_val_acc = 100 * val_correct / val_total\n",
    "                \n",
    "                # Update learning rate\n",
    "                scheduler.step(epoch_val_loss)\n",
    "                \n",
    "                # Get current learning rate\n",
    "                current_lr = optimizer.param_groups[0]['lr']\n",
    "                \n",
    "                # Save history\n",
    "                history['train_loss'].append(epoch_train_loss)\n",
    "                history['val_loss'].append(epoch_val_loss)\n",
    "                history['train_acc'].append(epoch_train_acc)\n",
    "                history['val_acc'].append(epoch_val_acc)\n",
    "                history['learning_rates'].append(current_lr)\n",
    "                \n",
    "                # Log progress\n",
    "                logging.info(f\"Epoch {epoch+1}/{num_epochs} - \"\n",
    "                           f\"Train Loss: {epoch_train_loss:.4f}, \"\n",
    "                           f\"Train Acc: {epoch_train_acc:.2f}%, \"\n",
    "                           f\"Val Loss: {epoch_val_loss:.4f}, \"\n",
    "                           f\"Val Acc: {epoch_val_acc:.2f}%, \"\n",
    "                           f\"LR: {current_lr:.6f}\")\n",
    "                \n",
    "                # Check for improvement\n",
    "                if epoch_val_acc > best_val_acc:\n",
    "                    best_val_acc = epoch_val_acc\n",
    "                    best_val_loss = epoch_val_loss\n",
    "                    patience_counter = 0\n",
    "                    \n",
    "                    # Save best model\n",
    "                    best_model_path = os.path.join(output_dir, \"best_model.pt\")\n",
    "                    torch.save({\n",
    "                        'epoch': epoch,\n",
    "                        'model_state_dict': model.state_dict(),\n",
    "                        'optimizer_state_dict': optimizer.state_dict(),\n",
    "                        'loss': best_val_loss,\n",
    "                        'accuracy': best_val_acc\n",
    "                    }, best_model_path)\n",
    "                    logging.info(f\"Saved best model with accuracy: {best_val_acc:.2f}%\")\n",
    "                else:\n",
    "                    patience_counter += 1\n",
    "                \n",
    "                # Early stopping if enabled\n",
    "                if early_stopping and patience_counter >= patience:\n",
    "                    logging.info(f\"Early stopping triggered after {epoch+1} epochs\")\n",
    "                    break\n",
    "            \n",
    "            # Calculate training time\n",
    "            total_time = time.time() - start_time\n",
    "            logging.info(f\"Training completed in {total_time:.2f} seconds\")\n",
    "            \n",
    "            # Save final model\n",
    "            final_model_path = os.path.join(output_dir, \"final_model.pt\")\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': epoch_val_loss,\n",
    "                'accuracy': epoch_val_acc\n",
    "            }, final_model_path)\n",
    "            \n",
    "            # If test directory exists, evaluate on test set\n",
    "            if os.path.exists(test_dir):\n",
    "                logging.info(\"Evaluating model on test set...\")\n",
    "                # Create test dataset and dataloader\n",
    "                if use_csv_labels:\n",
    "                    test_labels_file = os.path.join(data_dir, 'test_labels.csv')\n",
    "                    if not os.path.exists(test_labels_file):\n",
    "                        test_labels_file = train_labels_file  # Fallback to same labels file\n",
    "                    \n",
    "                    test_dataset = MedicalImageDataset(\n",
    "                        image_dir=test_dir,\n",
    "                        labels_file=test_labels_file,\n",
    "                        transform=val_transform,  # Use the same transform as validation\n",
    "                        is_test=False,\n",
    "                        image_size=inception_image_size  # Inception V3 requires 299x299\n",
    "                    )\n",
    "                else:\n",
    "                    test_dataset = datasets.ImageFolder(\n",
    "                        root=test_dir,\n",
    "                        transform=val_transform\n",
    "                    )\n",
    "                \n",
    "                test_loader = DataLoader(\n",
    "                    test_dataset,\n",
    "                    batch_size=batch_size,\n",
    "                    shuffle=False,\n",
    "                    num_workers=4\n",
    "                )\n",
    "                \n",
    "                # Evaluate on test set\n",
    "                model.eval()\n",
    "                test_loss = 0.0\n",
    "                test_correct = 0\n",
    "                test_total = 0\n",
    "                all_preds = []\n",
    "                all_labels = []\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    for inputs, labels in test_loader:\n",
    "                        inputs, labels = inputs.to(device), labels.to(device)\n",
    "                        \n",
    "                        outputs = model(inputs)\n",
    "                        \n",
    "                        # Handle the case where model returns a tuple (happens during evaluation with aux_logits=True)\n",
    "                        if isinstance(outputs, tuple):\n",
    "                            outputs = outputs[0]\n",
    "                            \n",
    "                        loss = criterion(outputs, labels)\n",
    "                        \n",
    "                        test_loss += loss.item() * inputs.size(0)\n",
    "                        _, predicted = torch.max(outputs, 1)\n",
    "                        test_total += labels.size(0)\n",
    "                        test_correct += (predicted == labels).sum().item()\n",
    "                        \n",
    "                        # Store predictions and labels for metrics\n",
    "                        all_preds.extend(predicted.cpu().numpy())\n",
    "                        all_labels.extend(labels.cpu().numpy())\n",
    "                \n",
    "                test_acc = 100 * test_correct / test_total\n",
    "                logging.info(f\"Test accuracy: {test_acc:.2f}%\")\n",
    "                \n",
    "                # Calculate and log detailed metrics\n",
    "                metrics = {}\n",
    "                metrics['accuracy'] = accuracy_score(all_labels, all_preds)\n",
    "                metrics['precision_macro'] = precision_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "                metrics['recall_macro'] = recall_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "                metrics['f1_macro'] = f1_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "                \n",
    "                # Add per-class metrics if there are multiple classes\n",
    "                if num_classes > 1:\n",
    "                    metrics['precision_per_class'] = precision_score(all_labels, all_preds, average=None, zero_division=0).tolist()\n",
    "                    metrics['recall_per_class'] = recall_score(all_labels, all_preds, average=None, zero_division=0).tolist()\n",
    "                    metrics['f1_per_class'] = f1_score(all_labels, all_preds, average=None, zero_division=0).tolist()\n",
    "                \n",
    "                # Save metrics to JSON\n",
    "                metrics_path = os.path.join(output_dir, \"test_metrics.json\")\n",
    "                with open(metrics_path, 'w') as f:\n",
    "                    json.dump(metrics, f, indent=4)\n",
    "                \n",
    "                # Generate confusion matrix\n",
    "                cm = confusion_matrix(all_labels, all_preds)\n",
    "                plt.figure(figsize=(10, 8))\n",
    "                sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "                plt.xlabel('Predicted')\n",
    "                plt.ylabel('True')\n",
    "                plt.title('Confusion Matrix')\n",
    "                cm_path = os.path.join(output_dir, \"confusion_matrix.png\")\n",
    "                plt.savefig(cm_path)\n",
    "                plt.close()\n",
    "            \n",
    "            # Save training history\n",
    "            history_path = os.path.join(output_dir, \"training_history.json\")\n",
    "            with open(history_path, 'w') as f:\n",
    "                json.dump(history, f, indent=4)\n",
    "            \n",
    "            # Create plots\n",
    "            plt.figure(figsize=(15, 5))\n",
    "            \n",
    "            plt.subplot(1, 3, 1)\n",
    "            plt.plot(history['train_loss'], label='Training Loss')\n",
    "            plt.plot(history['val_loss'], label='Validation Loss')\n",
    "            plt.xlabel('Epoch')\n",
    "            plt.ylabel('Loss')\n",
    "            plt.legend()\n",
    "            plt.title('Loss Curves')\n",
    "            \n",
    "            plt.subplot(1, 3, 2)\n",
    "            plt.plot(history['train_acc'], label='Training Accuracy')\n",
    "            plt.plot(history['val_acc'], label='Validation Accuracy')\n",
    "            plt.xlabel('Epoch')\n",
    "            plt.ylabel('Accuracy (%)')\n",
    "            plt.legend()\n",
    "            plt.title('Accuracy Curves')\n",
    "            \n",
    "            plt.subplot(1, 3, 3)\n",
    "            plt.plot(history['learning_rates'])\n",
    "            plt.xlabel('Epoch')\n",
    "            plt.ylabel('Learning Rate')\n",
    "            plt.title('Learning Rate Schedule')\n",
    "            plt.yscale('log')\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plots_path = os.path.join(output_dir, \"training_plots.png\")\n",
    "            plt.savefig(plots_path)\n",
    "            \n",
    "            # Save model configuration\n",
    "            config = {\n",
    "                'model_type': 'inception_v3',\n",
    "                'num_classes': num_classes,\n",
    "                'image_size': 299,  # Inception V3 uses 299x299 images\n",
    "                'pretrained': pretrained,\n",
    "                'early_stopping': early_stopping,\n",
    "                'patience': patience if early_stopping else None,\n",
    "                'best_epoch': epoch,\n",
    "                'best_accuracy': best_val_acc,\n",
    "                'training_epochs': epoch + 1,\n",
    "                'early_stopped': early_stopping and patience_counter >= patience,\n",
    "                'batch_size': batch_size,\n",
    "                'aux_logits': aux_logits\n",
    "            }\n",
    "            \n",
    "            config_path = os.path.join(output_dir, \"model_config.json\")\n",
    "            with open(config_path, 'w') as f:\n",
    "                json.dump(config, f, indent=4)\n",
    "            \n",
    "            # Return results\n",
    "            return {\n",
    "                \"status\": \"success\",\n",
    "                \"best_model_path\": best_model_path,\n",
    "                \"final_model_path\": final_model_path,\n",
    "                \"config_path\": config_path,\n",
    "                \"plots_path\": plots_path,\n",
    "                \"history_path\": history_path,\n",
    "                \"test_metrics_path\": metrics_path if 'metrics_path' in locals() else None,\n",
    "                \"confusion_matrix_path\": cm_path if 'cm_path' in locals() else None,\n",
    "                \"best_accuracy\": best_val_acc,\n",
    "                \"test_accuracy\": test_acc if 'test_acc' in locals() else None,\n",
    "                \"training_time_seconds\": total_time,\n",
    "                \"epochs_completed\": epoch + 1,\n",
    "                \"early_stopped\": early_stopping and patience_counter >= patience,\n",
    "                \"early_stopping_used\": early_stopping,\n",
    "                \"aux_logits_used\": aux_logits\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error during training: {str(e)}\", exc_info=True)\n",
    "            return {\n",
    "                \"status\": \"error\",\n",
    "                \"error_message\": str(e),\n",
    "                \"data_dir\": data_dir,\n",
    "                \"output_dir\": output_dir\n",
    "            }\n",
    "\n",
    "class PyTorchInceptionV3InferenceTool(Tool):\n",
    "    name = \"pytorch_inception_v3_inference\"\n",
    "    description = \"\"\"\n",
    "    This tool uses a trained PyTorch Inception V3 model to perform inference on new images.\n",
    "    It can also calculate performance metrics if ground truth labels are provided.\n",
    "    \"\"\"\n",
    "    \n",
    "    inputs = {\n",
    "        \"image_dir\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"Directory containing images for inference\"\n",
    "        },\n",
    "        \"model_path\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"Path to the trained model file (.pt format)\"\n",
    "        },\n",
    "        \"output_dir\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"Directory where prediction results will be saved\"\n",
    "        },\n",
    "        \"config_path\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"Path to model configuration JSON file (optional)\",\n",
    "            \"required\": False,\n",
    "            \"nullable\": True\n",
    "        },\n",
    "        \"ground_truth_file\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"CSV file with image filenames and ground truth labels for evaluation (optional)\",\n",
    "            \"required\": False,\n",
    "            \"nullable\": True\n",
    "        },\n",
    "        \"class_names\": {\n",
    "            \"type\": \"array\",\n",
    "            \"description\": \"List of class names corresponding to model output indices (optional)\",\n",
    "            \"required\": False,\n",
    "            \"nullable\": True\n",
    "        },\n",
    "        \"num_classes\": {\n",
    "            \"type\": \"integer\",\n",
    "            \"description\": \"Number of classes (needed if not provided in config file)\",\n",
    "            \"required\": False,\n",
    "            \"nullable\": True\n",
    "        },\n",
    "        \"batch_size\": {\n",
    "            \"type\": \"integer\",\n",
    "            \"description\": \"Batch size for inference\",\n",
    "            \"required\": False,\n",
    "            \"nullable\": True\n",
    "        },\n",
    "        \"case_column\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"Name of the column in ground truth file that contains case/file identifiers\",\n",
    "            \"required\": False,\n",
    "            \"nullable\": True\n",
    "        },\n",
    "        \"label_column\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"Name of the column in ground truth file that contains the labels\",\n",
    "            \"required\": False,\n",
    "            \"nullable\": True\n",
    "        },\n",
    "        \"file_extension\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"File extension to add to case IDs if they don't already have one\",\n",
    "            \"required\": False,\n",
    "            \"nullable\": True\n",
    "        },\n",
    "        \"aux_logits\": {\n",
    "            \"type\": \"boolean\",\n",
    "            \"description\": \"Whether the model was trained with auxiliary logits (Inception specific)\",\n",
    "            \"required\": False,\n",
    "            \"nullable\": True\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    output_type = \"object\"\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        image_dir: str,\n",
    "        model_path: str,\n",
    "        output_dir: str,\n",
    "        config_path: Optional[str] = None,\n",
    "        ground_truth_file: Optional[str] = None,\n",
    "        class_names: Optional[List[str]] = None,\n",
    "        num_classes: Optional[int] = None,\n",
    "        batch_size: Optional[int] = 32,\n",
    "        case_column: Optional[str] = \"case\",\n",
    "        label_column: Optional[str] = \"label\",\n",
    "        file_extension: Optional[str] = \".png\",\n",
    "        aux_logits: Optional[bool] = True\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Run inference using a trained PyTorch Inception V3 model on new images.\n",
    "        \n",
    "        Args:\n",
    "            image_dir: Directory containing images for inference\n",
    "            model_path: Path to the trained model file (.pt format)\n",
    "            output_dir: Directory to save prediction outputs\n",
    "            config_path: Path to model configuration JSON file (optional)\n",
    "            ground_truth_file: CSV file with filenames and ground truth labels (optional)\n",
    "            class_names: List of class names corresponding to model output indices\n",
    "            num_classes: Number of classes (needed if not in config file)\n",
    "            batch_size: Batch size for inference\n",
    "            case_column: Name of the column in ground truth file with case/file IDs\n",
    "            label_column: Name of the column in ground truth file with labels\n",
    "            file_extension: File extension to add to case IDs if needed\n",
    "            aux_logits: Whether the model was trained with auxiliary logits\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with inference results and file paths\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Set up logging\n",
    "            os.makedirs(output_dir, exist_ok=True)\n",
    "            log_file = os.path.join(output_dir, \"inference.log\")\n",
    "            logging.basicConfig(\n",
    "                level=logging.INFO,\n",
    "                format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "                handlers=[\n",
    "                    logging.FileHandler(log_file),\n",
    "                    logging.StreamHandler()\n",
    "                ]\n",
    "            )\n",
    "            \n",
    "            # Load model configuration if provided\n",
    "            model_config = {}\n",
    "            if config_path and os.path.exists(config_path):\n",
    "                logging.info(f\"Loading model configuration from {config_path}\")\n",
    "                try:\n",
    "                    with open(config_path, 'r') as f:\n",
    "                        model_config = json.load(f)\n",
    "                    logging.info(f\"Model configuration loaded: {model_config}\")\n",
    "                except Exception as e:\n",
    "                    logging.warning(f\"Error loading model configuration: {str(e)}. Will use provided parameters.\")\n",
    "            \n",
    "            # Set parameters, with priority to explicit parameters over config values\n",
    "            model_type = 'inception_v3'  # Fixed for this tool\n",
    "            \n",
    "            if num_classes is None:\n",
    "                num_classes = model_config.get('num_classes')\n",
    "                if num_classes is None:\n",
    "                    if class_names:\n",
    "                        num_classes = len(class_names)\n",
    "                    else:\n",
    "                        raise ValueError(\"Number of classes must be provided either directly, in config file, or through class_names\")\n",
    "            \n",
    "            # Inception V3 requires 299x299 images\n",
    "            image_size = model_config.get('image_size', 299)\n",
    "            \n",
    "            # Get aux_logits from config if not provided\n",
    "            if aux_logits is None:\n",
    "                aux_logits = model_config.get('aux_logits', True)\n",
    "            \n",
    "            # Log inference settings\n",
    "            logging.info(f\"Starting Inception V3 inference with settings:\")\n",
    "            logging.info(f\"Model path: {model_path}\")\n",
    "            logging.info(f\"Model type: {model_type}\")\n",
    "            logging.info(f\"Number of classes: {num_classes}\")\n",
    "            logging.info(f\"Batch size: {batch_size}\")\n",
    "            logging.info(f\"Image directory: {image_dir}\")\n",
    "            logging.info(f\"Aux logits: {aux_logits}\")\n",
    "            logging.info(f\"Has ground truth: {ground_truth_file is not None}\")\n",
    "            \n",
    "            # Check if CUDA is available\n",
    "            device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "            logging.info(f\"Using device: {device}\")\n",
    "            \n",
    "            # Define image transformation (Inception V3 requires 299x299 input)\n",
    "            transform = transforms.Compose([\n",
    "                transforms.Resize((image_size, image_size)),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "            ])\n",
    "            \n",
    "            # Create model\n",
    "            logging.info(f\"Creating Inception V3 model architecture...\")\n",
    "            \n",
    "            # Create Inception V3 model\n",
    "            model = models.inception_v3(pretrained=False, aux_logits=aux_logits)\n",
    "            \n",
    "            # Modify the classifiers for our number of classes\n",
    "            # Main classifier\n",
    "            in_features = model.fc.in_features\n",
    "            model.fc = nn.Linear(in_features, num_classes)\n",
    "            \n",
    "            # Auxiliary classifier (if used)\n",
    "            if aux_logits:\n",
    "                in_features_aux = model.AuxLogits.fc.in_features\n",
    "                model.AuxLogits.fc = nn.Linear(in_features_aux, num_classes)\n",
    "            \n",
    "            # Load trained weights\n",
    "            logging.info(f\"Loading model weights from {model_path}\")\n",
    "            try:\n",
    "                checkpoint = torch.load(model_path, map_location=device)\n",
    "                \n",
    "                # Handle different checkpoint formats\n",
    "                if 'model_state_dict' in checkpoint:\n",
    "                    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "                    logging.info(f\"Model loaded from checkpoint at epoch {checkpoint.get('epoch', 'unknown')}\")\n",
    "                else:\n",
    "                    model.load_state_dict(checkpoint)\n",
    "                    logging.info(\"Model loaded directly from state dict\")\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error loading model weights: {str(e)}\")\n",
    "                return {\n",
    "                    \"status\": \"error\",\n",
    "                    \"error_message\": f\"Failed to load model weights: {str(e)}\",\n",
    "                    \"model_path\": model_path\n",
    "                }\n",
    "            \n",
    "            # Move model to device and set to evaluation mode\n",
    "            model = model.to(device)\n",
    "            model.eval()\n",
    "            \n",
    "            # Load ground truth labels if provided\n",
    "            ground_truth = {}\n",
    "            if ground_truth_file and os.path.exists(ground_truth_file):\n",
    "                logging.info(f\"Loading ground truth data from {ground_truth_file}\")\n",
    "                try:\n",
    "                    gt_df = pd.read_csv(ground_truth_file)\n",
    "                    \n",
    "                    # Check required columns\n",
    "                    if case_column not in gt_df.columns or label_column not in gt_df.columns:\n",
    "                        raise ValueError(f\"Ground truth file must contain '{case_column}' and '{label_column}' columns\")\n",
    "                    \n",
    "                    # Create mapping of case ID to filename\n",
    "                    case_to_filename = {}\n",
    "                    \n",
    "                    # Create dictionary mapping filename to label\n",
    "                    for _, row in gt_df.iterrows():\n",
    "                        # Get case ID and convert to string\n",
    "                        case_id = str(row[case_column])\n",
    "                        \n",
    "                        # Add file extension if needed\n",
    "                        if not any(case_id.lower().endswith(ext) for ext in \n",
    "                                  ['.png', '.jpg', '.jpeg', '.bmp', '.tif', '.tiff']):\n",
    "                            filename = f\"{case_id}{file_extension}\"\n",
    "                        else:\n",
    "                            filename = case_id\n",
    "                        \n",
    "                        # Store the mapping and ground truth\n",
    "                        case_to_filename[case_id] = filename\n",
    "                        ground_truth[filename] = int(row[label_column])\n",
    "                        \n",
    "                    logging.info(f\"Loaded {len(ground_truth)} ground truth labels\")\n",
    "                    \n",
    "                    # Create a reverse mapping from filename to case ID for results\n",
    "                    filename_to_case = {v: k for k, v in case_to_filename.items()}\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    logging.error(f\"Error loading ground truth file: {str(e)}\")\n",
    "                    logging.warning(\"Will continue without ground truth evaluation\")\n",
    "                    ground_truth = {}\n",
    "            \n",
    "            # Get list of all image files\n",
    "            image_files = [f for f in os.listdir(image_dir) \n",
    "                         if os.path.isfile(os.path.join(image_dir, f)) and \n",
    "                         f.lower().endswith(('png', '.jpg', '.jpeg', '.bmp', '.tif', '.tiff'))]\n",
    "            \n",
    "            if not image_files:\n",
    "                logging.error(f\"No valid image files found in {image_dir}\")\n",
    "                return {\n",
    "                    \"status\": \"error\",\n",
    "                    \"error_message\": f\"No valid image files found in directory\",\n",
    "                    \"image_dir\": image_dir\n",
    "                }\n",
    "            \n",
    "            logging.info(f\"Found {len(image_files)} images for inference\")\n",
    "            \n",
    "            # Process images in batches for efficiency\n",
    "            all_predictions = []\n",
    "            batch_idx = 0\n",
    "            start_time = time.time()\n",
    "            \n",
    "            with torch.no_grad():  # No gradient computation needed for inference\n",
    "                for batch_start in range(0, len(image_files), batch_size):\n",
    "                    batch_end = min(batch_start + batch_size, len(image_files))\n",
    "                    batch_files = image_files[batch_start:batch_end]\n",
    "                    batch_imgs = []\n",
    "                    valid_files = []\n",
    "                    \n",
    "                    # Load and preprocess images in this batch\n",
    "                    for img_file in batch_files:\n",
    "                        try:\n",
    "                            img_path = os.path.join(image_dir, img_file)\n",
    "                            image = Image.open(img_path).convert('RGB')\n",
    "                            image_tensor = transform(image)\n",
    "                            batch_imgs.append(image_tensor)\n",
    "                            valid_files.append(img_file)\n",
    "                        except Exception as e:\n",
    "                            logging.warning(f\"Error processing image {img_file}: {str(e)}\")\n",
    "                            all_predictions.append({\n",
    "                                'filename': img_file,\n",
    "                                'error': str(e)\n",
    "                            })\n",
    "                    \n",
    "                    if not batch_imgs:  # Skip if no valid images in batch\n",
    "                        continue\n",
    "                    \n",
    "                    # Stack tensors into batch\n",
    "                    batch_tensor = torch.stack(batch_imgs).to(device)\n",
    "                    \n",
    "                    try:\n",
    "                        # Run inference - need to handle the aux_logits\n",
    "                        outputs = model(batch_tensor)\n",
    "                        \n",
    "                        # Inception V3 in eval mode returns a tuple if aux_logits=True\n",
    "                        if isinstance(outputs, tuple):\n",
    "                            # During evaluation, we only use the main output (index 0)\n",
    "                            outputs = outputs[0]\n",
    "                            \n",
    "                        probabilities = torch.nn.functional.softmax(outputs, dim=1)\n",
    "                        \n",
    "                        # Process results\n",
    "                        batch_probs = probabilities.cpu().numpy()\n",
    "                        batch_preds = np.argmax(batch_probs, axis=1)\n",
    "                        \n",
    "                        # Store predictions\n",
    "                        for i, (filename, pred_class, probs) in enumerate(zip(valid_files, batch_preds, batch_probs)):\n",
    "                            # Get corresponding case ID if available\n",
    "                            case_id = filename_to_case.get(filename, filename) if 'filename_to_case' in locals() else filename\n",
    "                            \n",
    "                            result = {\n",
    "                                'filename': filename,\n",
    "                                'case_id': case_id,\n",
    "                                'predicted_class': int(pred_class),\n",
    "                                'confidence': float(probs[pred_class])\n",
    "                            }\n",
    "                            \n",
    "                            # Add ground truth if available\n",
    "                            if filename in ground_truth:\n",
    "                                result['true_label'] = int(ground_truth[filename])\n",
    "                                result['correct'] = result['predicted_class'] == result['true_label']\n",
    "                            \n",
    "                            # Add class name if available\n",
    "                            if class_names and pred_class < len(class_names):\n",
    "                                result['predicted_class_name'] = class_names[pred_class]\n",
    "                                \n",
    "                            # Add probabilities for each class\n",
    "                            for class_idx, prob in enumerate(probs):\n",
    "                                if class_names and class_idx < len(class_names):\n",
    "                                    result[f'prob_{class_names[class_idx]}'] = float(prob)\n",
    "                                else:\n",
    "                                    result[f'prob_class_{class_idx}'] = float(prob)\n",
    "                            \n",
    "                            all_predictions.append(result)\n",
    "                    except Exception as e:\n",
    "                        logging.error(f\"Error during inference for batch {batch_idx}: {str(e)}\")\n",
    "                        for filename in valid_files:\n",
    "                            all_predictions.append({\n",
    "                                'filename': filename,\n",
    "                                'error': f\"Batch inference error: {str(e)}\"\n",
    "                            })\n",
    "                    \n",
    "                    batch_idx += 1\n",
    "                    if batch_idx % 10 == 0:\n",
    "                        logging.info(f\"Processed {batch_end}/{len(image_files)} images\")\n",
    "            \n",
    "            processing_time = time.time() - start_time\n",
    "            logging.info(f\"Inference completed in {processing_time:.2f} seconds\")\n",
    "            \n",
    "            # Create results dataframe\n",
    "            predictions_df = pd.DataFrame(all_predictions)\n",
    "            \n",
    "            # Save predictions to CSV\n",
    "            csv_path = os.path.join(output_dir, \"predictions.csv\")\n",
    "            predictions_df.to_csv(csv_path, index=False)\n",
    "            logging.info(f\"Saved predictions to {csv_path}\")\n",
    "            \n",
    "            # Calculate metrics if ground truth is available\n",
    "            metrics = {}\n",
    "            metrics_path = None\n",
    "            cm_path = None\n",
    "            roc_path = None\n",
    "            \n",
    "            if ground_truth and any('true_label' in p for p in all_predictions):\n",
    "                logging.info(\"Calculating performance metrics...\")\n",
    "                \n",
    "                # Filter predictions with ground truth\n",
    "                valid_preds = [p for p in all_predictions if 'true_label' in p]\n",
    "                \n",
    "                # Extract true labels and predictions\n",
    "                y_true = [p['true_label'] for p in valid_preds]\n",
    "                y_pred = [p['predicted_class'] for p in valid_preds]\n",
    "                \n",
    "                # Get unique classes in sorted order\n",
    "                unique_classes = sorted(list(set(y_true)))\n",
    "                \n",
    "                # Basic metrics\n",
    "                metrics['accuracy'] = accuracy_score(y_true, y_pred)\n",
    "                \n",
    "                try:\n",
    "                    # Multi-class metrics\n",
    "                    metrics['precision_macro'] = precision_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "                    metrics['recall_macro'] = recall_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "                    metrics['f1_macro'] = f1_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "                    \n",
    "                    # Per-class metrics\n",
    "                    class_report = classification_report(y_true, y_pred, output_dict=True, zero_division=0)\n",
    "                    \n",
    "                    # Extract per-class metrics\n",
    "                    for class_idx in unique_classes:\n",
    "                        class_key = str(class_idx)\n",
    "                        if class_key in class_report:\n",
    "                            class_metrics = class_report[class_key]\n",
    "                            class_name = class_names[class_idx] if class_names and class_idx < len(class_names) else f\"class_{class_idx}\"\n",
    "                            \n",
    "                            metrics[f'precision_{class_name}'] = class_metrics['precision']\n",
    "                            metrics[f'recall_{class_name}'] = class_metrics['recall']\n",
    "                            metrics[f'f1_{class_name}'] = class_metrics['f1-score']\n",
    "                            \n",
    "                            # Calculate sensitivity and specificity for each class\n",
    "                            # Sensitivity = recall\n",
    "                            metrics[f'sensitivity_{class_name}'] = class_metrics['recall']\n",
    "                            \n",
    "                            # Specificity = TN / (TN + FP)\n",
    "                            y_true_binary = [1 if y == class_idx else 0 for y in y_true]\n",
    "                            y_pred_binary = [1 if y == class_idx else 0 for y in y_pred]\n",
    "                            tn, fp, fn, tp = confusion_matrix(y_true_binary, y_pred_binary, labels=[0, 1]).ravel()\n",
    "                            specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "                            metrics[f'specificity_{class_name}'] = specificity\n",
    "                    \n",
    "                    # Calculate AUC and ROC curves if we have probability outputs\n",
    "                    if len(unique_classes) == 2:  # Binary classification\n",
    "                        # Extract probabilities for positive class\n",
    "                        class_idx = max(unique_classes)  # Assuming 1 is positive class in binary case\n",
    "                        if class_names and class_idx < len(class_names):\n",
    "                            prob_col = f'prob_{class_names[class_idx]}'\n",
    "                        else:\n",
    "                            prob_col = f'prob_class_{class_idx}'\n",
    "                        \n",
    "                        if prob_col in predictions_df.columns:\n",
    "                            y_score = predictions_df.loc[predictions_df['true_label'].notna(), prob_col].values\n",
    "                            y_true_for_auc = [y for i, y in enumerate(y_true)]\n",
    "                            \n",
    "                            # Calculate ROC curve and AUC\n",
    "                            metrics['auc'] = roc_auc_score(y_true_for_auc, y_score)\n",
    "                            fpr, tpr, _ = roc_curve(y_true_for_auc, y_score)\n",
    "                            \n",
    "                            # Plot ROC curve\n",
    "                            plt.figure(figsize=(8, 8))\n",
    "                            plt.plot(fpr, tpr, color='darkorange', lw=2, \n",
    "                                    label=f'ROC curve (area = {metrics[\"auc\"]:.3f})')\n",
    "                            plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "                            plt.xlim([0.0, 1.0])\n",
    "                            plt.ylim([0.0, 1.05])\n",
    "                            plt.xlabel('False Positive Rate')\n",
    "                            plt.ylabel('True Positive Rate')\n",
    "                            plt.title('Receiver Operating Characteristic')\n",
    "                            plt.legend(loc=\"lower right\")\n",
    "                            \n",
    "                            roc_path = os.path.join(output_dir, \"roc_curve.png\")\n",
    "                            plt.savefig(roc_path)\n",
    "                            plt.close()\n",
    "                    elif len(unique_classes) > 2:  # Multi-class\n",
    "                        # For multi-class, calculate one-vs-rest AUC for each class\n",
    "                        for class_idx in unique_classes:\n",
    "                            if class_names and class_idx < len(class_names):\n",
    "                                prob_col = f'prob_{class_names[class_idx]}'\n",
    "                                class_name = class_names[class_idx]\n",
    "                            else:\n",
    "                                prob_col = f'prob_class_{class_idx}'\n",
    "                                class_name = f\"class_{class_idx}\"\n",
    "                            \n",
    "                            if prob_col in predictions_df.columns:\n",
    "                                # Create binary labels for this class\n",
    "                                y_true_binary = [1 if y == class_idx else 0 for y in y_true]\n",
    "                                y_score = predictions_df.loc[predictions_df['true_label'].notna(), prob_col].values\n",
    "                                \n",
    "                                try:\n",
    "                                    # Calculate AUC\n",
    "                                    class_auc = roc_auc_score(y_true_binary, y_score)\n",
    "                                    metrics[f'auc_{class_name}'] = class_auc\n",
    "                                except Exception as e:\n",
    "                                    logging.warning(f\"Could not calculate AUC for {class_name}: {str(e)}\")\n",
    "                except Exception as e:\n",
    "                    logging.warning(f\"Error calculating some metrics: {str(e)}\")\n",
    "                \n",
    "                # Log all metrics\n",
    "                logging.info(\"Performance metrics:\")\n",
    "                for metric_name, metric_value in metrics.items():\n",
    "                    logging.info(f\"{metric_name}: {metric_value:.4f}\")\n",
    "                \n",
    "                # Create confusion matrix\n",
    "                cm = confusion_matrix(y_true, y_pred, labels=unique_classes)\n",
    "                \n",
    "                # Set up class labels for the plot\n",
    "                if class_names:\n",
    "                    labels = [class_names[i] if i < len(class_names) else f\"class_{i}\" for i in unique_classes]\n",
    "                else:\n",
    "                    labels = [f\"class_{i}\" for i in unique_classes]\n",
    "                \n",
    "                plt.figure(figsize=(10, 8))\n",
    "                sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)\n",
    "                plt.xlabel('Predicted')\n",
    "                plt.ylabel('True')\n",
    "                plt.title('Confusion Matrix')\n",
    "                cm_path = os.path.join(output_dir, \"confusion_matrix.png\")\n",
    "                plt.savefig(cm_path)\n",
    "                plt.close()\n",
    "                \n",
    "                # Save metrics to JSON\n",
    "                metrics_path = os.path.join(output_dir, \"metrics.json\")\n",
    "                with open(metrics_path, 'w') as f:\n",
    "                    json.dump(metrics, f, indent=4)\n",
    "                \n",
    "                # Create a readable metrics summary as CSV\n",
    "                metrics_summary = []\n",
    "                for metric_name, metric_value in metrics.items():\n",
    "                    metrics_summary.append({\n",
    "                        'metric': metric_name,\n",
    "                        'value': metric_value\n",
    "                    })\n",
    "                pd.DataFrame(metrics_summary).to_csv(\n",
    "                    os.path.join(output_dir, \"metrics_summary.csv\"), index=False\n",
    "                )\n",
    "            \n",
    "            # Return results\n",
    "            return {\n",
    "                \"status\": \"success\",\n",
    "                \"predictions_path\": csv_path,\n",
    "                \"log_file\": log_file,\n",
    "                \"metrics\": metrics if metrics else None,\n",
    "                \"metrics_path\": metrics_path,\n",
    "                \"confusion_matrix_path\": cm_path,\n",
    "                \"roc_curve_path\": roc_path if 'roc_path' in locals() else None,\n",
    "                \"image_dir\": image_dir,\n",
    "                \"model_path\": model_path,\n",
    "                \"output_dir\": output_dir,\n",
    "                \"num_images_processed\": len(all_predictions),\n",
    "                \"num_images_with_errors\": sum(1 for p in all_predictions if 'error' in p),\n",
    "                \"has_ground_truth\": bool(ground_truth),\n",
    "                \"processing_time_seconds\": processing_time,\n",
    "                \"model_type\": \"inception_v3\",\n",
    "                \"aux_logits_used\": aux_logits\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error during inference: {str(e)}\", exc_info=True)\n",
    "            return {\n",
    "                \"status\": \"error\",\n",
    "                \"error_message\": str(e),\n",
    "                \"image_dir\": image_dir,\n",
    "                \"model_path\": model_path,\n",
    "                \"output_dir\": output_dir\n",
    "            }\n",
    "\n",
    "class PyTorchVGG16TrainingTool(Tool):\n",
    "    name = \"pytorch_vgg16_training\"\n",
    "    description = \"\"\"\n",
    "    This tool trains a VGG16 model using PyTorch for medical image classification.\n",
    "    It can train from scratch or fine-tune a pre-trained model, and includes validation metrics.\n",
    "    \"\"\"\n",
    "    \n",
    "    inputs = {\n",
    "        \"data_dir\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"Directory containing dataset with training and validation folders\"\n",
    "        },\n",
    "        \"output_dir\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"Directory where the trained model and results will be saved\"\n",
    "        },\n",
    "        \"num_classes\": {\n",
    "            \"type\": \"integer\",\n",
    "            \"description\": \"Number of classes for classification\"\n",
    "        },\n",
    "        \"num_epochs\": {\n",
    "            \"type\": \"integer\",\n",
    "            \"description\": \"Number of training epochs\",\n",
    "            \"required\": False,\n",
    "            \"nullable\": True\n",
    "        },\n",
    "        \"batch_size\": {\n",
    "            \"type\": \"integer\",\n",
    "            \"description\": \"Batch size for training\",\n",
    "            \"required\": False,\n",
    "            \"nullable\": True\n",
    "        },\n",
    "        \"pretrained\": {\n",
    "            \"type\": \"boolean\",\n",
    "            \"description\": \"Whether to use pretrained weights\",\n",
    "            \"required\": False,\n",
    "            \"nullable\": True\n",
    "        },\n",
    "        \"early_stopping\": {\n",
    "            \"type\": \"boolean\", \n",
    "            \"description\": \"Whether to use early stopping\",\n",
    "            \"required\": False,\n",
    "            \"nullable\": True\n",
    "        },\n",
    "        \"patience\": {\n",
    "            \"type\": \"integer\",\n",
    "            \"description\": \"Number of epochs to wait before early stopping\",\n",
    "            \"required\": False,\n",
    "            \"nullable\": True\n",
    "        },\n",
    "        \"use_batch_norm\": {\n",
    "            \"type\": \"boolean\",\n",
    "            \"description\": \"Whether to use VGG16 with batch normalization (VGG16_BN)\",\n",
    "            \"required\": False,\n",
    "            \"nullable\": True\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    output_type = \"object\"\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        data_dir: str,\n",
    "        output_dir: str,\n",
    "        num_classes: int,\n",
    "        num_epochs: Optional[int] = 10,\n",
    "        batch_size: Optional[int] = 16,\n",
    "        pretrained: Optional[bool] = True,\n",
    "        early_stopping: Optional[bool] = True,\n",
    "        patience: Optional[int] = 5,\n",
    "        use_batch_norm: Optional[bool] = False\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Train a VGG16 model for medical image classification.\n",
    "        \n",
    "        Args:\n",
    "            data_dir: Directory containing dataset with train and val subdirectories\n",
    "            output_dir: Directory to save model and results\n",
    "            num_classes: Number of classes for classification\n",
    "            num_epochs: Number of training epochs\n",
    "            batch_size: Batch size for training\n",
    "            pretrained: Whether to use pretrained weights\n",
    "            early_stopping: Whether to use early stopping\n",
    "            patience: Number of epochs to wait before early stopping\n",
    "            use_batch_norm: Whether to use VGG16 with batch normalization\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with training results and model paths\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Define the image size for VGG16 (standard size is 224x224)\n",
    "            vgg_image_size = (224, 224)\n",
    "            \n",
    "            # Set up logging\n",
    "            os.makedirs(output_dir, exist_ok=True)\n",
    "            log_file = os.path.join(output_dir, \"training.log\")\n",
    "            logging.basicConfig(\n",
    "                level=logging.INFO,\n",
    "                format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "                handlers=[\n",
    "                    logging.FileHandler(log_file),\n",
    "                    logging.StreamHandler()\n",
    "                ]\n",
    "            )\n",
    "            \n",
    "            # Log configuration\n",
    "            logging.info(f\"Starting VGG16 training with configuration:\")\n",
    "            logging.info(f\"Number of classes: {num_classes}\")\n",
    "            logging.info(f\"Pretrained: {pretrained}\")\n",
    "            logging.info(f\"Early stopping: {early_stopping}\")\n",
    "            logging.info(f\"Use batch normalization: {use_batch_norm}\")\n",
    "            if early_stopping:\n",
    "                logging.info(f\"Patience: {patience}\")\n",
    "            logging.info(f\"Data directory: {data_dir}\")\n",
    "            \n",
    "            # Check if CUDA is available\n",
    "            device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "            logging.info(f\"Using device: {device}\")\n",
    "            \n",
    "            # Define transformations for VGG16 (224x224 input)\n",
    "            train_transform = transforms.Compose([\n",
    "                transforms.Resize(vgg_image_size),\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.RandomRotation(10),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "            ])\n",
    "            \n",
    "            val_transform = transforms.Compose([\n",
    "                transforms.Resize(vgg_image_size),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "            ])\n",
    "            \n",
    "            # Setup directories\n",
    "            train_dir = os.path.join(data_dir, 'train')\n",
    "            val_dir = os.path.join(data_dir, 'val')\n",
    "            test_dir = os.path.join(data_dir, 'test')\n",
    "            \n",
    "            if not os.path.exists(train_dir):\n",
    "                raise ValueError(f\"Training directory not found: {train_dir}\")\n",
    "            \n",
    "            # Check if validation directory exists, otherwise use a split from training\n",
    "            use_train_val_split = not os.path.exists(val_dir)\n",
    "            \n",
    "            # Check if data directory has a labels.csv file or if images are in class subdirectories\n",
    "            train_labels_file = os.path.join(data_dir, 'labels.csv')\n",
    "            use_csv_labels = os.path.exists(train_labels_file)\n",
    "            \n",
    "            # Create datasets based on the data organization\n",
    "            if use_csv_labels:\n",
    "                # Using CSV file with image paths and labels\n",
    "                logging.info(f\"Using labels from CSV file: {train_labels_file}\")\n",
    "                train_dataset = MedicalImageDataset(\n",
    "                    image_dir=train_dir,\n",
    "                    labels_file=train_labels_file,\n",
    "                    transform=train_transform,\n",
    "                    is_test=False,\n",
    "                    image_size=vgg_image_size\n",
    "                )\n",
    "                \n",
    "                if not use_train_val_split:\n",
    "                    val_labels_file = os.path.join(data_dir, 'val_labels.csv')\n",
    "                    if not os.path.exists(val_labels_file):\n",
    "                        val_labels_file = train_labels_file  # Fallback to same labels file\n",
    "                    \n",
    "                    val_dataset = MedicalImageDataset(\n",
    "                        image_dir=val_dir,\n",
    "                        labels_file=val_labels_file,\n",
    "                        transform=val_transform,\n",
    "                        is_test=False,\n",
    "                        image_size=vgg_image_size\n",
    "                    )\n",
    "            else:\n",
    "                # Using directory structure (each class in its own subdirectory)\n",
    "                logging.info(\"Using directory structure for class labels\")\n",
    "                train_dataset = datasets.ImageFolder(\n",
    "                    root=train_dir,\n",
    "                    transform=train_transform\n",
    "                )\n",
    "                \n",
    "                if not use_train_val_split:\n",
    "                    val_dataset = datasets.ImageFolder(\n",
    "                        root=val_dir,\n",
    "                        transform=val_transform\n",
    "                    )\n",
    "            \n",
    "            # Split training data if no validation directory\n",
    "            if use_train_val_split:\n",
    "                train_size = int(0.8 * len(train_dataset))\n",
    "                val_size = len(train_dataset) - train_size\n",
    "                train_dataset, val_dataset = torch.utils.data.random_split(\n",
    "                    train_dataset, [train_size, val_size]\n",
    "                )\n",
    "            \n",
    "            logging.info(f\"Training dataset size: {len(train_dataset)}\")\n",
    "            logging.info(f\"Validation dataset size: {len(val_dataset)}\")\n",
    "            \n",
    "            # Create data loaders\n",
    "            train_loader = DataLoader(\n",
    "                train_dataset, \n",
    "                batch_size=batch_size, \n",
    "                shuffle=True, \n",
    "                num_workers=4\n",
    "            )\n",
    "            \n",
    "            val_loader = DataLoader(\n",
    "                val_dataset,\n",
    "                batch_size=batch_size,\n",
    "                shuffle=False,\n",
    "                num_workers=4\n",
    "            )\n",
    "            \n",
    "            # Create VGG16 model\n",
    "            logging.info(\"Creating VGG16 model...\")\n",
    "            \n",
    "            # Choose between VGG16 and VGG16_BN (with batch normalization)\n",
    "            if use_batch_norm:\n",
    "                logging.info(\"Using VGG16 with batch normalization\")\n",
    "                if pretrained:\n",
    "                    model = models.vgg16_bn(weights=models.VGG16_BN_Weights.IMAGENET1K_V1)\n",
    "                    logging.info(\"Using pretrained weights from ImageNet\")\n",
    "                else:\n",
    "                    model = models.vgg16_bn(weights=None)\n",
    "                    logging.info(\"Initializing model with random weights\")\n",
    "            else:\n",
    "                logging.info(\"Using standard VGG16\")\n",
    "                if pretrained:\n",
    "                    model = models.vgg16(weights=models.VGG16_Weights.IMAGENET1K_V1)\n",
    "                    logging.info(\"Using pretrained weights from ImageNet\")\n",
    "                else:\n",
    "                    model = models.vgg16(weights=None)\n",
    "                    logging.info(\"Initializing model with random weights\")\n",
    "            \n",
    "            # Modify the classifier for our number of classes\n",
    "            # VGG16's classifier is a sequential model with the last layer being the output layer\n",
    "            in_features = model.classifier[-1].in_features\n",
    "            model.classifier[-1] = nn.Linear(in_features, num_classes)\n",
    "            \n",
    "            # Move model to device\n",
    "            model = model.to(device)\n",
    "            \n",
    "            # Loss function and optimizer\n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "            \n",
    "            # Using Adam optimizer with weight decay to prevent overfitting\n",
    "            # VGG16 is prone to overfitting due to its large number of parameters\n",
    "            optimizer = optim.Adam(model.parameters(), lr=0.0001, weight_decay=1e-5)\n",
    "            \n",
    "            # Learning rate scheduler\n",
    "            scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "                optimizer, 'min', patience=2, factor=0.5\n",
    "            )\n",
    "            \n",
    "            # Track best model\n",
    "            best_val_loss = float('inf')\n",
    "            best_val_acc = 0.0\n",
    "            patience_counter = 0\n",
    "            \n",
    "            # Training history\n",
    "            history = {\n",
    "                'train_loss': [],\n",
    "                'val_loss': [],\n",
    "                'train_acc': [],\n",
    "                'val_acc': [],\n",
    "                'learning_rates': []\n",
    "            }\n",
    "            \n",
    "            # Training loop\n",
    "            logging.info(\"Starting training...\")\n",
    "            start_time = time.time()\n",
    "            \n",
    "            for epoch in range(num_epochs):\n",
    "                # Training phase\n",
    "                model.train()\n",
    "                train_loss = 0.0\n",
    "                train_correct = 0\n",
    "                train_total = 0\n",
    "                \n",
    "                for inputs, labels in train_loader:\n",
    "                    inputs, labels = inputs.to(device), labels.to(device)\n",
    "                    \n",
    "                    # Zero the gradients\n",
    "                    optimizer.zero_grad()\n",
    "                    \n",
    "                    # Forward pass\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    \n",
    "                    # Backward pass and optimize\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    \n",
    "                    # Statistics\n",
    "                    train_loss += loss.item() * inputs.size(0)\n",
    "                    _, predicted = torch.max(outputs, 1)\n",
    "                    train_total += labels.size(0)\n",
    "                    train_correct += (predicted == labels).sum().item()\n",
    "                \n",
    "                epoch_train_loss = train_loss / train_total\n",
    "                epoch_train_acc = 100 * train_correct / train_total\n",
    "                \n",
    "                # Validation phase\n",
    "                model.eval()\n",
    "                val_loss = 0.0\n",
    "                val_correct = 0\n",
    "                val_total = 0\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    for inputs, labels in val_loader:\n",
    "                        inputs, labels = inputs.to(device), labels.to(device)\n",
    "                        \n",
    "                        outputs = model(inputs)\n",
    "                        loss = criterion(outputs, labels)\n",
    "                        \n",
    "                        val_loss += loss.item() * inputs.size(0)\n",
    "                        _, predicted = torch.max(outputs, 1)\n",
    "                        val_total += labels.size(0)\n",
    "                        val_correct += (predicted == labels).sum().item()\n",
    "                \n",
    "                epoch_val_loss = val_loss / val_total\n",
    "                epoch_val_acc = 100 * val_correct / val_total\n",
    "                \n",
    "                # Update learning rate\n",
    "                scheduler.step(epoch_val_loss)\n",
    "                \n",
    "                # Get current learning rate\n",
    "                current_lr = optimizer.param_groups[0]['lr']\n",
    "                \n",
    "                # Save history\n",
    "                history['train_loss'].append(epoch_train_loss)\n",
    "                history['val_loss'].append(epoch_val_loss)\n",
    "                history['train_acc'].append(epoch_train_acc)\n",
    "                history['val_acc'].append(epoch_val_acc)\n",
    "                history['learning_rates'].append(current_lr)\n",
    "                \n",
    "                # Log progress\n",
    "                logging.info(f\"Epoch {epoch+1}/{num_epochs} - \"\n",
    "                           f\"Train Loss: {epoch_train_loss:.4f}, \"\n",
    "                           f\"Train Acc: {epoch_train_acc:.2f}%, \"\n",
    "                           f\"Val Loss: {epoch_val_loss:.4f}, \"\n",
    "                           f\"Val Acc: {epoch_val_acc:.2f}%, \"\n",
    "                           f\"LR: {current_lr:.6f}\")\n",
    "                \n",
    "                # Check for improvement\n",
    "                if epoch_val_acc > best_val_acc:\n",
    "                    best_val_acc = epoch_val_acc\n",
    "                    best_val_loss = epoch_val_loss\n",
    "                    patience_counter = 0\n",
    "                    \n",
    "                    # Save best model\n",
    "                    best_model_path = os.path.join(output_dir, \"best_model.pt\")\n",
    "                    torch.save({\n",
    "                        'epoch': epoch,\n",
    "                        'model_state_dict': model.state_dict(),\n",
    "                        'optimizer_state_dict': optimizer.state_dict(),\n",
    "                        'loss': best_val_loss,\n",
    "                        'accuracy': best_val_acc\n",
    "                    }, best_model_path)\n",
    "                    logging.info(f\"Saved best model with accuracy: {best_val_acc:.2f}%\")\n",
    "                else:\n",
    "                    patience_counter += 1\n",
    "                \n",
    "                # Early stopping if enabled\n",
    "                if early_stopping and patience_counter >= patience:\n",
    "                    logging.info(f\"Early stopping triggered after {epoch+1} epochs\")\n",
    "                    break\n",
    "            \n",
    "            # Calculate training time\n",
    "            total_time = time.time() - start_time\n",
    "            logging.info(f\"Training completed in {total_time:.2f} seconds\")\n",
    "            \n",
    "            # Save final model\n",
    "            final_model_path = os.path.join(output_dir, \"final_model.pt\")\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': epoch_val_loss,\n",
    "                'accuracy': epoch_val_acc\n",
    "            }, final_model_path)\n",
    "            \n",
    "            # If test directory exists, evaluate on test set\n",
    "            if os.path.exists(test_dir):\n",
    "                logging.info(\"Evaluating model on test set...\")\n",
    "                # Create test dataset and dataloader\n",
    "                if use_csv_labels:\n",
    "                    test_labels_file = os.path.join(data_dir, 'test_labels.csv')\n",
    "                    if not os.path.exists(test_labels_file):\n",
    "                        test_labels_file = train_labels_file  # Fallback to same labels file\n",
    "                    \n",
    "                    test_dataset = MedicalImageDataset(\n",
    "                        image_dir=test_dir,\n",
    "                        labels_file=test_labels_file,\n",
    "                        transform=val_transform,  # Use the same transform as validation\n",
    "                        is_test=False,\n",
    "                        image_size=vgg_image_size\n",
    "                    )\n",
    "                else:\n",
    "                    test_dataset = datasets.ImageFolder(\n",
    "                        root=test_dir,\n",
    "                        transform=val_transform\n",
    "                    )\n",
    "                \n",
    "                test_loader = DataLoader(\n",
    "                    test_dataset,\n",
    "                    batch_size=batch_size,\n",
    "                    shuffle=False,\n",
    "                    num_workers=4\n",
    "                )\n",
    "                \n",
    "                # Evaluate on test set\n",
    "                model.eval()\n",
    "                test_loss = 0.0\n",
    "                test_correct = 0\n",
    "                test_total = 0\n",
    "                all_preds = []\n",
    "                all_labels = []\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    for inputs, labels in test_loader:\n",
    "                        inputs, labels = inputs.to(device), labels.to(device)\n",
    "                        \n",
    "                        outputs = model(inputs)\n",
    "                        loss = criterion(outputs, labels)\n",
    "                        \n",
    "                        test_loss += loss.item() * inputs.size(0)\n",
    "                        _, predicted = torch.max(outputs, 1)\n",
    "                        test_total += labels.size(0)\n",
    "                        test_correct += (predicted == labels).sum().item()\n",
    "                        \n",
    "                        # Store predictions and labels for metrics\n",
    "                        all_preds.extend(predicted.cpu().numpy())\n",
    "                        all_labels.extend(labels.cpu().numpy())\n",
    "                \n",
    "                test_acc = 100 * test_correct / test_total\n",
    "                logging.info(f\"Test accuracy: {test_acc:.2f}%\")\n",
    "                \n",
    "                # Calculate and log detailed metrics\n",
    "                metrics = {}\n",
    "                metrics['accuracy'] = accuracy_score(all_labels, all_preds)\n",
    "                metrics['precision_macro'] = precision_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "                metrics['recall_macro'] = recall_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "                metrics['f1_macro'] = f1_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "                \n",
    "                # Add per-class metrics if there are multiple classes\n",
    "                if num_classes > 1:\n",
    "                    metrics['precision_per_class'] = precision_score(all_labels, all_preds, average=None, zero_division=0).tolist()\n",
    "                    metrics['recall_per_class'] = recall_score(all_labels, all_preds, average=None, zero_division=0).tolist()\n",
    "                    metrics['f1_per_class'] = f1_score(all_labels, all_preds, average=None, zero_division=0).tolist()\n",
    "                \n",
    "                # Save metrics to JSON\n",
    "                metrics_path = os.path.join(output_dir, \"test_metrics.json\")\n",
    "                with open(metrics_path, 'w') as f:\n",
    "                    json.dump(metrics, f, indent=4)\n",
    "                \n",
    "                # Generate confusion matrix\n",
    "                cm = confusion_matrix(all_labels, all_preds)\n",
    "                plt.figure(figsize=(10, 8))\n",
    "                sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "                plt.xlabel('Predicted')\n",
    "                plt.ylabel('True')\n",
    "                plt.title('Confusion Matrix')\n",
    "                cm_path = os.path.join(output_dir, \"confusion_matrix.png\")\n",
    "                plt.savefig(cm_path)\n",
    "                plt.close()\n",
    "            \n",
    "            # Save training history\n",
    "            history_path = os.path.join(output_dir, \"training_history.json\")\n",
    "            with open(history_path, 'w') as f:\n",
    "                json.dump(history, f, indent=4)\n",
    "            \n",
    "            # Create plots\n",
    "            plt.figure(figsize=(15, 5))\n",
    "            \n",
    "            plt.subplot(1, 3, 1)\n",
    "            plt.plot(history['train_loss'], label='Training Loss')\n",
    "            plt.plot(history['val_loss'], label='Validation Loss')\n",
    "            plt.xlabel('Epoch')\n",
    "            plt.ylabel('Loss')\n",
    "            plt.legend()\n",
    "            plt.title('Loss Curves')\n",
    "            \n",
    "            plt.subplot(1, 3, 2)\n",
    "            plt.plot(history['train_acc'], label='Training Accuracy')\n",
    "            plt.plot(history['val_acc'], label='Validation Accuracy')\n",
    "            plt.xlabel('Epoch')\n",
    "            plt.ylabel('Accuracy (%)')\n",
    "            plt.legend()\n",
    "            plt.title('Accuracy Curves')\n",
    "            \n",
    "            plt.subplot(1, 3, 3)\n",
    "            plt.plot(history['learning_rates'])\n",
    "            plt.xlabel('Epoch')\n",
    "            plt.ylabel('Learning Rate')\n",
    "            plt.title('Learning Rate Schedule')\n",
    "            plt.yscale('log')\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plots_path = os.path.join(output_dir, \"training_plots.png\")\n",
    "            plt.savefig(plots_path)\n",
    "            \n",
    "            # Save model configuration\n",
    "            model_type = \"vgg16_bn\" if use_batch_norm else \"vgg16\"\n",
    "            config = {\n",
    "                'model_type': model_type,\n",
    "                'num_classes': num_classes,\n",
    "                'image_size': 224,  # VGG16 uses 224x224 images\n",
    "                'pretrained': pretrained,\n",
    "                'early_stopping': early_stopping,\n",
    "                'patience': patience if early_stopping else None,\n",
    "                'best_epoch': epoch,\n",
    "                'best_accuracy': best_val_acc,\n",
    "                'training_epochs': epoch + 1,\n",
    "                'early_stopped': early_stopping and patience_counter >= patience,\n",
    "                'batch_size': batch_size,\n",
    "                'use_batch_norm': use_batch_norm\n",
    "            }\n",
    "            \n",
    "            config_path = os.path.join(output_dir, \"model_config.json\")\n",
    "            with open(config_path, 'w') as f:\n",
    "                json.dump(config, f, indent=4)\n",
    "            \n",
    "            # Return results\n",
    "            return {\n",
    "                \"status\": \"success\",\n",
    "                \"best_model_path\": best_model_path,\n",
    "                \"final_model_path\": final_model_path,\n",
    "                \"config_path\": config_path,\n",
    "                \"plots_path\": plots_path,\n",
    "                \"history_path\": history_path,\n",
    "                \"test_metrics_path\": metrics_path if 'metrics_path' in locals() else None,\n",
    "                \"confusion_matrix_path\": cm_path if 'cm_path' in locals() else None,\n",
    "                \"best_accuracy\": best_val_acc,\n",
    "                \"test_accuracy\": test_acc if 'test_acc' in locals() else None,\n",
    "                \"training_time_seconds\": total_time,\n",
    "                \"epochs_completed\": epoch + 1,\n",
    "                \"early_stopped\": early_stopping and patience_counter >= patience,\n",
    "                \"early_stopping_used\": early_stopping,\n",
    "                \"model_type\": model_type,\n",
    "                \"use_batch_norm\": use_batch_norm\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error during training: {str(e)}\", exc_info=True)\n",
    "            return {\n",
    "                \"status\": \"error\",\n",
    "                \"error_message\": str(e),\n",
    "                \"data_dir\": data_dir,\n",
    "                \"output_dir\": output_dir\n",
    "            }\n",
    "\n",
    "class PyTorchVGG16InferenceTool(Tool):\n",
    "    name = \"pytorch_vgg16_inference\"\n",
    "    description = \"\"\"\n",
    "    This tool uses a trained PyTorch VGG16 model to perform inference on new images.\n",
    "    It can also calculate performance metrics if ground truth labels are provided.\n",
    "    \"\"\"\n",
    "    \n",
    "    inputs = {\n",
    "        \"image_dir\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"Directory containing images for inference\"\n",
    "        },\n",
    "        \"model_path\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"Path to the trained model file (.pt format)\"\n",
    "        },\n",
    "        \"output_dir\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"Directory where prediction results will be saved\"\n",
    "        },\n",
    "        \"config_path\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"Path to model configuration JSON file (optional)\",\n",
    "            \"required\": False,\n",
    "            \"nullable\": True\n",
    "        },\n",
    "        \"ground_truth_file\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"CSV file with image filenames and ground truth labels for evaluation (optional)\",\n",
    "            \"required\": False,\n",
    "            \"nullable\": True\n",
    "        },\n",
    "        \"class_names\": {\n",
    "            \"type\": \"array\",\n",
    "            \"description\": \"List of class names corresponding to model output indices (optional)\",\n",
    "            \"required\": False,\n",
    "            \"nullable\": True\n",
    "        },\n",
    "        \"num_classes\": {\n",
    "            \"type\": \"integer\",\n",
    "            \"description\": \"Number of classes (needed if not provided in config file)\",\n",
    "            \"required\": False,\n",
    "            \"nullable\": True\n",
    "        },\n",
    "        \"batch_size\": {\n",
    "            \"type\": \"integer\",\n",
    "            \"description\": \"Batch size for inference\",\n",
    "            \"required\": False,\n",
    "            \"nullable\": True\n",
    "        },\n",
    "        \"case_column\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"Name of the column in ground truth file that contains case/file identifiers\",\n",
    "            \"required\": False,\n",
    "            \"nullable\": True\n",
    "        },\n",
    "        \"label_column\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"Name of the column in ground truth file that contains the labels\",\n",
    "            \"required\": False,\n",
    "            \"nullable\": True\n",
    "        },\n",
    "        \"file_extension\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"File extension to add to case IDs if they don't already have one\",\n",
    "            \"required\": False,\n",
    "            \"nullable\": True\n",
    "        },\n",
    "        \"use_batch_norm\": {\n",
    "            \"type\": \"boolean\",\n",
    "            \"description\": \"Whether the model uses batch normalization (VGG16_BN)\",\n",
    "            \"required\": False,\n",
    "            \"nullable\": True\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    output_type = \"object\"\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        image_dir: str,\n",
    "        model_path: str,\n",
    "        output_dir: str,\n",
    "        config_path: Optional[str] = None,\n",
    "        ground_truth_file: Optional[str] = None,\n",
    "        class_names: Optional[List[str]] = None,\n",
    "        num_classes: Optional[int] = None,\n",
    "        batch_size: Optional[int] = 32,\n",
    "        case_column: Optional[str] = \"case\",\n",
    "        label_column: Optional[str] = \"label\",\n",
    "        file_extension: Optional[str] = \".png\",\n",
    "        use_batch_norm: Optional[bool] = False\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Run inference using a trained PyTorch VGG16 model on new images.\n",
    "        \n",
    "        Args:\n",
    "            image_dir: Directory containing images for inference\n",
    "            model_path: Path to the trained model file (.pt format)\n",
    "            output_dir: Directory to save prediction outputs\n",
    "            config_path: Path to model configuration JSON file (optional)\n",
    "            ground_truth_file: CSV file with filenames and ground truth labels (optional)\n",
    "            class_names: List of class names corresponding to model output indices\n",
    "            num_classes: Number of classes (needed if not in config file)\n",
    "            batch_size: Batch size for inference\n",
    "            case_column: Name of the column in ground truth file with case/file IDs\n",
    "            label_column: Name of the column in ground truth file with labels\n",
    "            file_extension: File extension to add to case IDs if needed\n",
    "            use_batch_norm: Whether the model uses batch normalization\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with inference results and file paths\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Set up logging\n",
    "            os.makedirs(output_dir, exist_ok=True)\n",
    "            log_file = os.path.join(output_dir, \"inference.log\")\n",
    "            logging.basicConfig(\n",
    "                level=logging.INFO,\n",
    "                format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "                handlers=[\n",
    "                    logging.FileHandler(log_file),\n",
    "                    logging.StreamHandler()\n",
    "                ]\n",
    "            )\n",
    "            \n",
    "            # Load model configuration if provided\n",
    "            model_config = {}\n",
    "            if config_path and os.path.exists(config_path):\n",
    "                logging.info(f\"Loading model configuration from {config_path}\")\n",
    "                try:\n",
    "                    with open(config_path, 'r') as f:\n",
    "                        model_config = json.load(f)\n",
    "                    logging.info(f\"Model configuration loaded: {model_config}\")\n",
    "                except Exception as e:\n",
    "                    logging.warning(f\"Error loading model configuration: {str(e)}. Will use provided parameters.\")\n",
    "            \n",
    "            # Set parameters, with priority to explicit parameters over config values\n",
    "            # Determine if we're using VGG16 with batch normalization\n",
    "            if use_batch_norm is None:\n",
    "                use_batch_norm = model_config.get('use_batch_norm', False)\n",
    "            \n",
    "            # Set model type based on batch normalization flag\n",
    "            model_type = \"vgg16_bn\" if use_batch_norm else \"vgg16\"\n",
    "            \n",
    "            if num_classes is None:\n",
    "                num_classes = model_config.get('num_classes')\n",
    "                if num_classes is None:\n",
    "                    if class_names:\n",
    "                        num_classes = len(class_names)\n",
    "                    else:\n",
    "                        raise ValueError(\"Number of classes must be provided either directly, in config file, or through class_names\")\n",
    "            \n",
    "            # VGG16 uses 224x224 images\n",
    "            image_size = model_config.get('image_size', 224)\n",
    "            \n",
    "            # Log inference settings\n",
    "            logging.info(f\"Starting VGG16 inference with settings:\")\n",
    "            logging.info(f\"Model path: {model_path}\")\n",
    "            logging.info(f\"Model type: {model_type}\")\n",
    "            logging.info(f\"Number of classes: {num_classes}\")\n",
    "            logging.info(f\"Batch size: {batch_size}\")\n",
    "            logging.info(f\"Image directory: {image_dir}\")\n",
    "            logging.info(f\"Has ground truth: {ground_truth_file is not None}\")\n",
    "            \n",
    "            # Check if CUDA is available\n",
    "            device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "            logging.info(f\"Using device: {device}\")\n",
    "            \n",
    "            # Define image transformation\n",
    "            transform = transforms.Compose([\n",
    "                transforms.Resize((image_size, image_size)),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "            ])\n",
    "            \n",
    "            # Create model\n",
    "            logging.info(f\"Creating {model_type} model architecture...\")\n",
    "            \n",
    "            # Choose between VGG16 and VGG16_BN\n",
    "            if use_batch_norm:\n",
    "                model = models.vgg16_bn(pretrained=False)\n",
    "            else:\n",
    "                model = models.vgg16(pretrained=False)\n",
    "            \n",
    "            # Modify the classifier for our number of classes\n",
    "            # VGG16's classifier is a sequential model with the last layer being the output layer\n",
    "            in_features = model.classifier[-1].in_features\n",
    "            model.classifier[-1] = nn.Linear(in_features, num_classes)\n",
    "            \n",
    "            # Load trained weights\n",
    "            logging.info(f\"Loading model weights from {model_path}\")\n",
    "            try:\n",
    "                checkpoint = torch.load(model_path, map_location=device)\n",
    "                \n",
    "                # Handle different checkpoint formats\n",
    "                if 'model_state_dict' in checkpoint:\n",
    "                    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "                    logging.info(f\"Model loaded from checkpoint at epoch {checkpoint.get('epoch', 'unknown')}\")\n",
    "                else:\n",
    "                    model.load_state_dict(checkpoint)\n",
    "                    logging.info(\"Model loaded directly from state dict\")\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error loading model weights: {str(e)}\")\n",
    "                return {\n",
    "                    \"status\": \"error\",\n",
    "                    \"error_message\": f\"Failed to load model weights: {str(e)}\",\n",
    "                    \"model_path\": model_path\n",
    "                }\n",
    "            \n",
    "            # Move model to device and set to evaluation mode\n",
    "            model = model.to(device)\n",
    "            model.eval()\n",
    "            \n",
    "            # Load ground truth labels if provided\n",
    "            ground_truth = {}\n",
    "            if ground_truth_file and os.path.exists(ground_truth_file):\n",
    "                logging.info(f\"Loading ground truth data from {ground_truth_file}\")\n",
    "                try:\n",
    "                    gt_df = pd.read_csv(ground_truth_file)\n",
    "                    \n",
    "                    # Check required columns\n",
    "                    if case_column not in gt_df.columns or label_column not in gt_df.columns:\n",
    "                        raise ValueError(f\"Ground truth file must contain '{case_column}' and '{label_column}' columns\")\n",
    "                    \n",
    "                    # Create mapping of case ID to filename\n",
    "                    case_to_filename = {}\n",
    "                    \n",
    "                    # Create dictionary mapping filename to label\n",
    "                    for _, row in gt_df.iterrows():\n",
    "                        # Get case ID and convert to string\n",
    "                        case_id = str(row[case_column])\n",
    "                        \n",
    "                        # Add file extension if needed\n",
    "                        if not any(case_id.lower().endswith(ext) for ext in \n",
    "                                  ['.png', '.jpg', '.jpeg', '.bmp', '.tif', '.tiff']):\n",
    "                            filename = f\"{case_id}{file_extension}\"\n",
    "                        else:\n",
    "                            filename = case_id\n",
    "                        \n",
    "                        # Store the mapping and ground truth\n",
    "                        case_to_filename[case_id] = filename\n",
    "                        ground_truth[filename] = int(row[label_column])\n",
    "                        \n",
    "                    logging.info(f\"Loaded {len(ground_truth)} ground truth labels\")\n",
    "                    \n",
    "                    # Create a reverse mapping from filename to case ID for results\n",
    "                    filename_to_case = {v: k for k, v in case_to_filename.items()}\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    logging.error(f\"Error loading ground truth file: {str(e)}\")\n",
    "                    logging.warning(\"Will continue without ground truth evaluation\")\n",
    "                    ground_truth = {}\n",
    "            \n",
    "            # Get list of all image files\n",
    "            image_files = [f for f in os.listdir(image_dir) \n",
    "                         if os.path.isfile(os.path.join(image_dir, f)) and \n",
    "                         f.lower().endswith(('png', 'jpg', 'jpeg', 'bmp', 'tif', 'tiff'))]\n",
    "            \n",
    "            if not image_files:\n",
    "                logging.error(f\"No valid image files found in {image_dir}\")\n",
    "                return {\n",
    "                    \"status\": \"error\",\n",
    "                    \"error_message\": f\"No valid image files found in directory\",\n",
    "                    \"image_dir\": image_dir\n",
    "                }\n",
    "            \n",
    "            logging.info(f\"Found {len(image_files)} images for inference\")\n",
    "            \n",
    "            # Process images in batches for efficiency\n",
    "            all_predictions = []\n",
    "            batch_idx = 0\n",
    "            start_time = time.time()\n",
    "            \n",
    "            with torch.no_grad():  # No gradient computation needed for inference\n",
    "                for batch_start in range(0, len(image_files), batch_size):\n",
    "                    batch_end = min(batch_start + batch_size, len(image_files))\n",
    "                    batch_files = image_files[batch_start:batch_end]\n",
    "                    batch_imgs = []\n",
    "                    valid_files = []\n",
    "                    \n",
    "                    # Load and preprocess images in this batch\n",
    "                    for img_file in batch_files:\n",
    "                        try:\n",
    "                            img_path = os.path.join(image_dir, img_file)\n",
    "                            image = Image.open(img_path).convert('RGB')\n",
    "                            image_tensor = transform(image)\n",
    "                            batch_imgs.append(image_tensor)\n",
    "                            valid_files.append(img_file)\n",
    "                        except Exception as e:\n",
    "                            logging.warning(f\"Error processing image {img_file}: {str(e)}\")\n",
    "                            all_predictions.append({\n",
    "                                'filename': img_file,\n",
    "                                'error': str(e)\n",
    "                            })\n",
    "                    \n",
    "                    if not batch_imgs:  # Skip if no valid images in batch\n",
    "                        continue\n",
    "                    \n",
    "                    # Stack tensors into batch\n",
    "                    batch_tensor = torch.stack(batch_imgs).to(device)\n",
    "                    \n",
    "                    # Run inference\n",
    "                    outputs = model(batch_tensor)\n",
    "                    probabilities = torch.nn.functional.softmax(outputs, dim=1)\n",
    "                    \n",
    "                    # Process results\n",
    "                    batch_probs = probabilities.cpu().numpy()\n",
    "                    batch_preds = np.argmax(batch_probs, axis=1)\n",
    "                    \n",
    "                    # Store predictions\n",
    "                    for i, (filename, pred_class, probs) in enumerate(zip(valid_files, batch_preds, batch_probs)):\n",
    "                        # Get corresponding case ID if available\n",
    "                        case_id = filename_to_case.get(filename, filename) if 'filename_to_case' in locals() else filename\n",
    "                        \n",
    "                        result = {\n",
    "                            'filename': filename,\n",
    "                            'case_id': case_id,\n",
    "                            'predicted_class': int(pred_class),\n",
    "                            'confidence': float(probs[pred_class])\n",
    "                        }\n",
    "                        \n",
    "                        # Add ground truth if available\n",
    "                        if filename in ground_truth:\n",
    "                            result['true_label'] = int(ground_truth[filename])\n",
    "                            result['correct'] = result['predicted_class'] == result['true_label']\n",
    "                        \n",
    "                        # Add class name if available\n",
    "                        if class_names and pred_class < len(class_names):\n",
    "                            result['predicted_class_name'] = class_names[pred_class]\n",
    "                            \n",
    "                        # Add probabilities for each class\n",
    "                        for class_idx, prob in enumerate(probs):\n",
    "                            if class_names and class_idx < len(class_names):\n",
    "                                result[f'prob_{class_names[class_idx]}'] = float(prob)\n",
    "                            else:\n",
    "                                result[f'prob_class_{class_idx}'] = float(prob)\n",
    "                        \n",
    "                        all_predictions.append(result)\n",
    "                    \n",
    "                    batch_idx += 1\n",
    "                    if batch_idx % 10 == 0:\n",
    "                        logging.info(f\"Processed {batch_end}/{len(image_files)} images\")\n",
    "            \n",
    "            processing_time = time.time() - start_time\n",
    "            logging.info(f\"Inference completed in {processing_time:.2f} seconds\")\n",
    "            \n",
    "            # Create results dataframe\n",
    "            predictions_df = pd.DataFrame(all_predictions)\n",
    "            \n",
    "            # Save predictions to CSV\n",
    "            csv_path = os.path.join(output_dir, \"predictions.csv\")\n",
    "            predictions_df.to_csv(csv_path, index=False)\n",
    "            logging.info(f\"Saved predictions to {csv_path}\")\n",
    "            \n",
    "            # Calculate metrics if ground truth is available\n",
    "            metrics = {}\n",
    "            metrics_path = None\n",
    "            cm_path = None\n",
    "            roc_path = None\n",
    "            \n",
    "            if ground_truth and any('true_label' in p for p in all_predictions):\n",
    "                logging.info(\"Calculating performance metrics...\")\n",
    "                \n",
    "                # Filter predictions with ground truth\n",
    "                valid_preds = [p for p in all_predictions if 'true_label' in p]\n",
    "                \n",
    "                # Extract true labels and predictions\n",
    "                y_true = [p['true_label'] for p in valid_preds]\n",
    "                y_pred = [p['predicted_class'] for p in valid_preds]\n",
    "                \n",
    "                # Get unique classes in sorted order\n",
    "                unique_classes = sorted(list(set(y_true)))\n",
    "                \n",
    "                # Basic metrics\n",
    "                metrics['accuracy'] = accuracy_score(y_true, y_pred)\n",
    "                \n",
    "                try:\n",
    "                    # Multi-class metrics\n",
    "                    metrics['precision_macro'] = precision_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "                    metrics['recall_macro'] = recall_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "                    metrics['f1_macro'] = f1_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "                    \n",
    "                    # Per-class metrics\n",
    "                    class_report = classification_report(y_true, y_pred, output_dict=True, zero_division=0)\n",
    "                    \n",
    "                    # Extract per-class metrics\n",
    "                    for class_idx in unique_classes:\n",
    "                        class_key = str(class_idx)\n",
    "                        if class_key in class_report:\n",
    "                            class_metrics = class_report[class_key]\n",
    "                            class_name = class_names[class_idx] if class_names and class_idx < len(class_names) else f\"class_{class_idx}\"\n",
    "                            \n",
    "                            metrics[f'precision_{class_name}'] = class_metrics['precision']\n",
    "                            metrics[f'recall_{class_name}'] = class_metrics['recall']\n",
    "                            metrics[f'f1_{class_name}'] = class_metrics['f1-score']\n",
    "                            \n",
    "                            # Calculate sensitivity and specificity for each class\n",
    "                            # Sensitivity = recall\n",
    "                            metrics[f'sensitivity_{class_name}'] = class_metrics['recall']\n",
    "                            \n",
    "                            # Specificity = TN / (TN + FP)\n",
    "                            y_true_binary = [1 if y == class_idx else 0 for y in y_true]\n",
    "                            y_pred_binary = [1 if y == class_idx else 0 for y in y_pred]\n",
    "                            tn, fp, fn, tp = confusion_matrix(y_true_binary, y_pred_binary, labels=[0, 1]).ravel()\n",
    "                            specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "                            metrics[f'specificity_{class_name}'] = specificity\n",
    "                    \n",
    "                    # Calculate AUC and ROC curves if we have probability outputs\n",
    "                    if len(unique_classes) == 2:  # Binary classification\n",
    "                        # Extract probabilities for positive class\n",
    "                        class_idx = max(unique_classes)  # Assuming 1 is positive class in binary case\n",
    "                        if class_names and class_idx < len(class_names):\n",
    "                            prob_col = f'prob_{class_names[class_idx]}'\n",
    "                        else:\n",
    "                            prob_col = f'prob_class_{class_idx}'\n",
    "                        \n",
    "                        if prob_col in predictions_df.columns:\n",
    "                            y_score = predictions_df.loc[predictions_df['true_label'].notna(), prob_col].values\n",
    "                            y_true_for_auc = [y for i, y in enumerate(y_true)]\n",
    "                            \n",
    "                            # Calculate ROC curve and AUC\n",
    "                            metrics['auc'] = roc_auc_score(y_true_for_auc, y_score)\n",
    "                            fpr, tpr, _ = roc_curve(y_true_for_auc, y_score)\n",
    "                            \n",
    "                            # Plot ROC curve\n",
    "                            plt.figure(figsize=(8, 8))\n",
    "                            plt.plot(fpr, tpr, color='darkorange', lw=2, \n",
    "                                    label=f'ROC curve (area = {metrics[\"auc\"]:.3f})')\n",
    "                            plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "                            plt.xlim([0.0, 1.0])\n",
    "                            plt.ylim([0.0, 1.05])\n",
    "                            plt.xlabel('False Positive Rate')\n",
    "                            plt.ylabel('True Positive Rate')\n",
    "                            plt.title('Receiver Operating Characteristic')\n",
    "                            plt.legend(loc=\"lower right\")\n",
    "                            \n",
    "                            roc_path = os.path.join(output_dir, \"roc_curve.png\")\n",
    "                            plt.savefig(roc_path)\n",
    "                            plt.close()\n",
    "                    elif len(unique_classes) > 2:  # Multi-class\n",
    "                        # For multi-class, calculate one-vs-rest AUC for each class\n",
    "                        for class_idx in unique_classes:\n",
    "                            if class_names and class_idx < len(class_names):\n",
    "                                prob_col = f'prob_{class_names[class_idx]}'\n",
    "                                class_name = class_names[class_idx]\n",
    "                            else:\n",
    "                                prob_col = f'prob_class_{class_idx}'\n",
    "                                class_name = f\"class_{class_idx}\"\n",
    "                            \n",
    "                            if prob_col in predictions_df.columns:\n",
    "                                # Create binary labels for this class\n",
    "                                y_true_binary = [1 if y == class_idx else 0 for y in y_true]\n",
    "                                y_score = predictions_df.loc[predictions_df['true_label'].notna(), prob_col].values\n",
    "                                \n",
    "                                try:\n",
    "                                    # Calculate AUC\n",
    "                                    class_auc = roc_auc_score(y_true_binary, y_score)\n",
    "                                    metrics[f'auc_{class_name}'] = class_auc\n",
    "                                except Exception as e:\n",
    "                                    logging.warning(f\"Could not calculate AUC for {class_name}: {str(e)}\")\n",
    "                except Exception as e:\n",
    "                    logging.warning(f\"Error calculating some metrics: {str(e)}\")\n",
    "                \n",
    "                # Log all metrics\n",
    "                logging.info(\"Performance metrics:\")\n",
    "                for metric_name, metric_value in metrics.items():\n",
    "                    logging.info(f\"{metric_name}: {metric_value:.4f}\")\n",
    "                \n",
    "                # Create confusion matrix\n",
    "                cm = confusion_matrix(y_true, y_pred, labels=unique_classes)\n",
    "                \n",
    "                # Set up class labels for the plot\n",
    "                if class_names:\n",
    "                    labels = [class_names[i] if i < len(class_names) else f\"class_{i}\" for i in unique_classes]\n",
    "                else:\n",
    "                    labels = [f\"class_{i}\" for i in unique_classes]\n",
    "                \n",
    "                plt.figure(figsize=(10, 8))\n",
    "                sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)\n",
    "                plt.xlabel('Predicted')\n",
    "                plt.ylabel('True')\n",
    "                plt.title('Confusion Matrix')\n",
    "                cm_path = os.path.join(output_dir, \"confusion_matrix.png\")\n",
    "                plt.savefig(cm_path)\n",
    "                plt.close()\n",
    "                \n",
    "                # Save metrics to JSON\n",
    "                metrics_path = os.path.join(output_dir, \"metrics.json\")\n",
    "                with open(metrics_path, 'w') as f:\n",
    "                    json.dump(metrics, f, indent=4)\n",
    "                \n",
    "                # Create a readable metrics summary as CSV\n",
    "                metrics_summary = []\n",
    "                for metric_name, metric_value in metrics.items():\n",
    "                    metrics_summary.append({\n",
    "                        'metric': metric_name,\n",
    "                        'value': metric_value\n",
    "                    })\n",
    "                pd.DataFrame(metrics_summary).to_csv(\n",
    "                    os.path.join(output_dir, \"metrics_summary.csv\"), index=False\n",
    "                )\n",
    "            \n",
    "            # Return results\n",
    "            return {\n",
    "                \"status\": \"success\",\n",
    "                \"predictions_path\": csv_path,\n",
    "                \"log_file\": log_file,\n",
    "                \"metrics\": metrics if metrics else None,\n",
    "                \"metrics_path\": metrics_path,\n",
    "                \"confusion_matrix_path\": cm_path,\n",
    "                \"roc_curve_path\": roc_path if 'roc_path' in locals() else None,\n",
    "                \"image_dir\": image_dir,\n",
    "                \"model_path\": model_path,\n",
    "                \"output_dir\": output_dir,\n",
    "                \"num_images_processed\": len(all_predictions),\n",
    "                \"num_images_with_errors\": sum(1 for p in all_predictions if 'error' in p),\n",
    "                \"has_ground_truth\": bool(ground_truth),\n",
    "                \"processing_time_seconds\": processing_time,\n",
    "                \"model_type\": model_type,\n",
    "                \"use_batch_norm\": use_batch_norm\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error during inference: {str(e)}\", exc_info=True)\n",
    "            return {\n",
    "                \"status\": \"error\",\n",
    "                \"error_message\": str(e),\n",
    "                \"image_dir\": image_dir,\n",
    "                \"model_path\": model_path,\n",
    "                \"output_dir\": output_dir\n",
    "            }\n",
    "\n",
    "\n",
    "pytorch_vgg16_inference_tool = PyTorchVGG16InferenceTool()\n",
    "pytorch_vgg16_training_tool = PyTorchVGG16TrainingTool()\n",
    "pytorch_inceptionv3_inference_tool = PyTorchInceptionV3InferenceTool()\n",
    "pytorch_inceptionv3_training_tool = PyTorchInceptionV3TrainingTool()\n",
    "pytorch_resnet_inference_tool = PyTorchResNetInferenceTool()\n",
    "pytorch_resnet_training_tool = PyTorchResNetTrainingTool()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LLM Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use models through API calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gpt-4.1\n",
    "model = LiteLLMModel(model_id=\"openai/gpt-4.1\", api_key=\"YOUR_API_KEY_HERE\")\n",
    "#gpt-4o\n",
    "#model = LiteLLMModel(model_id=\"openai/gpt-4o\", api_key=\"YOUR_API_KEY_HERE\")\n",
    "#claude\n",
    "#model = LiteLLMModel(model_id=\"anthropic/claude-3-7-sonnet-20250219\", api_key=\"YOUR_API_KEY_HERE\") \n",
    "#deepseek V3\n",
    "#model = LiteLLMModel(model_id=\"deepseek/deepseek-chat\", api_key=\"YOUR_API_KEY_HERE\")\n",
    "#deepseek R1\n",
    "#model = LiteLLMModel(model_id=\"deepseek/deepseek-reasoner\", api_key=\"YOUR_API_KEY_HERE\")\n",
    "\n",
    "#hf API\n",
    "#Qwen\n",
    "#model_id = \"Qwen/QwQ-32B\" \n",
    "#model = HfApiModel(model_id=model_id, token=\"YOUR_API_KEY_HERE\")\n",
    "#Llama 3.3\n",
    "#model_id = \"meta-llama/Llama-3.3-70B-Instruct\" \n",
    "#model = HfApiModel(model_id=model_id, token=\"YOUR_API_KEY_HERE\")\n",
    "\n",
    "#Together AI API\n",
    "#llama-4-Scout\n",
    "#model = LiteLLMModel(model_id=\"together_ai/meta-llama/Llama-4-Scout-17B-16E-Instruct\", api_key=\"YOUR_API_KEY_HERE\")\n",
    "#QwQ-32B\n",
    "#model = LiteLLMModel(model_id=\"together_ai/Qwen/QwQ-32B\", api_key=\"YOUR_API_KEY_HERE\")\n",
    "#Llama 3.3\n",
    "#model = LiteLLMModel(model_id=\"together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo-Free\", api_key=\"YOUR_API_KEY_HERE\")\n",
    "#Llama 3.1 8b\n",
    "#model = LiteLLMModel(model_id=\"together_ai/meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo-classifier\", api_key=\"YOUR_API_KEY_HERE\")\n",
    "#Mistral 24b\n",
    "#model = LiteLLMModel(model_id=\"together_ai/mistralai/Mistral-Small-24B-Instruct-2501\", api_key=\"YOUR_API_KEY_HERE\")\n",
    "#Mistral 7b\n",
    "#model = LiteLLMModel(model_id=\"together_ai/mistralai/Mistral-7B-Instruct-v0.2\", api_key=\"YOUR_API_KEY_HERE\")\n",
    "#DeepSeek R1 Distill Qwen 14B\n",
    "#model = LiteLLMModel(model_id=\"together_ai/deepseek-ai/DeepSeek-R1-Distill-Qwen-14B\", api_key=\"YOUR_API_KEY_HERE\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agents Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Radiomic extraction Agent\n",
    "radiomic_extraction_agent = CodeAgent(\n",
    "    tools = [pyradiomics_feature_extraction_tool],\n",
    "    model=model,\n",
    "    add_base_tools=True,\n",
    "    additional_authorized_imports=['os', 'pandas'],\n",
    "    name=\"radiomic_extraction_agent\",\n",
    "    description=\"Extracts radiomic features from medical images and saves them as CSV files\",\n",
    "    max_steps = 5\n",
    ")\n",
    "#EDA Agent\n",
    "eda_agent = CodeAgent(\n",
    "    tools = [eda_tool],\n",
    "    model=model,\n",
    "    add_base_tools=True,\n",
    "    additional_authorized_imports=['os', 'pandas'],\n",
    "    name=\"eda_agent\",\n",
    "    description=\"Performs comprehensive exploratory data analysis\",\n",
    "    max_steps = 5\n",
    ")\n",
    "#Feature Importance Agent\n",
    "feature_selection_agent = CodeAgent(\n",
    "    tools = [feature_selection_tool],\n",
    "    model=model,\n",
    "    add_base_tools=True,\n",
    "    additional_authorized_imports=['os', 'pandas'],\n",
    "    name=\"feature_importance_agent\",\n",
    "    description=\"Performs feature importance analysis and exports the most important features to CSV files\",\n",
    "    max_steps = 5\n",
    ")\n",
    "#nnUNet Agent\n",
    "nnunet_agent = agent = CodeAgent(\n",
    "    tools = [nnunet_training_tool, nnunet_inference_tool],\n",
    "    model=model,\n",
    "    add_base_tools=True,\n",
    "    additional_authorized_imports=['re', 'subprocess', 'os'],\n",
    "    name=\"nnunet_agent\",\n",
    "    description=\"Uses the nnUNet framework for training and inference of medical image segmentation models\",\n",
    "    max_steps = 5\n",
    ")\n",
    "#TotalSegmentator Agent\n",
    "totalsegmentator_agent = CodeAgent(\n",
    "    tools = [totalsegmentator_tool],\n",
    "    model=model,\n",
    "    add_base_tools=True,\n",
    "    additional_authorized_imports=['os'],\n",
    "    name=\"totalsegmentator_agent\",\n",
    "    description=\"Utilizes the TotalSegmentator framework to segment organs and tissues in medical imaging data\",\n",
    "    max_steps = 5\n",
    ")\n",
    "#PyCaret classification Agent (tabulated data)\n",
    "pycaret_classification_agent = CodeAgent(\n",
    "    tools = [pycaret_class_training_tool, pycaret_class_inference_tool],\n",
    "    model=model,\n",
    "    add_base_tools=True,\n",
    "    additional_authorized_imports=['pycaret', 'setup', 'compare_models', 'tune_model', 'blend_models', \n",
    "                    'pull', 'predict_model', 'save_model',\n",
    "                    'plot_model', 'interpret_model', 'cuml', 'pandas'],\n",
    "    name=\"pycaret_classification_agent\",\n",
    "    description=\"Builds and deploys classification models using the PyCaret framework on tabular data inputs\",\n",
    "    max_steps = 5\n",
    ")\n",
    "#PyCaret regression Agent (tabulated data)\n",
    "pycaret_regression_agent = CodeAgent(\n",
    "    tools = [pycaret_regression_training_tool, pycaret_regression_inference_tool],\n",
    "    model=model,\n",
    "    add_base_tools=True,\n",
    "    additional_authorized_imports=['pycaret', 'setup', 'compare_models', 'tune_model', 'blend_models', \n",
    "                    'pull', 'predict_model', 'save_model',\n",
    "                    'plot_model', 'interpret_model', 'cuml', 'pandas'],\n",
    "    name=\"pycaret_regression_agent\",\n",
    "    description=\"Builds and deploys regression models using the PyCaret framework on tabular data inputs\",\n",
    "    max_steps = 5\n",
    ")\n",
    "#Image classification Agent\n",
    "image_classification_agent = CodeAgent(\n",
    "    tools = [pytorch_resnet_training_tool, pytorch_resnet_inference_tool, \n",
    "             pytorch_inceptionv3_training_tool, pytorch_inceptionv3_inference_tool,\n",
    "             pytorch_vgg16_training_tool, pytorch_vgg16_inference_tool],\n",
    "    model=model,\n",
    "    add_base_tools=True,\n",
    "    additional_authorized_imports=['os'],\n",
    "    name=\"image_classification_agent\",\n",
    "    description=\"Builds and deploys ResNet, Inceptionv3 and VGG16 image classification models\",\n",
    "    max_steps = 5\n",
    ")\n",
    "\n",
    "#Master Agent\n",
    "master_agent = CodeAgent(\n",
    "    tools = [],\n",
    "    managed_agents=[radiomic_extraction_agent, eda_agent, feature_selection_agent,\n",
    "                    nnunet_agent, totalsegmentator_agent, pycaret_classification_agent,\n",
    "                    pycaret_regression_agent, image_classification_agent],\n",
    "    model=model,\n",
    "    add_base_tools=True,\n",
    "    additional_authorized_imports=['os'],\n",
    "    name=\"master_agent\",\n",
    "    max_steps = 15\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_agent.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Radiomic Feature Extraction from CT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test: Generic query\n",
    "rfe_ct_prompt_1 = \"\"\"\n",
    "Perform a comprehensive radiomic feature extraction for the CT scans in: \"/path/to/ct/images\".\n",
    "The corresponding masks are here: \"/path/to/ct/labels\".\n",
    "Save the results here: \"/path/to/output_directory\".\n",
    "\"\"\"\n",
    "\n",
    "master_agent.run(rfe_ct_prompt_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test: Ask for specific radiomic freatures and filters.\n",
    "rfe_ct_prompt_2 = \"\"\"\n",
    "Extract shape and first order radiomic features for the CT scans in: \"/path/to/ct/images\".\n",
    "The respective masks are here: \"/path/to/ct/labels\".\n",
    "Save the results here: \"/path/to/output_directory\". \n",
    "Use the followng filters: Exponential, Gradient, LBP2D.\n",
    "\"\"\"\n",
    "\n",
    "master_agent.run(rfe_ct_prompt_2) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Radiomic Feature Extraction from MRI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test: Generic query for pre contrast MRI images (MAMAMIA dataset)\n",
    "rfe_mri_prompt_1 = \"\"\"\n",
    "Perform a comprehensive radiomic feature extraction for the MR scans in: \"/path/to/mri/mama_mia/images_pre_contrast\".\n",
    "The respective masks are here: \"/path/to//mri/mama_mia/labels\". \n",
    "Save the results here: \"/path/to/output_directory\".\n",
    "\"\"\"\n",
    "master_agent.run(rfe_mri_prompt_1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test: Extraction of radiomic features from mpMRI images (BraTS21 dataset) with multiple labels.\n",
    "#Asking specific features and filters\n",
    "\n",
    "rfe_mri_prompt_2 = \"\"\"\n",
    "Extract shape and glrlm and ngtdm radiomic features for the MR scans in: \"/path/to/mri/brats21/images\".\n",
    "The corresponding masks are here: \"/path/to/mri/brats21/labels\". \n",
    "Use the followng filters: Exponential, Gradient, SquareRoot.\n",
    "Save the results here: \"/path/to/output_directory\".\n",
    "\"\"\"\n",
    "master_agent.run(rfe_mri_prompt_2) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prompt for breast wisconsin dataset\n",
    "eda_prompt_1 = \"\"\"\n",
    "Perform comprehensive exploratory data analysis for the file: \"/path/to/breast_cancer_wisconsin_diagnosis_datasetdata.csv\". \n",
    "Save the output here: \"/path/to/output_directory\".\n",
    "\"\"\"\n",
    "\n",
    "#Prompt for predict_diabetes dataset\n",
    "eda_prompt_2 =\"\"\"\n",
    "Perform comprehensive EDA for the file: /path/to/predict_diabetes.csv\n",
    "Save the output here: \"/path/to/output_directory\"\n",
    "\"\"\"\n",
    "\n",
    "#Prompt for heart disease dataset\n",
    "eda_prompt_3 = \"\"\"\n",
    "Perform comprehensive EDA for the file: /path/to/heart_disease_classification.csv. \n",
    "Save the results here: \"/path/to/output_directory\".\n",
    "\"\"\"\n",
    "\n",
    "#Prompt for heart failure dataset\n",
    "eda_prompt_4 = \"\"\"\n",
    "Perform EDA for the file: /path/to/heart_failure_clinical_records_dataset.csv \n",
    "Save the results here: \"/path/to/output_directory\".\n",
    "\"\"\"\n",
    "\n",
    "#Prompt for life expectancy dataset\n",
    "eda_prompt_5 = \"\"\"\n",
    "Perform comprehensive EDA for the file: /path/to/Life-Expectancy-Data.csv \n",
    "Save the results here: \"/path/to/output_directory\".\n",
    "\"\"\"\n",
    "\n",
    "master_agent.run(eda_prompt_1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Importance Analysis and Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prompt for breast wisconsin dataset\n",
    "fia_prompt_1 = \"\"\"\n",
    "Perform feature importance analysis for the file: \"/path/to/breast_cancer_wisconsin_diagnosis_datasetdata.csv\".\n",
    "Save three csv files with the top 5, 10 and 20 features here: \"/path/to/output_directory\".\n",
    "Targert column is \"diagnosis\".\n",
    "\"\"\"\n",
    "\n",
    "#Prompt for predict_diabetes dataset !Asking to export more features than the original file has.\n",
    "fia_prompt_2 = \"\"\"\n",
    "        Perform feature importance analysis for the file: \"/path/to/predict_diabetes.csv\".\n",
    "        Save three csv files with the top 5, 10 and 20 features here: \"/path/to/output_directory\".\n",
    "        Targert column is \"Outcome\".\n",
    "        \"\"\"\n",
    "\n",
    "#Prompt for heart disease dataset\n",
    "fia_prompt_3 = \"\"\"\n",
    "        Perform feature importance analysis for the file: \"/path/to/heart_disease_classification.csv\".\n",
    "        Save three csv files with the top 5, 10 features here: \"/path/to/output_directory\".\n",
    "        Targert column is \"target\".\n",
    "        \"\"\"\n",
    "\n",
    "#Prompt for heart failure dataset\n",
    "fia_prompt_4 = \"\"\"\n",
    "        Perform feature importance analysis for the file: \"/path/to/heart_failure_clinical_records_dataset.csv\".\n",
    "        Save three csv files with the top 8 features here: \"/path/to/output_directory\".\n",
    "        Targert column is \"DEATH_EVENT\".\n",
    "        \"\"\"\n",
    "\n",
    "#Prompt for life expectancy dataset\n",
    "fia_prompt_5 = \"\"\"\n",
    "        Perform feature importance analysis for the file: /path/to/Life-Expectancy-Data-Updated.csv\n",
    "        Save two csv files with the top 10 and 15 features here: \"/path/to/output_directory\"\n",
    "        Targert column is \"Life_expectancy\". Create plots.\n",
    "        \"\"\"\n",
    "\n",
    "master_agent.run(fia_prompt_1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nnUNet framework - Train and Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prompt for training segmentation UNet (Brats21 dataset)\n",
    "nnunet_prompt_1 = 'Train a segmentation 3d full res for the dataset in: /nnUNet_raw/Dataset135_Brats21. For fold all'\n",
    "\n",
    "#Prompt for training segmentation UNet (Kits23 dataset)\n",
    "nnunet_prompt_1 = 'Train a segmentation 3d full res for the dataset in: /nnUNet_raw/Dataset140_Kits23. For fold all'\n",
    "\n",
    "#Prompt for inference mpMRI (Brats21 dataset)\n",
    "nnunet_prompt_2 =\"\"\"\n",
    "Using the nnUNet dataset 135, 3d full res fold_all model, segment the scans in: /inference_nnunet/brats21_validation. \n",
    "Output folder: /path/to/ouput_directory\n",
    "\"\"\"\n",
    "\n",
    "#Prompt for inference mpMRI (Kits23 dataset)\n",
    "nnunet_prompt_2 =\"\"\"\n",
    "Using the nnUNet dataset 140, 3d full res fold_all model, segment the scans in: /inference_nnunet/kits23_validation. \n",
    "Output folder: /path/to/ouput_directory\n",
    "\"\"\"\n",
    "\n",
    "master_agent.run(nnunet_prompt_2) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TotalSegmentator Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prompt 1 segmenting only one organ (CT)\n",
    "totalsegmentator_prompt_1 = \"\"\"\n",
    "Use TotalSegmentator with the total task to segment only the spleen in the CT scan located at /path/to/ct_input \n",
    "Save the mask in this directory: /path/to/ouput_directory\n",
    "\"\"\"\n",
    "\n",
    "#Prompt 2 segmenting three organs (CT)\n",
    "totalsegmentator_prompt_2 = \"\"\"\n",
    "Use TotalSegmentator with the total task to segment only the liver, stomach and kidneys in the CT scan found at /path/to/ct_input  \n",
    "Save the mask here /path/to/ouput_directory\n",
    "\"\"\"\n",
    "\n",
    "#Prompt 3 segmenting all available organs (CT)\n",
    "totalsegmentator_prompt_3 = \"\"\"\n",
    "Use TotalSegmentator with the total task to segment all available organs in the CT scan located at \"/path/to/ct_input\".  \n",
    "Save the mask in this folder: /path/to/ouput_directory\n",
    "\"\"\"\n",
    "\n",
    "#Prompt 4 segmenting all available organs (MR)\n",
    "totalsegmentator_prompt_4 = \"\"\"\n",
    "Use TotalSegmentator with the total_mr task to segment all available organs in the MRI scan found at \"/path/to/ct_input\"\n",
    "Save the mask in this folder: \"/path/to/ouput_directory\".\n",
    "\"\"\"\n",
    "\n",
    "master_agent.run(totalsegmentator_prompt_4) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training prompts Classification Model (Tabulated data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prompt for classification model development Breast wisconsin\n",
    "tct_prompt_1 = \"\"\"\n",
    "Train a classification model using the tabulated data: /path/to/breast_cancer_wisconsin_diagnosis_datasetdata.csv.\n",
    "Target column: \"diagnosis\". Exclude lightgbm classifier. Set normalization and transormation to False.\n",
    "Save the results here: \"/path/to/ouput_directory\".\n",
    "\"\"\"\n",
    "\n",
    "#Prompt for classification model development Predict diabetes\n",
    "tct_prompt_2 = \"\"\"\n",
    "Train a classification model using the file: /path/to/predict_diabetes.csv. \n",
    "Target column: \"Outcome\". Exclude lightgbm classifier.\n",
    "Save the results here: \"/path/to/ouput_directory\".\n",
    "\"\"\"\n",
    "\n",
    "#Prompt for classification model development Predict Heart Disease\n",
    "tct_prompt_3 = \"\"\"\n",
    "Train a classification model using the file: /path/to/heart_disease_classification.csv\n",
    "Target column: \"target\". Exclude lightgbm classifier.\n",
    "Save the results here: \"/path/to/ouput_directory\".\n",
    "\"\"\"\n",
    "\n",
    "#Prompt for classification model development Predict Heart Failure\n",
    "tct_prompt_4 = \"\"\"\n",
    "Train a classification model using the file: /path/to/heart_failure_clinical_records_dataset.csv\n",
    "Target column: \"DEATH_EVENT\". Exclude lightgbm, dummy and catboost classifier.\n",
    "Save the results here: \"/path/to/ouput_directory\"\n",
    "\"\"\"\n",
    "\n",
    "master_agent.run(tct_prompt_1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inference prompts Classification Model (Tabulated data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prompt for classification inference Breast wisconsin tuned model 1\n",
    "ict_prompt_1 = \"\"\"\n",
    "Use the classification model: /path/to/breast_cancer_wisconsin/models/tuned_model_1/tuned_model_1.pkl \n",
    "Make predictions using the predictors in the file: /path/to/inferer_results/independent_eval_cohort.csv \n",
    "The ground truth values are in the column: \"diagnosis\"\n",
    "Output directory: \"/path/to/ouput_directory\"\n",
    "\"\"\"\n",
    "\n",
    "#Prompt for classification inference Predict diabetes tuned model 3\n",
    "ict_prompt_2 = \"\"\"\n",
    "Use the classification model: /path/to/predict_diabetes/models/tuned_model_3/tuned_model_3.pkl \n",
    "Make predictions using the predictors in the file: /path/to/predict_diabetes/test_set.csv \n",
    "The ground truth values are in the column: \"Outcome\"\n",
    "Output directory: \"/path/to/ouput_directory\"\n",
    "Deploy the proper agent for this task.\n",
    "\"\"\"\n",
    "\n",
    "#Prompt for classification inference Predict diabetes blended model\n",
    "ict_prompt_3 = \"\"\"\n",
    "Use the classification model: /path/to/predict_diabetes/models/blended_model/blended_model.pkl\n",
    "Make predictions using the predictors in the file: /path/to/predict_diabetes/test_set.csv \n",
    "The ground truth values are in the column: \"Outcome\"\n",
    "Output directory: \"/path/to/ouput_directory\"\n",
    "\"\"\"\n",
    "\n",
    "#Prompt for classification inference Predict heart disease tuned model 3\n",
    "ict_prompt_4 = \"\"\"\n",
    "Use the classification model: /path/to/heart_disease/models/tuned_model_3/tuned_model_3.pkl\n",
    "Make predictions using the features in the file: /path/to/heart_disease/independent_eval_cohort.csv\n",
    "The ground truth values are in the column: \"target\"\n",
    "Output directory: \"/path/to/ouput_directory\"\n",
    "\"\"\"\n",
    "\n",
    "#Prompt for classification inference Predict heart failure tuned model 2\n",
    "ict_prompt_5 = \"\"\"\n",
    "Use the classification model: /path/to/heart_failure/models/tuned_model_2/tuned_model_2.pkl\n",
    "Make predictions using the predictors in the file: /path/to//heart_failure/independent_eval_cohort.csv\n",
    "The ground truth values are in the column: \"DEATH_EVENT\"\n",
    "Output directory: \"/path/to/ouput_directory\"\n",
    "\"\"\"\n",
    "\n",
    "master_agent.run(ict_prompt_5) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training prompts Regression Model (Tabulated data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prompt for regression model development Life expectancy dataset\n",
    "trt_prompt_1 = \"\"\"\n",
    "Train a regression model using the file: /path/to/Life-Expectancy-Data.csv \n",
    "Exclude \"lightgbm\".\n",
    "Target column: \"Life_expectancy\".\n",
    "Save the results here: \"/path/to/ouput_directory\"\n",
    "\"\"\"\n",
    "\n",
    "master_agent.run(trt_prompt_1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inference prompts Regression Model (Tabulated data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prompt for regression inference Life expectancy (providing the gt for comparison)\n",
    "irt_prompt_1 = \"\"\"\n",
    "Use the regression model: /path/to/life_expectancy/models/tuned_model_1/tuned_model_1.pkl\n",
    "Make predictions using the predictors in the file: /path/to/life_expectancy/independent_eval_cohort.csv\n",
    "The ground truth values are in the column: \"Life_expectancy\"\n",
    "Output directory: /path/to/ouput_directory\n",
    "Deploy the proper agent and tool for this task\n",
    "\"\"\"\n",
    "\n",
    "#Prompt for regression inference Life expectancy (without gt)\n",
    "irt_prompt_2 = \"\"\"\n",
    "Use the regression model: /path/to/life_expectancy/models/tuned_model_1/tuned_model_1.pkl\n",
    "Make predictions using the predictors in the file: /path/to/life_expectancy/no_gt_independent_eval_cohort.csv\n",
    "Output directory: /path/to/ouput_directory\n",
    "\"\"\"\n",
    "\n",
    "master_agent.run(irt_prompt_1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Prompts - Image Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prompt for classification resnet18 model development for pneumoniamnist_28 dataset\n",
    "ict_prompt_1 = \"\"\"\n",
    "Deploy the appropriate agent and tool to train a classification resnet18 model. \n",
    "The train dataset directory: \"/path/to/pneumoniamnist_28/dataset_pneumoniamnist_28/train\",  \n",
    "the validation dataset directory: \"/path/to/pneumoniamnist_28/dataset_pneumoniamnist_28/val\",\n",
    "the test dataset directory: \"/path/to/pneumoniamnist_28/dataset_pneumoniamnist_28/test\", \n",
    "Number of classes 2. \n",
    "Use a batch size of 64. Number of epochs: 60\n",
    "Output directory: \"/path/to/ouput_directory\" \n",
    "\"\"\"\n",
    "\n",
    "#Prompt for classification resnet34 model development for pathmnist_64 dataset \n",
    "ict_prompt_2 = \"\"\"\n",
    "Train a classification resnet34 model. \n",
    "The train, validation and test datasets: /path/to/pathmnist_64/dataset_pathmnist_64. \n",
    "Number of classes 9. \n",
    "Use a batch size of 32. Set patience to 10 and number of epochs to 50.\n",
    "Output directory: /path/to/ouput_directory \n",
    "\"\"\"\n",
    "\n",
    "#Prompt for classification resnet50 model development for breastmnist_128 dataset\n",
    "ict_prompt_3 = \"\"\"\n",
    "Train a classification resnet50 model. \n",
    "The train, val and test data are available here: \"/path/to/breastmnist_128/dataset_breastmnist_128/\".\n",
    "Number of classes 2. \n",
    "Train for 50 epochs. Do not use early stopping. \n",
    "Output folder: \"/path/to/ouput_directory\"\n",
    "\"\"\"\n",
    "\n",
    "##Prompt for classification resnet101 model development for dermamnist_224 dataset\n",
    "ict_prompt_4 = \"\"\"\n",
    "Train a classification resnet101 model. \n",
    "The train, val and test data are available here: \"/path/to/dermamnist_224/dataset_dermamnist_224\".\n",
    "Number of classes 7. Train for 200 epochs. Set patience for early stopping to 10. \n",
    "Output folder: \"/path/to/ouput_directory\"\n",
    "\"\"\"\n",
    "\n",
    "##Prompt for classification resnet152 model development for organamnist_28 dataset\n",
    "ict_prompt_5 = \"\"\"\n",
    "Train a classification resnet152 model. \n",
    "The train, val and test data are available here: \"/path/to/organamnist_28/dataset_organamnist_28/\".\n",
    "Number of classes 11. Set patience to 5.\n",
    "Output folder: \"/path/to/ouput_directory\"\n",
    "\"\"\"\n",
    "\n",
    "##Prompt for classification vgg16 model development for octmnist_28 dataset\n",
    "ict_prompt_6 = \"\"\"\n",
    "Train a classification vgg16 model. \n",
    "The train, val and test data are available here: \"/path/to/octmnist_28/dataset_octmnist_28/\".\n",
    "Number of classes 4. Do not use pretrained weights.\n",
    "Output folder: \"/path/to/ouput_directory\"\n",
    "\"\"\"\n",
    "\n",
    "##Prompt for classification InceptionV3 model development for bloodmnist_128 dataset\n",
    "ict_prompt_7 = \"\"\"\n",
    "Train a classification InceptionV3 model. \n",
    "The train, val and test data are available here: \"/path/to/bloodmnist_128/dataset_bloodmnist_128\".\n",
    "Number of classes 8. Use pretrained weights and a batch size of 64. Train for 150 epochs.\n",
    "Output folder: \"/path/to/ouput_directory\"\n",
    "\"\"\"\n",
    "\n",
    "master_agent.run(ict_prompt_5) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inference Prompts - Image Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prompt for inference with resnet18 model - pneumoniamnist_28 dataset\n",
    "ici_prompt_1 = \"\"\"\n",
    "Use the resnet-18 model available here: \"/path/to/pneumoniamnist_28/output_master_agent/resnet18_ict_prompt_1/best_model.pt\", \n",
    "to classify the images in this folder: \"/path/to/pneumoniamnist_28/dataset_pneumoniamnist_28/test\".\n",
    "The number of classes is 2. \n",
    "The ground truth labels for the evaluation are availabe here:\"/path/to/pneumoniamnist_28/inference/gt_test_labels.csv\".\n",
    "Save the evaluation output in this directory: \"/path/to/ouput_directory\". \n",
    "Deploy the appropriate agent and tool for this task.\n",
    "\"\"\"\n",
    "\n",
    "#Prompt for inference with resnet34 model - pathmnist_64 dataset\n",
    "ici_prompt_2 = \"\"\"\n",
    "Use the resnet34 model available here: \"/path/to/pathmnist_64/output_master_agent/resnet34_ict_prompt_2/best_model.pt\", \n",
    "to classify the images in this folder: \"/path/to/pathmnist_64/dataset_pathmnist_64/test\".\n",
    "The number of classes is 9. \n",
    "The ground truth labels for the evaluation are availabe here:\"/path/to/pathmnist_64/inference/gt_test_labels.csv\".\n",
    "Save the evaluation output in this directory: \"/path/to/ouput_directory\".\n",
    "\"\"\"\n",
    "\n",
    "#Prompt for inference with resnet50 model - breastmnist_128 dataset\n",
    "ici_prompt_3 = \"\"\"\n",
    "Use the resnet50 model available here: \"/path/to/breastmnist_128/output_master_agent/resnet50_ict_prompt_3/best_model.pt\",\n",
    "to classify the images in this folder: \"/path/to/breastmnist_128/dataset_breastmnist_128/test\".\n",
    "The number of classes is 2. \n",
    "The ground truth labels for the evaluation are availabe here: \"/path/to/breastmnist_128/inference/gt_test_labels.csv\".\n",
    "Output directory: \"/path/to/ouput_directory\".\n",
    "\"\"\"\n",
    "\n",
    "##Prompt for inference with resnet101 model - dermamnist_224 dataset\n",
    "ici_prompt_4 = \"\"\"\n",
    "Use the resnet101 model available here: \"/path/to/dermamnist_224/output_master_agent/resnet101_ict_prompt_4/best_model.pt\",\n",
    "to classify the images in this folder: \"/path/to/dermamnist_224/dataset_dermamnist_224/test\".\n",
    "Number of classes 7. \n",
    "Output folder: \"/path/to/ouput_directory\".\n",
    "\"\"\"\n",
    "\n",
    "##Prompt for inference with resnet152 model - organamnist_28 dataset\n",
    "ici_prompt_5 = \"\"\"\n",
    "Use the resnet152 model available here: \"/path/to/dermamnist_224/output_master_agent/resnet101_ict_prompt_4/best_model.pt\",\n",
    "to classify the images in this folder: \"/path/to/dermamnist_224/inference_test/images\".\n",
    "Number of classes 11. \n",
    "Output folder: \"/path/to/ouput_directory\".\n",
    "\"\"\"\n",
    "\n",
    "##Prompt for inference with vgg16 model - octmnist_28 dataset\n",
    "ici_prompt_6 = \"\"\"\n",
    "Use the vgg16 model available here: \"/path/to/octmnist_28/output_master_agent/vgg16_ict_prompt_6/best_model.pt\",\n",
    "to classify the images in this folder: \"/path/to/octmnist_28/dataset_octmnist_28/test\".\n",
    "Number of classes 4. The ground truth labels are availabe in this directory: \"/path/to/octmnist_28/inference\".\n",
    "Output folder: \"/path/to/ouput_directory\".\n",
    "\"\"\"\n",
    "\n",
    "##Prompt for inference with InceptionV3 model - bloodmnist_128 dataset\n",
    "ici_prompt_7 = \"\"\"\n",
    "Use the InceptionV3 model available here: \"/path/to/bloodmnist_128/output_master_agent/InceptionV3_ict_prompt_7/best_model.pt\",\n",
    "to classify the images in this folder: \"/path/to/bloodmnist_128/dataset_bloodmnist_128/test\".\n",
    "Number of classes 8. The ground truth labels are availabe in this directory: \"/path/to/bloodmnist_128/inference/gt_test_labels.csv\".\n",
    "Output folder: \"/path/to/ouput_directory\".\n",
    "\"\"\"\n",
    "\n",
    "master_agent.run(ici_prompt_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_task_prompt_1 = \"\"\"\n",
    "Use TotalSegmentator with the total task to segment only the spleen from the CT scans (one at a time) available here: \"/path/to/test_multi_tasking/multi_task_prompt_1/ct_scans\". \n",
    "Save the mask files to \"/path/to/test_multi_tasking/multi_task_prompt_1/masks\".\n",
    "Rename each mask file to have the exact same filename with the corresponding CT scan.\n",
    "Use the CT scans and the corresponding spleen masks to extract radiomic features. \n",
    "Save the resulting radiomic features CSV file in here: \"/path/to/ouput_directory\".\n",
    "Use the saved csv file with the radiomic features to perform Exploratory Data Analysis, without producing plots, on the extracted features and save all EDA results in this directory: \"/path/to/ouput_directory\".\n",
    "Deploy the appropriate agent for each task.\n",
    "\"\"\"\n",
    "\n",
    "multi_task_prompt_2 = \"\"\"\n",
    "Using the nnUNet dataset 135, 3d full res fold_all model, segment the mpMRI scans in: \"/path/to/test_multi_tasking/multi_task_prompt_2/mpMRI_scans\". \n",
    "Save the mask files to: \"/path/to/test_multi_tasking/multi_task_prompt_2/masks\"\n",
    "Use the MRI scans and the corresponding masks to extract radiomic features for each label in the mask files. Use the followng filters: Exponential, Original, Wavelet.\n",
    "Extract radiomic features only from the T1 MRI scans. For example, the mask name for the first case will be 'BraTS2021_00000.nii.gz', the corresponding T1 image name will be 'BraTS2021_00000_0000.nii.gz'.\n",
    "Save the resulting radiomic features CSV files in this folder: \"/path/to/ouput_directory\".\n",
    "Use the saved csv files with the radiomic features to perform Exploratory Data Analysis. Save EDA results for each label in subfolders, in this directory:\"/path/to/ouput_directory\".\n",
    "\"\"\"\n",
    "\n",
    "multi_task_prompt_3 = \"\"\"\n",
    "Perform a feature importance analysis for the file: \"/path/to/test_multi_tasking/multi_task_prompt_3/mamamia_features/dataset_mamamia.csv\".\n",
    "Save three csv files with the top 20, 50 and 100 features here: \"/path/to/ouput_directory\".\n",
    "Targert column is \"pcr\".\n",
    "Then use the csv file with the top 20 features to train a classifier. Save the output of the training process in this directory: \"/path/to/ouput_directory\".\n",
    "\"\"\"\n",
    "\n",
    "multi_task_prompt_4 = \"\"\"\n",
    "Perform exploratory data analysis for the file: \"/path/to/breast_cancer_wisconsin_diagnosis_dataset.csv\".\n",
    "Save eda results and a csv files with the top 10 features here: \"/path/to/ouput_directory\".\n",
    "Targert column is \"diagnosis\".\n",
    "Then use the csv file with the top 10 features to train a classifier. Save the output of the training process in this directory: \"/path/to/ouput_directory\".\n",
    "\"\"\"\n",
    "\n",
    "multi_task_prompt_5 = \"\"\"\n",
    "Train an InceptionV3 classification model using data available in the following directory: \"/path/to/pneumoniamnist_128/dataset_pneumoniamnist_128\".\n",
    "The number of classes is 2. Train for 100 epochs. Set patience to 10.\n",
    "Then, use the trained model to classify images in this directory: \"/path/to/pneumoniamnist_128/inference/test_data\".\n",
    "The ground truth labels for the evaluation are availabe here: \"/path/to/pneumoniamnist_128/inference/gt_test_labels.csv\".\n",
    "Save the trained model and inference results in this directory: \"/path/to/ouput_directory\".\n",
    "\"\"\"\n",
    "\n",
    "master_agent.run(multi_task_prompt_5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Interactive communication with the master agent run the following cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GradioUI(master_agent).launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agentic_segmentation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
